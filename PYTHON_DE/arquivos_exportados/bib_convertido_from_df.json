[{"Title":"Big data and human resource management research: An integrative review and new directions for future research","author":"Yucheng Zhang and Shan Xu and Long Zhang and Mengxi Yang","keywords":"Human resource management research, Big data, Integrative review, Inductive and deductive paradigms","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"The lack of sufficient big data-based approaches impedes the development of human resource management (HRM) research and practices. Although scholars have realized the importance of applying a big data approach to HRM research, clear guidance is lacking regarding how to integrate the two. Using a clustering algorithm based on the big data research paradigm, we first conduct a bibliometric review to quantitatively assess and scientifically map the evolution of the current big data HRM literature. Based on this systematic review, we propose a general theoretical framework described as \u201cInductive (Prediction paradigm: Data mining\/Theory building) vs. Deductive (Explanation paradigm: Theory testing)\u201d. In this framework, we discuss potential research questions, their corresponding levels of analysis, relevant methods, data sources and software. We then summarize the general procedures for conducting big data research within HRM research. Finally, we propose a future agenda for applying big data approaches to HRM research and identify five promising HRM research topics at the micro, meso and macro levels along with three challenges and limitations that HRM scholars may face in the era of big data.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2021.04.019","JIR":7.55,"SJR":2.049},{"Title":"Firm-level capabilities towards big data value creation","author":"Morten Brinch and Angappa Gunasekaran and Samuel {Fosso Wamba}","keywords":"Big data, Supply chain management, Operations management, Value creation, Business analytics, Capabilities","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Big data has played an increasingly important role in using data to improve business value. In response to several big data challenges, the purpose of this study is to identify firm-level capabilities required to create value from big data. The adjacent theories of business process management and IT business value underpinned the study, together with an in-depth case study that led to the identification of twenty-four types of capabilities related to IT, process, performance, human, strategic, and organizational practices. The findings confirmed the application of practices and capabilities of adjacent theories, as well as certain practices and attributes that were both changed and reinforced at the intersection of big data. As an outstanding additional support to the extant big data studies, this work empirically confirms and portrays hitherto unexplored capabilities of big data and set their roles, thus providing a holistic overview of firm-level capabilities that are required for big data value creation.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.07.036","JIR":7.55,"SJR":2.049},{"Title":"Exploring future challenges for big data in the humanitarian domain","author":"David Bell and Mark Lycett and Alaa Marshan and Asmat Monaghan","keywords":"Big data, Veracity, Granularity, Heterogeneous datasets, Humanitarian, Value","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"This paper examines the challenges of leveraging big data in the humanitarian sector in support of UN Sustainable Development Goal 17 \u201cPartnerships for the Goals\u201d. The full promise of Big Data is underpinned by a tacit assumption that the heterogeneous \u2018exhaust trail\u2019 of data is contextually relevant and sufficiently granular to be mined for value. This promise, however, relies on relationality \u2013 that patterns can be derived from combining different pieces of data that are of corresponding detail or that there are effective mechanisms to resolve differences in detail. Here, we present empirical work integrating eight heterogeneous datasets from the humanitarian domain to provide evidence of the inherent challenge of complexity resulting from differing levels of data granularity. In clarifying this challenge, we explore the reasons why it is manifest, discuss strategies for addressing it and, as our principal contribution, identify five propositions to guide future research.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.09.035","JIR":7.55,"SJR":2.049},{"Title":"Strategic orientations, developmental culture, and big data capability","author":"Canchu Lin and Anand Kunnathur","keywords":"Big data capability, Customer orientation, Entrepreneurial orientation, Technology orientation, And developmental culture","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Prior research articulated the importance of developing a big data analytics capability but did not show how to cultivate this development. Drawing on the literature on this topic, this study develops the concept of Big Data capability, which enhances our understanding of Big Data practice beyond that captured in previous literature on the concept of big data analytics capability. This study further highlights the strategic implications of the concept by testing its relationship to three strategic orientations and one aspect of organizational culture. Findings show that customer, entrepreneurial, and technology orientations, and developmental culture are important contributors to the development of Big Data capability.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2019.07.016","JIR":7.55,"SJR":2.049},{"Title":"Exploring how consumer goods companies innovate in the digital age: The role of big data analytics companies","author":"Marcello M. Mariani and Samuel {Fosso Wamba}","keywords":"Big data analytics, Forecasting, Innovation, Online review crowdsourcing, Consumer goods companies, Digital data","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"The advent and development of digital technologies have brought about a proliferation of online consumer reviews (OCRs), i.e., real-time customers\u2019 evaluations of products, services, and brands. Increasingly, e-commerce platforms are using them to gain insights from customer feedback. Meanwhile, a new generation of big data analytics (BDA) companies are crowdsourcing large volumes of OCRs by means of controlled ad hoc online experiments and advanced machine learning (ML) techniques to forecast demand and determine the market potential for new products in several industries. We illustrate how this process is taking place for consumer goods companies by exploring the case of UK digital BDA company, SoundOut. Based on an in-depth qualitative analysis, we develop the consumer goods company innovation (CGCI) conceptual framework, which illustrates how digital BDA firms help consumer goods companies to test new products before they are launched on the market, and innovate. Theoretical and managerial implications are discussed.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.09.012","JIR":7.55,"SJR":2.049},{"Title":"Does big data enhance firm innovation competency? The mediating role of data-driven insights","author":"Maryam Ghasemaghaei and Goran Calic","keywords":"Big data characteristics, Descriptive insight, Predictive insight, Prescriptive insight, Innovation competency","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Grounded in gestalt insight learning theory and organizational learning theory, we collected data from 280 middle and top-level managers to investigate the impact of each big data characteristic (i.e., data volume, data velocity, data variety, and data veracity) on firm innovation competency (i.e., exploitation competency and exploration competency), mediated through data-driven insight generation (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings show that while data velocity, variety, and veracity enhance data-driven insight generation, data volume does not impact it. Additionally, results of the post hoc analysis indicate that while descriptive and predictive insights improve innovation competency, prescriptive insight does not affect it. These results provide interesting and unique theoretical and practical insights.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2019.07.006","JIR":7.55,"SJR":2.049},{"Title":"Factors influencing big data decision-making quality","author":"Marijn Janssen and Haiko {van der Voort} and Agung Wahyudi","keywords":"Big data, Big data analytics, Big data chain, E-government, Governance, Decision-making, Decision-making quality","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Organizations are looking for ways to harness the power of big data (BD) to improve their decision making. Despite its significance the effects of BD on decision-making quality has been given scant attention in the literature. In this paper factors influencing decision-making based on BD are identified using a case study. BD is collected from different sources that have various data qualities and are processed by various organizational entities resulting in the creation of a big data chain. The veracity (manipulation, noise), variety (heterogeneity of data) and velocity (constantly changing data sources) amplified by the size of big data calls for relational and contractual governance mechanisms to ensure BD quality and being able to contextualize data. The case study reveals that taking advantage of big data is an evolutionary process in which the gradually understanding of the potential of big data and the routinization of processes plays a crucial role.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.007","JIR":7.55,"SJR":2.049},{"Title":"Unlocking the drivers of big data analytics value in firms","author":"Nadine C\u00f4rte-Real and Pedro Ruivo and Tiago Oliveira and Ale\u0161 Popovi\u010d","keywords":"IT business value, Big data analytics (BDA), Delphi method, Mixed methodology, Competitive advantage","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Although big data analytics (BDA) is considered the next \u201cfrontier\u201d in data science by creating potential business opportunities, the way to extract those opportunities is unclear. This paper aims to understand the antecedents of BDA value at a firm level. The authors performed a study using a mixed methodology approach. First, by carrying out a Delphi study to explore and rank the antecedents affecting the creation of BDA value. Based on the Delphi results, we propose an empirically validated model supported by a survey conducted on 175 European firms to explain the antecedents of BDA sustained value. The results show that the proposed model explains 62% of BDA sustained value at the firm level, where the most critical contributor is BDA use. We provide directions for managers to support their decisions on BDA strategy definition and refinement. For academics, we extend BDA value literature and outline some potential research opportunities.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2018.12.072","JIR":7.55,"SJR":2.049},{"Title":"On big data-guided upstream business research and its knowledge management","author":"Shastri L. Nimmagadda and Torsten Reiners and Lincoln C. Wood","keywords":"Upstream business, Heterogeneous and multidimensional data, Data warehousing and mining, Big Data paradigm, Spatial-temporal dimensions","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"The emerging Big Data integration imposes diverse challenges, compromising the sustainable business research practice. Heterogeneity, multi-dimensionality, velocity, and massive volumes that challenge Big Data paradigm may preclude the effective data and system integration processes. Business alignments get affected within and across joint ventures as enterprises attempt to adapt to changes in industrial environments rapidly. In the context of the Oil and Gas industry, we design integrated artefacts for a resilient multidimensional warehouse repository. With access to several decades of resource data in upstream companies, we incorporate knowledge-based data models with spatial-temporal dimensions in data schemas to minimize ambiguity in warehouse repository implementation. The design considerations ensure uniqueness and monotonic properties of dimensions, maintaining the connectivity between artefacts and achieving the business alignments. The multidimensional attributes envisage Big Data analysts a scope of business research with valuable new knowledge for decision support systems and adding further business values in geographic scales.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2018.04.029","JIR":7.55,"SJR":2.049},{"Title":"Customer experience management in the age of big data analytics: A strategic framework","author":"Maria Holmlund and Yves {Van Vaerenbergh} and Robert Ciuchita and Annika Ravald and Panagiotis Sarantopoulos and Francisco Villarroel Ordenes and Mohamed Zaki","keywords":"Customer experience, Customer experience management, Customer experience insight, Big data analytics","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Customer experience (CX) has emerged as a sustainable source of competitive differentiation. Recent developments in big data analytics (BDA) have exposed possibilities to unlock customer insights for customer experience management (CXM). Research at the intersection of these two fields is scarce and there is a need for conceptual work that (1) provides an overview of opportunities to use BDA for CXM and (2) guides management practice and future research. The purpose of this paper is therefore to develop a strategic framework for CXM based on CX insights resulting from BDA. Our conceptualisation is comprehensive and is particularly relevant for researchers and practitioners who are less familiar with the potential of BDA for CXM. For managers, we provide a step-by-step guide on how to kick-start or implement our strategic framework. For researchers, we propose some opportunities for future studies in this promising research area.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.01.022","JIR":7.55,"SJR":2.049},{"Title":"Big data analytics and firm performance: Findings from a mixed-method approach","author":"Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie","keywords":"Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method, Environmental uncertainty","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Big data analytics has been widely regarded as a breakthrough technological development in academic and business communities. Despite the growing number of firms that are launching big data initiatives, there is still limited understanding on how firms translate the potential of such technologies into business value. The literature argues that to leverage big data analytics and realize performance gains, firms must develop strong big data analytics capabilities. Nevertheless, most studies operate under the assumption that there is limited heterogeneity in the way firms build their big data analytics capabilities and that related resources are of similar importance regardless of context. This paper draws on complexity theory and investigates the configurations of resources and contextual factors that lead to performance gains from big data analytics investments. Our empirical investigation followed a mixed methods approach using survey data from 175 chief information officers and IT managers working in Greek firms, and three case studies to show that depending on the context, big data analytics resources differ in significance when considering performance gains. Applying a fuzzy-set qualitative comparative analysis (fsQCA) method on the quantitative data, we show that there are four different patterns of elements surrounding big data analytics that lead to high performance. Outcomes of the three case studies highlight the inter-relationships between these elements and outline challenges that organizations face when orchestrating big data analytics resources.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2019.01.044","JIR":7.55,"SJR":2.049},{"Title":"Exploring the path to big data analytics success in healthcare","author":"Yichuan Wang and Nick Hajli","keywords":"Big data analytics, Business value, Capability building view, Resource-based theory, Information technology source management, Health care industries","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Although big data analytics have tremendous benefits for healthcare organizations, extant research has paid insufficient attention to the exploration of its business value. In order to bridge this knowledge gap, this study proposes a big data analytics-enabled business value model in which we use the resource-based theory (RBT) and capability building view to explain how big data analytics capabilities can be developed and what potential benefits can be obtained by these capabilities in the health care industries. Using this model, we investigate 109 case descriptions, covering 63 healthcare organizations to explore the causal relationships between the big data analytics capabilities and business value and the path-to-value chains for big data analytics success. Our findings provide new insights to healthcare practitioners on how to constitute big data analytics capabilities for business transformation and offer an empirical basis that can stimulate a more detailed investigation of big data analytics implementation.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.002","JIR":7.55,"SJR":2.049},{"Title":"Big Data consumer analytics and the transformation of marketing","author":"Sunil Erevelles and Nobuyuki Fukawa and Linda Swayne","keywords":"Big Data, Consumer analytics, Consumer insights, Resource-based theory, Induction, Ignorance","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Consumer analytics is at the epicenter of a Big Data revolution. Technology helps capture rich and plentiful data on consumer phenomena in real time. Thus, unprecedented volume, velocity, and variety of primary data, Big Data, are available from individual consumers. To better understand the impact of Big Data on various marketing activities, enabling firms to better exploit its benefits, a conceptual framework that builds on resource-based theory is proposed. Three resources\u2014physical, human, and organizational capital\u2014moderate the following: (1) the process of collecting and storing evidence of consumer activity as Big Data, (2) the process of extracting consumer insight from Big Data, and (3) the process of utilizing consumer insight to enhance dynamic\/adaptive capabilities. Furthermore, unique resource requirements for firms to benefit from Big Data are discussed.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2015.07.001","JIR":7.55,"SJR":2.049},{"Title":"Critical analysis of Big Data challenges and analytical methods","author":"Uthayasankar Sivarajah and Muhammad Mustafa Kamal and Zahir Irani and Vishanth Weerakkody","keywords":"Big Data, Big Data Analytics, Challenges, Methods, Systematic literature review","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Big Data (BD), with their potential to ascertain valued insights for enhanced decision-making process, have recently attracted substantial interest from both academics and practitioners. Big Data Analytics (BDA) is increasingly becoming a trending practice that many organizations are adopting with the purpose of constructing valuable information from BD. The analytics process, including the deployment and use of BDA tools, is seen by organizations as a tool to improve operational efficiency though it has strategic potential, drive new revenue streams and gain competitive advantages over business rivals. However, there are different types of analytic applications to consider. Therefore, prior to hasty use and buying costly BD tools, there is a need for organizations to first understand the BDA landscape. Given the significant nature of the BD and BDA, this paper presents a state-of-the-art review that presents a holistic view of the BD challenges and BDA methods theorized\/proposed\/employed by organizations to help others understand this landscape with the objective of making robust investment decisions. In doing so, systematically analysing and synthesizing the extant research published on BD and BDA area. More specifically, the authors seek to answer the following two principal questions: Q1 \u2013 What are the different types of BD challenges theorized\/proposed\/confronted by organizations? and Q2 \u2013 What are the different types of BDA methods theorized\/proposed\/employed to overcome BD challenges?. This systematic literature review (SLR) is carried out through observing and understanding the past trends and extant patterns\/themes in the BDA research area, evaluating contributions, summarizing knowledge, thereby identifying limitations, implications and potential further research avenues to support the academic community in exploring research themes\/patterns. Thus, to trace the implementation of BD strategies, a profiling method is employed to analyze articles (published in English-speaking peer-reviewed journals between 1996 and 2015) extracted from the Scopus database. The analysis presented in this paper has identified relevant BD research studies that have contributed both conceptually and empirically to the expansion and accrual of intellectual wealth to the BDA in technology and organizational resource management discipline.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2016.08.001","JIR":7.55,"SJR":2.049},{"Title":"Assessing the drivers of machine learning business value","author":"Carolina Reis and Pedro Ruivo and Tiago Oliveira and Paulo Faroleiro","keywords":"Machine learning, Business value, Competitive advantage, Dynamic capabilities theory","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Machine learning (ML) is expected to transform the business landscape in the near future completely. Hitherto, some successful ML case-stories have emerged. However, how organizations can derive business value (BV) from ML has not yet been substantiated. We assemble a conceptual model, grounded on the dynamic capabilities theory, to uncover key drivers of ML BV, in terms of financial and strategic performance. The proposed model was assessed by surveying 319 corporations. Our findings are that ML use, big data analytics maturity, platform maturity, top management support, and process complexity are, to some extent, drivers of ML BV. We also find that platform maturity has, to some degree, a moderator influence between ML use and ML BV, and between big data analytics maturity and ML BV. To the best of our knowledge, this is the first research to deliver such findings in the ML field.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.05.053","JIR":7.55,"SJR":2.049},{"Title":"Examining the determinants of successful adoption of data analytics in human resource management \u2013 A framework for implications","author":"Sateesh.V. Shet and Tanuj Poddar and Fosso {Wamba Samuel} and Yogesh K. Dwivedi","keywords":"Human resource analytics, HRM analytics, People analytics, Adoption of HR analytics, Challenges, Implementation of HR analytics, Big data, Data analytics, Framework synthesis","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Data analytics has gained importance in human resource management (HRM) for its ability to provide insights based on data-driven decision-making processes. However, integrating an analytics-based approach in HRM is a complex process, and hence, many organizations are unable to adopt HR Analytics (HRA). Using a framework synthesis approach, we first identify the challenges that hinder the practice of HRA and then develop a framework to explain the different factors that impact the adoption of HRA within organizations. This study identifies the key aspects related to the technological, organizational, environmental, data governance, and individual factors that influence the adoption of HRA. In addition, this paper determines 23 sub-dimensions of these five factors as the crucial aspects for successfully implementing and practicing HRA within organizations. We also discuss the implications of the framework for HR leaders, HR Managers, CEOs, IT Managers and consulting practitioners for effective adoption of HRA in organization.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2021.03.054","JIR":7.55,"SJR":2.049},{"Title":"Social media use: A review of innovation management practices","author":"Marie-Isabelle Muninger and Dominik Mahr and Wafa Hammedi","keywords":"Social media, Innovation, Systematic review, Framework and research agenda","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"The use of social media for innovation requires firms to manage rapid information transfers, big data, and multiway communication. Yet managers lack clear insights on the way social media should be managed and current literature is dispersed across various research streams. In this article, the authors aim to develop a better understanding of how social media use should be leveraged for innovation. To achieve this objective, they build a systematic review of evidence from 177 scientific articles across four key management disciplines. They analyze research perspectives and conceptualizations of social media use for innovation and provide a framework of the drivers, contingencies and outcomes related to this topic. Next, they attempt to identify what is currently known about social media use for innovation. Last, they suggest critical areas for future inquiry on this important subject.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2022.01.039","JIR":7.55,"SJR":2.049},{"Title":"Leveraging machine learning in the global fight against money laundering and terrorism financing: An affordances perspective","author":"Ana Isabel Canhoto","keywords":"Big data, Artificial intelligence, Machine learning, Algorithm, Customer profiling, Financial services, Anti-money laundering, United Nations, Sustainable development goals","journal":"JOURNAL OF BUSINESS RESEARCH","abstract":"Financial services organisations facilitate the movement of money worldwide, and keep records of their clients\u2019 identity and financial behaviour. As such, they have been enlisted by governments worldwide to assist with the detection and prevention of money laundering, which is a key tool in the fight to reduce crime and create sustainable economic development, corresponding to Goal 16 of the United Nations Sustainable Development Goals. In this paper, we investigate how the technical and contextual affordances of machine learning algorithms may enable these organisations to accomplish that task. We find that, due to the unavailability of high-quality, large training datasets regarding money laundering methods, there is limited scope for using supervised machine learning. Conversely, it is possible to use reinforced machine learning and, to an extent, unsupervised learning, although only to model unusual financial behaviour, not actual money laundering.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jbusres.2020.10.012","JIR":7.55,"SJR":2.049},{"Title":"The convergence of big data and accounting: innovative research opportunities","author":"Awad Elsayed Awad Ibrahim and Ahmed A. Elamer and Amr Nazieh Ezat","keywords":"Big data, Analytics, Accounting, Data science, Business intelligence","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"This study aims to develop accounting standards, curriculums, and research to cope with the rapid development of big data. The study presents several potential convergence points between big data and different accounting techniques and theories. The study discusses how big data can overcome the data limitations of six accounting issues: financial reporting, performance measurement, audit evidence, risk management, corporate budgeting and activity-based techniques. It presents six exciting research questions for future research. Then, the study explains the potential convergence between big data and agency theory, stakeholders theory, and legitimacy theory. This theoretical study develops new convergence points between big data and accounting by reviewing the literature and proposing new ideas and research questions. The conclusion indicates a significant convergence between big data and accounting on the premise that data is the heart of accounting. Big data and advanced analytics have the potential to overcome the data limitations of accounting techniques that require estimations and predictions. A remarkable convergence is argued between big data and three accounting theories. Overall, the study presents helpful insights to members of the accounting and auditing community on the potential of big data.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121171","JIR":8.593,"SJR":2.226},{"Title":"Big data analytics for clinical decision-making: Understanding health sector perceptions of policy and practice","author":"Kasuni Weerasinghe and Shane L. Scahill and David J. Pauleen and Nazim Taskin","keywords":"Analytics, Clinical decision-making, Big data, Healthcare, Social representation theory","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The introduction and use of \u2018big data and analytics\u2019 is an on-going issue of discussion in health sectors globally. Healthcare systems of developed countries are trying to create more value and better healthcare through data and use of big data technologies. With an increasing number of articles identifying the value creation of big data and analytics for clinical decision-making, this paper examines how big data is applied, or not applied, in clinical practice. Using social representation theory as a theoretical foundation the paper explores people's perceptions of big data across all levels (policy making, planning, funding, and clinical care) of the New Zealand healthcare sector. The findings show that although adoption of big data technologies is planned for population health and health management, the potential of big data for clinical care has yet to be explored in the New Zealand context. The findings also highlight concern over data quality. The paper provides recommendations for policy and practice particularly around the need for engagement and participation of all levels to discuss data quality as well as big-data-based changes such as precision medicine and technology-assisted clinical decision-making tools. Future avenues of research are suggested.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121222","JIR":8.593,"SJR":2.226},{"Title":"Big data analytics capability and decision making performance in emerging market firms: The role of contractual and relational governance mechanisms","author":"Saqib Shamim and Jing Zeng and Zaheer Khan and Najam Ul Zia","keywords":"Big data, Contractual governance, Relational governance, Big data analytics capability, Culture, Decision-making performance, Emerging markets","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"This study examines the role of big data contractual and relational governance in big data decision-making performance of firms based in China. It investigates the mediation of big data analytics (BDA) capability in the association of contractual and relational governance with decision-making performance. Furthermore, moderating role of data-driven culture in the relationship of BDA capability and decision-making performance is examined. Data are collected from 108 Chinese firms engaged in big data-related activities. Structural equation modeling is employed to test the hypotheses. This study contributes towards the literature on big data management and governance mechanisms, by establishing the relationship of decision-making performance with big data contractual and relational governance directly and through the mediation of BDA capabilities. It also contributes towards knowledge based dynamic capabilities (KBDCs) view of firms, arguing that dynamic capabilities such as BDA capabilities can be influenced through knowledge sources and activities. We add to the discussions on whether contractual and relational governance are alternatives or they complement each other, by establishing the moderating role of big data relational governance in the relationship of contractual governance and decision-making performance. Finally, we argue that social capital can enhance KBDCs through contractual and relational governance in big data context.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120315","JIR":8.593,"SJR":2.226},{"Title":"Big data and firm marketing performance: Findings from knowledge-based view","author":"Shivam Gupta and Th\u00e9o Justy and Shampy Kamboj and Ajay Kumar and Eivind Kristoffersen","keywords":"Big data analytics, Artificial intelligence, Marketing performance, Knowledge-based view","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"A universal trend in advanced manufacturing countries is defining Industry 4.0, industrialized internet and future factories as a recent wave, which may transform the production and its related services. Further, big data analytics has emerged as a game changer in the business world due to its uses for increasing accuracy in decision-making and enhancing performance of sustainable industry 4.0 applications.\u00a0This study intends to emphasize on how to support Industry 4.0 with knowledge based view. For the same, a conceptual model is framed and presented with essential components that are required for a real world implementation. The study used qualitative analysis and was guided by a knowledge-based theoretical framework. Thematic analysis resulted in the identification of a number of emergent categories. Key findings highlight significant gaps in conventional decision-making systems and demonstrate how big data enhances firms\u2019 strategic and operational decisions as well as facilitates informational access for improved marketing performance. The resulting proposed model can provide managers with a reference point for using big data to line up firms\u2019 activities for more effective marketing efforts and presents a conceptual basis for further empirical studies in this area.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.120986","JIR":8.593,"SJR":2.226},{"Title":"Evaluating the impact of big data analytics usage on the decision-making quality of organizations","author":"Lei Li and Jiabao Lin and Ye Ouyang and Xin (Robert) Luo","keywords":"Big data analytics usage, Data analytics capabilities, Decision-making quality, Agricultural firms","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Big data initiatives are critical for transforming traditional organizational decision making into data-driven decision making. However, prior information systems research has not paid enough attention to the impact of big data analytics usage on decision-making quality. Drawing on the dynamic capability theory, this study investigated the impact of big data analytics usage on decision-making quality and tested the mediating effect of data analytics capabilities. We collected data from 240 agricultural firms in China. The empirical results showed that big data analytics usage had a positive impact on decision-making quality and that data analytics capabilities played a mediating role in the relationship between big data analytics usage and decision-making quality. Hence, firms should not only popularize big data analytics usage in their business activities but also take measures to improve their data analytics capabilities, which will improve their decision-making quality toward competitive advantages.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121355","JIR":8.593,"SJR":2.226},{"Title":"Sowing the seeds of value? Persuasive practices and the embedding of big data analytics","author":"Jeffrey Hughes and Kirstie Ball","keywords":"Big data analytics, Persuasion, Practice, Capabilities, Value","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"This paper draws on data from three organisational case studies and expert interviews to propose that persuasive practices are the precursors and enablers of analytical capability development. A bundle of seven practices was identified and observed to bridge multiple gaps between technical and non-technical colleagues on big data analytics (BDA) projects. The deployment of these practices varied according to the level of BDA maturity and featured a host of socio-material elements. This paper complements existing technical case studies with a fine-grained qualitative account of the managerial and human elements of BDA implementation. Effective deployment of persuasive practices potentially both embeds the benefits and mitigates the risks of BDA, sowing the seeds of many different forms of value.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120300","JIR":8.593,"SJR":2.226},{"Title":"Influence of big data adoption on manufacturing companies' performance: An integrated DEMATEL-ANFIS approach","author":"Elaheh Yadegaridehkordi and Mehdi Hourmand and Mehrbakhsh Nilashi and Liyana Shuib and Ali Ahani and Othman Ibrahim","keywords":"Big data, Firm performance, Manufacturing companies, DEMATEL, ANFIS","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Big Data is one of the recent technological advances with the strong applicability in almost every industry, including manufacturing. However, despite business opportunities offered by this technology, its adoption is still in early stage in many industries. Thus, this study aimed to identify and rank the significant factors influencing adoption of big data and in turn to predict the influence of big data adoption on manufacturing companies' performance using a hybrid approach of decision-making trial and evaluation laboratory (DEMATEL)- adaptive neuro-fuzzy inference systems (ANFIS). This study identified the critical adoption factors from literature review and categorized them into technological, organizational and environmental dimensions. Data was collected from 234 industrial managers who were involved in the decision-making process regarding IT procurement in Malaysian manufacturing companies. Research results showed that technological factors (perceived benefits, complexity, technology resources, big data quality and integration) have the highest influence on the big data adoption and firms' performance. This study is one of the pioneers in using DEMATEL-ANFIS approach in the big data adoption context. In addition to the academic contribution, findings of this study can hopefully assist manufacturing industries, big data service providers, and governments to precisely focus on vital factors found in this study in order to improve firm performance by adopting big data.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2018.07.043","JIR":8.593,"SJR":2.226},{"Title":"Role of institutional pressures and resources in the adoption of big data analytics powered artificial intelligence, sustainable manufacturing practices and circular economy capabilities","author":"Surajit Bag and Jan Ham Christiaan Pretorius and Shivam Gupta and Yogesh K. Dwivedi","keywords":"Big data, Artificial intelligence, Industry 4.0, Circular economy, Sustainable manufacturing","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"ABSTRACT\nThe significance of big data analytics-powered artificial intelligence has grown in recent years. The literature indicates that big data analytics-powered artificial intelligence has the ability to enhance supply chain performance, but there is limited research concerning the reasons for which firms engaging in manufacturing activities adopt big data analytics-powered artificial intelligence. To address this gap, our study employs institutional theory and resource-based view theory to elucidate the way in which automotive firms configure tangible resources and workforce skills to drive technological enablement and improve sustainable manufacturing practices and furthermore develop circular economy capabilities. We tested the research hypothesis using primary data collected from 219 automotive and allied manufacturing companies operating in South Africa. The contribution of this work lies in the statistical validation of the theoretical framework, which provides insight regarding the role of institutional pressures on resources and their effects on the adoption of big data analytics-powered artificial intelligence, and how this affects sustainable manufacturing and circular economy capabilities under the moderating effects of organizational flexibility and industry dynamism.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120420","JIR":8.593,"SJR":2.226},{"Title":"Tension in big data using machine learning: Analysis and applications","author":"Huamao Wang and Yumei Yao and Said Salhi","keywords":"Big data, Machine learning, Data size, Prediction accuracy, Social media","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The access of machine learning techniques in popular programming languages and the exponentially expanding big data from social media, news, surveys, and markets provide exciting challenges and invaluable opportunities for organizations and individuals to explore implicit information for decision making. Nevertheless, the users of machine learning usually find that these sophisticated techniques could incur a high level of tensions caused by the selection of the appropriate size of the training data set among other factors. In this paper, we provide a systematic way of resolving such tensions by examining practical examples of predicting popularity and sentiment of posts on Twitter and Facebook, blogs on Mashable, news on Google and Yahoo, the US house survey, and Bitcoin prices. Interesting results show that for the case of big data, using around 20% of the full sample often leads to a better prediction accuracy than opting for the full sample. Our conclusion is found to be consistent across a series of experiments. The managerial implication is that using more is not necessarily the best and users need to be cautious about such an important sensitivity as the simplistic approach may easily lead to inferior solutions with potentially detrimental consequences.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2020.120175","JIR":8.593,"SJR":2.226},{"Title":"Circular economy and big data analytics: A stakeholder perspective","author":"Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and Ernesto D.R. {Santiba\u00f1ez Gonzalez}","keywords":"Circular economy, Big data, Stakeholder theory, Relational view, Supply chain management, Sustainability","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The business concept of the circular economy (CE) has gained significant momentum among practitioners and researchers alike. However, successful adoption and implementation of this paradigm of managing business remains a challenge. In this article, we build a case for utilizing big data analytics (BDA) as a fundamental basis for informed and data driven decision making in supply chain networks supporting CE. We view this from a stakeholder perspective and argue that a collaborative association among all supply chain members can positively affect CE implementation. We propose a model highlighting the facilitating role of big data analytics for achieving shared sustainability goals. The model is based on integrating thematic categories coming out of 10 semi-structured interviews with key position holders in industry. We argue that mutual support and coordination driven by a stakeholder perspective coupled with holistic information processing and sharing along the entire supply chain network can effectively create a basis for achieving the triple bottom line of economic, ecological and social benefits. The proposed model is useful for managers in that it provides a reference point for aligning activities with the circular economy paradigm. The conceptual model provides a theoretical basis for future empirical research in this domain.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2018.06.030","JIR":8.593,"SJR":2.226},{"Title":"Big Data sources and methods for social and economic analyses","author":"Desamparados Blazquez and Josep Domenech","keywords":"Big Data architecture, Forecasting, Nowcasting, Data lifecycle, Socio-economic data, Non-traditional data sources, Non-traditional analysis methods","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The Data Big Bang that the development of the ICTs has raised is providing us with a stream of fresh and digitized data related to how people, companies and other organizations interact. To turn these data into knowledge about the underlying behavior of the social and economic agents, organizations and researchers must deal with such amount of unstructured and heterogeneous data. Succeeding in this task requires to carefully plan and organize the whole process of data analysis taking into account the particularities of the social and economic analyses, which include the wide variety of heterogeneous sources of information and a strict governance policy. Grounded on the data lifecycle approach, this paper develops a Big Data architecture that properly integrates most of the non-traditional information sources and data analysis methods in order to provide a specifically designed system for forecasting social and economic behaviors, trends and changes.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2017.07.027","JIR":8.593,"SJR":2.226},{"Title":"Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations","author":"Yichuan Wang and LeeAnn Kung and Terry Anthony Byrd","keywords":"Big data analytics, Big data analytics architecture, Big data analytics capabilities, Business value of information technology (IT), Health care","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"To date, health care industry has not fully grasped the potential benefits to be gained from big data analytics. While the constantly growing body of academic research on big data analytics is mostly technology oriented, a better understanding of the strategic implications of big data is urgently needed. To address this lack, this study examines the historical development, architectural design and component functionalities of big data analytics. From content analysis of 26 big data implementation cases in healthcare, we were able to identify five big data analytics capabilities: analytical capability for patterns of care, unstructured data analytical capability, decision support capability, predictive capability, and traceability. We also mapped the benefits driven by big data analytics in terms of information technology (IT) infrastructure, operational, organizational, managerial and strategic areas. In addition, we recommend five strategies for healthcare organizations that are considering to adopt big data analytics technologies. Our findings will help healthcare organizations understand the big data analytics capabilities and potential benefits and support them seeking to formulate more effective data-driven analytics strategies.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2015.12.019","JIR":8.593,"SJR":2.226},{"Title":"Governance of big data collaborations: How to balance regulatory compliance and disruptive innovation","author":"Tijs {van den Broek} and Anne Fleur {van Veenstra}","keywords":"Disruptive innovation, Data protection regulation, Big data, Governance, Inter-organizational collaboration","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Big data is an important driver of disruptive innovation that may increase organizations' competitive advantage. To create innovative data combinations and decrease investments, big data is often shared among organizations, crossing organizational boundaries. However, these big data collaborations need to balance disruptive innovation and compliance to a strict data protection regime in the EU. This paper investigates how inter-organizational big data collaborations arrange and govern their activities in the context of this dilemma. We conceptualize big data as inter-organizational systems and build on IS and Organization Theory literature to develop four archetypical governance arrangements: Market, Hierarchy, Bazaar and Network. Subsequently, these arrangements are investigated in four big data collaboration use cases. The contributions of this study to literature are threefold. First, we conceptualize the organization behind big data collaborations as IOS governance. Second, we show that the choice for an inter-organizational governance arrangement highly depends on the institutional pressure from regulation and the type of data that is shared. In this way, we contribute to the limited body of research on the antecedents of IOS governance. Last, we highlight with four use cases how the principles of big data, specifically data maximization, clash with the principles of EU data protection regulation. Practically, our study provides guidelines for IT and innovation managers how to arrange and govern the sharing of data among multiple organizations.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2017.09.040","JIR":8.593,"SJR":2.226},{"Title":"Big data analytics sentiment: US-China reaction to data collection by business and government","author":"Ryan C. LaBrie and Gerhard H. Steinke and Xiangmin Li and Joseph A. Cazier","keywords":"Big data ethics, Business data usage, Corporate data collection, Government data usage, Technology ethics, US-China similarities, US-China differences","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"As society continues its rapid change to a digitized individual, corporate, and government environment it is prudent for researchers to investigate the zeitgeist of the global citizenry. The technological changes brought about by big data analytics are changing the way we gather and view data. This big data analytics sentiment research examines how Chinese and American respondents may view big data collection and analytics differently. The paper follows with an analysis of reported attitudes toward possible viewpoints from each country on various big data analytics topics ranging from individual to business and governmental foci. Hofstede's cultural dimensions are used to inform and frame our research hypotheses. Findings suggest that Chinese and American perspectives differ on individual data values, with the Chinese being more open to data collection and analytic techniques targeted toward individuals. Furthermore, support is found that US respondents have a more favorable view of businesses' use of data analytics. Finally, there is a strong difference in the attitudes toward governmental use of data, where US respondents do not favor governmental big data analytics usage and the Chinese respondents indicated a greater acceptance of governmental data usage. These findings are helpful in better understanding appropriate technological change and adoption from a societal perspective. Specifically, this research provides insights for corporate business and government entities suggesting how they might adjust their approach to big data collection and management in order to better support and sustain their organization's services and products.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2017.06.029","JIR":8.593,"SJR":2.226},{"Title":"Systematic method for finding emergence research areas as data quality","author":"Babak Sohrabi and Ahmad Khalilijafarabad","keywords":"Data quality, Text mining, Science mapping, Data mining, Trend analysis","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The analysis of the transformation and changes in scientific disciplines has always been a critical path for policymakers and researchers. The current study examines the changes in the research areas of data and information quality (DIQ). The aim of this study was to detect different types of changes occurring in the scientific areas including birth, death, growth, decline, merge, and splitting. A model has been developed for this data mining. To test the model, all DIQ articles published in online scientific citation indexing service or Web of Science (WOS) between 1970 and 2016 were extracted and analyzed using the given model. The study is related to the Big Data as well as the integration methods in Big Data which is the most important area in DIQ. It is demonstrated that the first and second emerging research areas are sub-disciplines of entity resolution and record linkage. Accordingly, linkage and privacy are the first emerging research area and the entity resolution using ontology is the second in DIQ. This is followed by the social media issues and genetic related DIQ issues.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2018.08.003","JIR":8.593,"SJR":2.226},{"Title":"A technology delivery system for characterizing the supply side of technology emergence: Illustrated for Big Data & Analytics","author":"Ying Huang and Alan L. Porter and Scott W. Cunningham and Douglas K.R. Robinson and Jianhua Liu and Donghua Zhu","keywords":"Technology delivery system, Tech mining, Emerging technology, Big Data, Technology assessment, Impact assessment","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"While there is a general recognition that breakthrough innovation is non-linear and requires an alignment between producers (supply) and users (demand), there is still a need for strategic intelligence about the emerging supply chains of new technological innovations. This technology delivery system (TDS) is an updated form of the TDS model and provides a promising chain-link approach to the supply side of innovation. Building on early research into supply-side TDS studies, we present a systematic approach to building a TDS model that includes four phases: (1) identifying the macroeconomic and policy environment, including market competition, financial investment, and industrial policy; (2) specifying the key public and private institutions; (3) addressing the core technical complements and their owners, then tracing their interactions through information linkages and technology transfers; and (4) depicting the market prospects and evaluating the potential profound influences on technological change and social developments. Our TDS methodology is illustrated using the field of Big Data & Analytics (\u201cBDA\u201d).","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2017.09.012","JIR":8.593,"SJR":2.226},{"Title":"Sustainable resource allocation for power generation: The role of big data in enabling interindustry architectural innovation","author":"Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and Fiona Lettice and Sonal Choudhary","keywords":"Energy innovation, Interindustry architectural innovation, Sustainable energy, Fuel mix, Grey TOPSIS, grey linear programming","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Economic, social and environmental requirements make planning for a sustainable electricity generation mix a demanding endeavour. Technological innovation offers a range of renewable generation and energy management options which require fine tuning and accurate control to be successful, which calls for the use of large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria Decision Making (MCDM) to evaluate electricity generation options against technical, environmental and social criteria. Data incompleteness and redundancy, usual in large-scale datasets, as well as expert opinion ambiguity are dealt with using a comprehensive grey TOPSIS model. We used evaluation scores to develop a multi-objective optimization model to maximize the technical, environmental and social utility of the electricity generation mix and to enable a larger role for innovative technologies. Demand uncertainty was handled with an interval range and we developed our problem with multi-objective grey linear programming (MOGLP). Solving the mathematical model provided us with the electricity generation mix for every 5\u202fmin of the period under study. Our results indicate that nuclear and renewable energy options, specifically wind, solar, and hydro, but not biomass energy, perform better against all criteria indicating that interindustry architectural innovation in the power generation mix is key to sustainable UK electricity production and supply.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2018.04.031","JIR":8.593,"SJR":2.226},{"Title":"Green innovation and organizational performance: The influence of big data and the moderating role of management commitment and HR practices","author":"Abdul-Nasser El-Kassar and Sanjay Kumar Singh","keywords":"Green innovation, Corporate environmental ethics, Large scale data, Human resource practices, Management commitment, Environmental and economic performance","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Faced with internal and external pressure to adapt and implement environmental friendly business activities, it is becoming crucial for firms to identify practices that enhance their competitive advantage, economic, and environmental performance. Green innovation, green technologies, and the implementation of green supply chain management are examples of such practices. Green innovation and the adoption of the combination of green product innovation and green process innovation involve reduction in consumption of energy and pollution emission, recycling of wastes, sustainable utilization of resources, and green product designs. Although the extent research in this area is substantial, research on the importance of considering corporate environmental ethics, stakeholders view of green product, and demand for green products as drivers of green innovation must be conducted. Moreover, the role of large scale data, management commitment, and human resource practices play to overcome the technological challenges, achieve competitive advantage, and enhance the economic and environmental performance have yet to be addressed. This paper develops and tests a holistic model that depicts and examines the relationships among green innovation, its drivers, as well as factors that help overcome the technological challenges and influence the performance and competitive advantage of the firm. This paper is among the first works to deal with such a complex framework which considers the interrelationships among numerous constructs and their effects on competitive advantage as well as overall organizational performance. A questionnaire was designed to measure the influence of green innovation adoption\/implementation and its drivers on performance and competitive advantage while taking into consideration the impact of management commitment and HR practices, as well as the use of large data on these relationships. Data collected from a sample of 215 respondents working in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC) were used to test the proposed relationships. The proposed model proved to be fit. The hypotheses were supported, and implications were discussed.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2017.12.016","JIR":8.593,"SJR":2.226},{"Title":"Technology in the 21st century: New challenges and opportunities","author":"Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang","keywords":"Business intelligence, Big data, Big data analytics, Advanced techniques, Decision-making","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Although big data, big data analytics (BDA) and business intelligence have attracted growing attention of both academics and practitioners, a lack of clarity persists about how BDA has been applied in business and management domains. In reflecting on Professor Ayre's contributions, we want to extend his ideas on technological change by incorporating the discourses around big data, BDA and business intelligence. With this in mind, we integrate the burgeoning but disjointed streams of research on big data, BDA and business intelligence to develop unified frameworks. Our review takes on both technical and managerial perspectives to explore the complex nature of big data, techniques in big data analytics and utilisation of big data in business and management community. The advanced analytics techniques appear pivotal in bridging big data and business intelligence. The study of advanced analytics techniques and their applications in big data analytics led to identification of promising avenues for future research.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2018.06.009","JIR":8.593,"SJR":2.226},{"Title":"Data intelligence and analytics: A bibliometric analysis of human\u2013Artificial intelligence in public sector decision-making effectiveness","author":"Assunta {Di Vaio} and Rohail Hassan and Claude Alavoine","keywords":"Ambidexterity, Industry 4.0, Business intelligence, Big data, Intellectual capital, Human intellect, Accountability and performance","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"This study investigates the literary corpus of the role and potential of data intelligence and analytics through the lenses of artificial intelligence (AI), big data, and the human\u2013AI interface to improve overall decision-making processes. It investigates how data intelligence and analytics improve decision-making processes in the public sector. A bibliometric analysis of a database containing 161 English-language articles published between 2017 and 2021 is performed, providing a map of the knowledge produced and disseminated in previous studies. It provides insights into key topics, citation patterns, publication activities, the status of collaborations between contributors over past studies, aggregated data intelligence, and analytics research contributions. The study provides a retrospective review of published content in the field of data intelligence and analytics. The findings indicate that field research has been concentrated mainly on emerging technologies' intelligence capabilities rather than on human\u2013artificial intelligence in decision-making performance in the public sector. This study extends an ambidexterity theory in decision support, which enlightens how this ambidexterity can be encouraged and how it affects decision outcomes. The study emphasises the importance of the public sector adoption of data intelligence and analytics, as well as its efficiency. Furthermore, this study expands how researchers and practitioners interpret and understand data intelligence and analytics, AI, and big data for effective public sector decision-making.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121201","JIR":8.593,"SJR":2.226},{"Title":"Data science roadmapping: An architectural framework for facilitating transformation towards a data-driven organization","author":"Kerem Kayabay and Mert Onuralp G\u00f6kalp and Ebru G\u00f6kalp and P. {Erhan Eren} and Altan Ko\u00e7yi\u011fit","keywords":"Technology roadmapping, Technology management, Data science, Digital transformation, Data-driven organization, Big data","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Leveraging data science can enable businesses to exploit data for competitive advantage by generating valuable insights. However, many industries cannot effectively incorporate data science into their business processes, as there is no comprehensive approach that allows strategic planning for organization-wide data science efforts and data assets. Accordingly, this study explores the Data Science Roadmapping (DSR) to guide organizations in aligning their business strategies with data-related, technological, and organizational resources. The proposed approach is built on the widely adopted technology roadmapping framework and customizes its context, architecture, and process by synthesizing data science, big data, and data-driven organization literature. Based on industry collaborations, the framework provides a hybrid and agile methodology comprising the recommended steps. We applied DSR with a research group with sector experience to create a comprehensive data science roadmap to validate and refine the framework. The results indicate that the framework facilitates DSR initiatives by creating a comprehensive roadmap capturing strategy, data, technology, and organizational perspectives. The contemporary literature illustrates prebuilt roadmaps to help businesses become data-driven. However, becoming data-driven also necessitates significant social change toward openness and trust. The DSR initiative can facilitate this social change by opening communication channels, aligning perspectives, and generating consensus among stakeholders.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121264","JIR":8.593,"SJR":2.226},{"Title":"Evaluating the critical success factors of data intelligence implementation in the public sector using analytical hierarchy process","author":"Mohammad I. Merhi","keywords":"Data intelligence, Systems implementation, Data analytics, Success factors, Public sector, AHP","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"This study aims to fill a gap in the literature by identifying, defining, and evaluating the critical success factors that impact the implementation of data intelligence in the public sector. Fourteen factors were identified, and then divided into three categories: organization, process, and technology. We used the analytical hierarchy process, a quantitative method of decision-making, to evaluate the importance of the factors presented in the study using data collected from nine experts. The results showed that technology, as a category, is the most important. The analysis also indicated that project management, information systems & data, and data quality are the most important factors among all fourteen critical success factors. We discuss the implications of the analysis for practitioners and researchers in the paper.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121180","JIR":8.593,"SJR":2.226},{"Title":"The science of statistics versus data science: What is the future?","author":"Hossein Hassani and Christina Beneki and Emmanuel Sirimal Silva and Nicolas Vandeput and Dag \u00d8ivind Madsen","keywords":"Perspective, Science, Statistics, Data science, Similarities, Differences, Pragmatism","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The importance and relevance of the discipline of statistics with the merits of the evolving field of data science continues to be debated in academia and industry. Following a narrative literature review with over 100 scholarly and practitioner-oriented publications from statistics and data science, this article generates a pragmatic perspective on the relationships and differences between statistics and data science. Some data scientists argue that statistics is not necessary for data science as statistics delivers simple explanations and data science delivers results. Therefore, this article aims to stimulate debate and discourse among both academics and practitioners in these fields. The findings reveal the need for stakeholders to accept the inherent advantages and disadvantages within the science of statistics and data science. The science of statistics enables data science (aiding its reliability and validity), and data science expands the application of statistics to Big Data. Data scientists should accept the contribution and importance of statistics and statisticians must humbly acknowledge the novel capabilities made possible through data science and support this field of study with their theoretical and pragmatic expertise. Indeed, the emergence of data science does pose a threat to statisticians, but the opportunities for synergies are far greater.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.121111","JIR":8.593,"SJR":2.226},{"Title":"Towards a business analytics capability for the circular economy","author":"Eivind Kristoffersen and Patrick Mikalef and Fenna Blomsma and Jingyue Li","keywords":"Digital circular economy, Sustainability, Big data analytics, Competitive advantage, Resource-based view, Expert interviews","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"Digital technologies are growing in importance for accelerating firms\u2019 circular economy transition. However, so far, the focus has primarily been on the technical aspects of implementing these technologies with limited research on the organizational resources and capabilities required for successfully leveraging digital technologies for circular economy. To address this gap, this paper explores the business analytics resources firms should develop and how these should be orchestrated towards a firm-wide capability. The paper proposes a conceptual model highlighting eight business analytics resources that, in combination, build a business analytics capability for the circular economy and how this relates to firms\u2019 circular economy implementation, resource orchestration capability, and competitive performance. The model is based on the results of a thematic analysis of 15 semi-structured expert interviews with key positions in industry. Our approach is informed by and further develops, the theory of the resource-based view and the resource orchestration view. Based on the results, we develop a deeper understanding of the importance of taking a holistic approach to business analytics when leveraging data and analytics towards a more efficient and effective digital-enabled circular economy, the smart circular economy.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2021.120957","JIR":8.593,"SJR":2.226},{"Title":"Linking circular economy and digitalisation technologies: A systematic literature review of past achievements and future promises","author":"Chetna Chauhan and Vinit Parida and Amandeep Dhir","keywords":"Circular economy, Sustainability, Product-service system (PSS), Circular business model, Artificial intelligence, Internet of things","journal":"TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE","abstract":"The circular economy (CE) has the potential to capitalise upon emerging digital technologies, such as big data, artificial intelligence (AI), blockchain and the Internet of things (IoT), amongst others. These digital technologies combined with business model innovation are deemed to provide solutions to myriad problems in the world, including those related to circular economy transformation. Given the societal and practical importance of CE and digitalisation, last decade has witnessed a significant increase in academic publication on these topics. Therefore, this study aims to capture the essence of the scholarly work at the intersection of the CE and digital technologies. A detailed analysis of the literature based on emerging themes was conducted with a focus on illuminating the path of CE implementation. The results reveal that IoT and AI play a key role in the transition towards the CE. A multitude of studies focus on barriers to digitalisation-led CE transition and highlight policy-related issues, the lack of predictability, psychological issues and information vulnerability as some important barriers. In addition, product-service system (PSS) has been acknowledged as an important business model innovation for achieving the digitalisation enabled CE. Through a detailed assessment of the existing literature, a viable systems-based framework for digitalisation enabled CE has been developed which show the literature linkages amongst the emerging research streams and provide novel insights regarding the realisation of CE benefits.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.techfore.2022.121508","JIR":8.593,"SJR":2.226},{"Title":"The perils of working with big data, and a SMALL checklist you can use to recognize them","author":"Scott A. Brave and R. Andrew Butters and Michael Fogarty","keywords":"Big data, Data analysis, Economic forecasting, Selection bias, Reporting lags, High-frequency data, Real-time forecasts, Leading indicator","journal":"BUSINESS HORIZONS","abstract":"The use of big data to help explain fluctuations in the broader economy and key business performance indicators is now so commonplace that in some instances it has even begun to rival more traditional measures. Big data sources can very often provide advantages when compared with these more traditional data sources, but with these advantages also come potential pitfalls. We lay out a checklist called SMALL that we have developed in order to help interested parties as they navigate the big data minefield. Based on a set of five questions, the SMALL checklist should help users of big data draw justifiable conclusions and avoid making mistakes in matters of interpretation. To demonstrate, we provide several case studies that demonstrate the subtle nuances of several of these new big data sets and show how the problems they face often closely relate to age-old concerns that more traditional data sources are also forced to tackle.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bushor.2021.06.004","JIR":6.361,"SJR":2.174},{"Title":"Big data: Dimensions, evolution, impacts, and challenges","author":"In Lee","keywords":"Big data, Internet of things, Data analytics, Sentiment analysis, Social network analysis, Web analytics","journal":"BUSINESS HORIZONS","abstract":"Big data represents a new technology paradigm for data that are generated at high velocity and high volume, and with high variety. Big data is envisioned as a game changer capable of revolutionizing the way businesses operate in many industries. This article introduces an integrated view of big data, traces the evolution of big data over the past 20 years, and discusses data analytics essential for processing various structured and unstructured data. This article illustrates the application of data analytics using merchant review data. The impacts of big data on key business performances are then evaluated. Finally, six technical and managerial challenges are discussed.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bushor.2017.01.004","JIR":6.361,"SJR":2.174},{"Title":"Uncovering the message from the mess of big data","author":"Neil T. Bendle and Xin (Shane) Wang","keywords":"Big data, User-Generated content, Latent Dirichlet Allocation, Topic modeling, Market research, Qualitative data","journal":"BUSINESS HORIZONS","abstract":"User-generated content, such as online product reviews, is a valuable source of consumer insight. Such unstructured big data is generated in real-time, is easily accessed, and contains messages consumers want managers to hear. Analyzing such data has potential to revolutionize market research and competitive analysis, but how can the messages be extracted? How can the vast amount of data be condensed into insights to help steer businesses\u2019 strategy? We describe a non-proprietary technique that can be applied by anyone with statistical training. Latent Dirichlet Allocation (LDA) can analyze huge amounts of text and describe the content as focusing on unseen attributes in a specific weighting. For example, a review of a graphic novel might be analyzed to focus 70% on the storyline and 30% on the graphics. Aggregating the content from numerous consumers allows us to understand what is, collectively, on consumers\u2019 minds, and from this we can infer what consumers care about. We can even highlight which attributes are seen positively or negatively. The value of this technique extends well beyond the CMO's office as LDA can map the relative strategic positions of competitors where they matter most: in the minds of consumers.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bushor.2015.10.001","JIR":6.361,"SJR":2.174},{"Title":"Machine learning for enterprises: Applications, algorithm selection, and challenges","author":"In Lee and Yong Jae Shin","keywords":"Machine learning, Artificial intelligence, Deep learning, Big data, Neural networks, Chatbot, Innovation capability, Resources and capabilities","journal":"BUSINESS HORIZONS","abstract":"Machine learning holds great promise for lowering product and service costs, speeding up business processes, and serving customers better. It is recognized as one of the most important application areas in this era of unprecedented technological development, and its adoption is gaining momentum across almost all industries. In view of this, we offer a brief discussion of categories of machine learning and then present three types of machine-learning usage at enterprises. We then discuss the trade-off between the accuracy and interpretability of machine-learning algorithms, a crucial consideration in selecting the right algorithm for the task at hand. We next outline three cases of machine-learning development in financial services. Finally, we discuss challenges all managers must confront in deploying machine-learning applications.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bushor.2019.10.005","JIR":6.361,"SJR":2.174},{"Title":"What the hack? A growth hacking taxonomy and practical applications for firms","author":"Ren\u00e9 Bohnsack and Meike Malena Liesner","keywords":"Growth hacking, Digital transformation, Lean startup, Digital marketing, Big data","journal":"BUSINESS HORIZONS","abstract":"As companies become increasingly digital, growth hacking emerged as a new way of scaling businesses. While the term is fashionable in business, many executives remain confused about the concept. Even if firms have an idea of what growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution gap. Our article assists firms by bridging the growth hacking strategy-execution gap. First, we provide a growth hacking framework and deconstruct its building blocks: marketing, data analysis, coding, and the lean startup philosophy. We then present a taxonomy of 34 growth hacking patterns along the customer lifecycle of acquisition, activation, revenue, retention, and referral; categorize them on the two dimensions of resource intensity and time lag; and provide an example of how to apply the taxonomy in the case of a fitness application. Finally, we discuss seven opportunities and challenges of growth hacking that firms should keep in mind.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bushor.2019.09.001","JIR":6.361,"SJR":2.174},{"Title":"Managerial work in the realm of the digital universe: The role of the data triad","author":"Vijay Khatri","keywords":"Analytics, Big data, Managerial decision making, Managerial work, Digital universe","journal":"BUSINESS HORIZONS","abstract":"With the explosion of the digital universe, it is becoming increasingly important to understand how organizational decision making (i.e., the business-oriented perspective) is intertwined with an understanding of enterprise data assets (i.e., the data-oriented perspective). This article first compares the business- and data-oriented perspectives to describe how the two views mesh with each other. It then presents three elements in the data-oriented perspective that are collectively referred to as the data triad: (1) use, (2) design and storage, and (3) processes and people. In describing the data triad, this article highlights practices, architectural techniques, and example tools that are used to manage, access, analyze, and deliver data. By presenting different elements of the data-oriented perspective, this article broadly and concretely describes the data triad and how it can play a role in the redefined scope of work for data-driven business managers.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bushor.2016.06.001","JIR":6.361,"SJR":2.174},{"Title":"Big data management capabilities in the hospitality sector: Service innovation and customer generated online quality ratings","author":"Saqib Shamim and Yumei Yang and Najam Ul Zia and Mahmood Hussain Shah","keywords":"Big data management, Dynamic capabilities, Service innovation, Knowledge creation, Customer generated online quality rating, Hospitality","journal":"COMPUTERS IN HUMAN BEHAVIOR","abstract":"Despite the wide usage of big data in tourism and the hospitality sector, little research has been done to understand the role of organizations\u2019 capability of managing big data in value creation. This study bridges this gap by investigating how big data management capabilities lead to service innovation and high online quality ratings. Instead of treating big data management as a whole, we access big data management capabilities at the strategic and operational level. Using a sample of 202 hotels in Pakistan, we collected the primary data for big data capabilities, knowledge creation and service innovation; the secondary data about quality rating were collected from Booking.com. Structural equation modelling through SmartPLS was used for data analysis. The results indicated that big data management capabilities lead to high online quality ratings through the mediation of knowledge creation and service innovation. We contribute to the current literature by empirically testing how strategic level big data capabilities enable the firm to add value in innovativeness and positive online quality ratings through acquiring, contextualizing, experimenting and applying big data.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.chb.2021.106777","JIR":6.829,"SJR":2.108},{"Title":"Social media big data analytics: A survey","author":"Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed","keywords":"Big data, Social media, Machine learning, Analytics","journal":"COMPUTERS IN HUMAN BEHAVIOR","abstract":"Big data analytics has recently emerged as an important research area due to the popularity of the Internet and the advent of the Web 2.0 technologies. Moreover, the proliferation and adoption of social media applications have provided extensive opportunities and challenges for researchers and practitioners. The massive amount of data generated by users using social media platforms is the result of the integration of their background details and daily activities. This enormous volume of generated data known as \u201cbig data\u201d has been intensively researched recently. A review of the recent works is presented to obtain a broad perspective of the social media big data analytics research topic. We classify the literature based on important aspects. This study also compares possible big data analytics techniques and their quality attributes. Moreover, we provide a discussion on the applications of social media big data analytics by highlighting the state-of-the-art techniques, methods, and the quality attributes of various studies. Open research challenges in big data analytics are described as well.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.chb.2018.08.039","JIR":6.829,"SJR":2.108},{"Title":"Consumer interaction with cutting-edge technologies: Implications for future research","author":"Nisreen Ameen and Sameer Hosany and Ali Tarhini","keywords":"Consumer interaction, Cutting-edge technologies, Artificial intelligence, Virtual reality and augmented reality, Robotics, Wearable technology, Big data analytics","journal":"COMPUTERS IN HUMAN BEHAVIOR","abstract":"This article provides an overview of extant literature addressing consumer interaction with cutting-edge technologies. Six focal cutting-edge technologies are identified: artificial intelligence, augmented reality, virtual reality, wearable technology, robotics and big data analytics. Our analysis shows research on consumer interaction with cutting-edge technologies is at a nascent stage, and there are several gaps requiring attention. To further advance knowledge, our article offers avenues for future interdisciplinary research addressing implications of consumer interaction with cutting-edge technologies. More specifically, we propose six main areas for future research namely: rethinking consumer behaviour models, identifying behavioural differences among different generations of consumers, understanding how consumers interact with automated services, ethics, privacy and the blackbox, consumer security concerns and consumer interaction with new-age technologies during and after a major global crisis such as the COVID-19 pandemic.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.chb.2021.106761","JIR":6.829,"SJR":2.108},{"Title":"Exploring Determinants of Semantic Web Technology Adoption from IT Professionals' Perspective: Industry Competition, Organization Innovativeness, and Data Management Capability","author":"Dan J. Kim and John Hebeler and Victoria Yoon and Fred Davis","keywords":"Semantic web, IT professionals' perspective technology adoption, Technology-organization-environment framework, Innovation diffusion theory","journal":"COMPUTERS IN HUMAN BEHAVIOR","abstract":"The scale and complexity of big data quickly exceed the reach of direct human comprehension and increasingly require machine assistance to semantically analyze, organize, and interpret vast and diverse sources of big data in order to unlock its strategic value. Due to its volume, velocity, variety, and veracity, big data integration challenges overwhelm traditional integration approaches leaving many integration possibilities out of reach. Unlocking the value of big data requires innovative technology. Organizations must have the innovativeness and data capability to adopt the technology and harness its potential value. The Semantic Web (SW) technology has demonstrated its potential for integrating big data and has become important technology for tackling big data. Despite its importance to manage big data, little research has examined the determinants affecting SW adoption. Drawing upon the technology\u2013organization\u2013environment framework as a theory base, this study develops a research model explaining the factors affecting the adoption of SW technology from IT professionals' perspective, specifically in the context of corporate computing enterprises. We validate the proposed model using a set of empirical data collected from IT professionals including IT managers, system architects, software developers, and web developers. The findings suggest that perceived usefulness, perceived ease of use, organization's innovativeness, organization's data capability, and applicability to data management are important drivers of SW adoption. This study provides new insights on theories of organizational IT adoption from IT professionals' perspectives tailored to the context of SW technology.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.chb.2018.04.014","JIR":6.829,"SJR":2.108},{"Title":"Data quality management, data usage experience and acquisition intention of big data analytics","author":"Ohbyung Kwon and Namyeon Lee and Bongsik Shin","keywords":"Big data analytics, Resource-based view, Data quality management, IT capability, Data usage","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Big data analytics associated with database searching, mining, and analysis can be seen as an innovative IT capability that can improve firm performance. Even though some leading companies are actively adopting big data analytics to strengthen market competition and to open up new business opportunities, many firms are still in the early stage of the adoption curve due to lack of understanding of and experience with big data. Hence, it is interesting and timely to understand issues relevant to big data adoption. In this study, a research model is proposed to explain the acquisition intention of big data analytics mainly from the theoretical perspectives of data quality management and data usage experience. Our empirical investigation reveals that a firm's intention for big data analytics can be positively affected by its competence in maintaining the quality of corporate data. Moreover, a firm's favorable experience (i.e., benefit perceptions) in utilizing external source data could encourage future acquisition of big data analytics. Surprisingly, a firm's favorable experience (i.e., benefit perceptions) in utilizing internal source data could hamper its adoption intention for big data analytics.","year":2014,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2014.02.002","JIR":14.098,"SJR":2.77},{"Title":"Management theory and big data literature: From a review to a research agenda","author":"Paula {de Camargo Fiorini} and Bruno Michel {Roman Pais Seles} and Charbel Jose {Chiappetta Jabbour} and Enzo {Barberio Mariano} and Ana Beatriz Lopes {de Sousa Jabbour}","keywords":"Big data, Big data analytics, Organizational theory, Firms\u2019 performance, Research agenda","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The purpose of this study is to enrich the existing state-of-the-art literature on the impact of big data on business growth by examining how dozens of organizational theories can be applied to enhance the understanding of the effects of big data on organizational performance. While the majority of management disciplines have had research dedicated to the conceptual discussion of how to link a variety of organizational theories to empirically quantified research topics, the body of research into big data so far lacks an academic work capable of systematising the organizational theories supporting big data domain. The three main contributions of this work are: (a) it addresses the application of dozens of organizational theories to big data research; (b) it offers a research agenda on how to link organizational theories to empirical research in big data; and (c) it foresees promising linkages between organizational theories and the effects of big data on organizational performance, with the aim of contributing to further research in this field. This work concludes by presenting implications for researchers and managers, and by highlighting intrinsic limitations of the research.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.07.005","JIR":14.098,"SJR":2.77},{"Title":"Using big data for co-innovation processes: Mapping the field of data-driven innovation, proposing theoretical developments and providing a research agenda","author":"Stefano Bresciani and Francesco Ciampi and Francesco Meli and Alberto Ferraris","keywords":"Big data, Co-innovation, Open innovation, Bibliometric analysis, Literature review","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"This is the first systematic literature review concerning the interconnections between big data (BD) and co-innovation. It uses BD as a common perspective of analysis as well as a concept aggregating different research streams (open innovation, co-creation and collaborative innovation). The review is based on the results of a bibliographic coupling analysis performed with 51 peer-reviewed papers published before the end of 2019. Three thematic clusters were discovered, which respectively focused on BD as a knowledge creation enabler within co-innovation contexts, BD as a driver of co-innovation processes based on customer engagement, and the impact of BD on co-innovation within service ecosystems. The paper theoretically argues that the use of BD, in addition to enhancing intentional and direct collaborative innovation processes, allows the development of passive and unintentional co-innovation that can be implemented through indirect relationships between the collaborative actors. This study also makes eleven unique research propositions concerning further theoretical developments and managerial implementations in the field of BD-driven co-innovation.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2021.102347","JIR":14.098,"SJR":2.77},{"Title":"Big data with cognitive computing: A review for the future","author":"Shivam Gupta and Arpan Kumar Kar and Abdullah Baabdullah and Wassan A.A. Al-Khowaiter","keywords":"Big data, Cognitive computing, Literature review, Resource based View, Institutional theory","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Analysis of data by humans can be a time-consuming activity and thus use of sophisticated cognitive systems can be utilized to crunch this enormous amount of data. Cognitive computing can be utilized to reduce the shortcomings of the concerns faced during big data analytics. The aim of the study is to provide readers a complete understanding of past, present and future directions in the domain big data and cognitive computing. A systematic literature review has been adopted for this study by using the Scopus, DBLP and Web of Science databases. The work done in the field of big data and cognitive computing is currently at the nascent stage and this is evident from the publication record. The characteristics of cognitive computing, namely observation, interpretation, evaluation and decision were mapped to the five V\u2019s of big data namely volume, variety, veracity, velocity and value. Perspectives which touch all these parameters are yet to be widely explored in existing literature.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.06.005","JIR":14.098,"SJR":2.77},{"Title":"Healthcare big data processing mechanisms: The role of cloud computing","author":"Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein Ghasemi and Arshad Badfar","keywords":"Cloud computing, Processing, Healthcare, Big data, Review","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Recently, patient safety and healthcare have gained high attention in professional and health policy-makers. This rapid growth causes generating a high amount of data, which is known as big data. Therefore, handling and processing of this data are attracted great attention. Cloud computing is one of the main choices for handling and processing of this type of data. But, as far as we know, the detailed review and deep discussion in this filed are very rare. Therefore, this paper reviews and discusses the recently introduced mechanisms in this field as well as providing a deep analysis of their applied mechanisms. Moreover, the drawbacks and benefits of the reviewed mechanisms have been discussed and the main challenges of these mechanisms are highlighted for developing more efficient healthcare big data processing techniques over cloud computing in the future.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.05.017","JIR":14.098,"SJR":2.77},{"Title":"Real-time big data processing for anomaly detection: A Survey","author":"Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran","keywords":"Real-time, Big data processing, Anomaly detection and machine learning algorithms","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The advent of connected devices and omnipresence of Internet have paved way for intruders to attack networks, which leads to cyber-attack, financial loss, information theft in healthcare, and cyber war. Hence, network security analytics has become an important area of concern and has gained intensive attention among researchers, off late, specifically in the domain of anomaly detection in network, which is considered crucial for network security. However, preliminary investigations have revealed that the existing approaches to detect anomalies in network are not effective enough, particularly to detect them in real time. The reason for the inefficacy of current approaches is mainly due the amassment of massive volumes of data though the connected devices. Therefore, it is crucial to propose a framework that effectively handles real time big data processing and detect anomalies in networks. In this regard, this paper attempts to address the issue of detecting anomalies in real time. Respectively, this paper has surveyed the state-of-the-art real-time big data processing technologies related to anomaly detection and the vital characteristics of associated machine learning algorithms. This paper begins with the explanation of essential contexts and taxonomy of real-time big data processing, anomalous detection, and machine learning algorithms, followed by the review of big data processing technologies. Finally, the identified research challenges of real-time big data processing in anomaly detection are discussed.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.08.006","JIR":14.098,"SJR":2.77},{"Title":"A Big Data system supporting Bosch Braga Industry 4.0 strategy","author":"Maribel Yasmina Santos and Jorge {Oliveira e S\u00e1} and Carina Andrade and Francisca {Vale Lima} and Eduarda Costa and Carlos Costa and Bruno Martinho and Jo\u00e3o Galv\u00e3o","keywords":"Big Data, Industry 4.0, Big Data analytics, Big Data architecture, Bosch","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"People, devices, infrastructures and sensors can constantly communicate exchanging data and generating new data that trace many of these exchanges. This leads to vast volumes of data collected at ever increasing velocities and of different variety, a phenomenon currently known as Big Data. In particular, recent developments in Information and Communications Technologies are pushing the fourth industrial revolution, Industry 4.0, being data generated by several sources like machine controllers, sensors, manufacturing systems, among others. Joining volume, variety and velocity of data, with Industry 4.0, makes the opportunity to enhance sustainable innovation in the Factories of the Future. In this, the collection, integration, storage, processing and analysis of data is a key challenge, being Big Data systems needed to link all the entities and data needs of the factory. Thereby, this paper addresses this key challenge, proposing and implementing a Big Data Analytics architecture, using a multinational organisation (Bosch Car Multimedia \u2013 Braga) as a case study. In this work, all the data lifecycle, from collection to analysis, is handled, taking into consideration the different data processing speeds that can exist in the real environment of a factory (batch or stream).","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.07.012","JIR":14.098,"SJR":2.77},{"Title":"Big data: From beginning to future","author":"Ibrar Yaqoob and Ibrahim Abaker Targio Hashem and Abdullah Gani and Salimah Mokhtar and Ejaz Ahmed and Nor Badrul Anuar and Athanasios V. Vasilakos","keywords":"Big data, Parallel and distributed computing, Cloud computing, Internet of things, Social media, Analytics","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Big data is a potential research area receiving considerable attention from academia and IT communities. In the digital world, the amounts of data generated and stored have expanded within a short period of time. Consequently, this fast growing rate of data has created many challenges. In this paper, we use structuralism and functionalism paradigms to analyze the origins of big data applications and its current trends. This paper presents a comprehensive discussion on state-of-the-art big data technologies based on batch and stream data processing. Moreover, strengths and weaknesses of these technologies are analyzed. This study also discusses big data analytics techniques, processing methods, some reported case studies from different vendors, several open research challenges, and the opportunities brought about by big data. The similarities and differences of these techniques and technologies based on important parameters are also investigated. Emerging technologies are recommended as a solution for big data problems.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.07.009","JIR":14.098,"SJR":2.77},{"Title":"Big data reduction framework for value creation in sustainable enterprises","author":"Muhammad Habib ur Rehman and Victor Chang and Aisha Batool and Teh Ying Wah","keywords":"Sustainable enterprises, Value creation, Big data analytics, Data reduction, Business model","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Value creation is a major sustainability factor for enterprises, in addition to profit maximization and revenue generation. Modern enterprises collect big data from various inbound and outbound data sources. The inbound data sources handle data generated from the results of business operations, such as manufacturing, supply chain management, marketing, and human resource management, among others. Outbound data sources handle customer-generated data which are acquired directly or indirectly from customers, market analysis, surveys, product reviews, and transactional histories. However, cloud service utilization costs increase because of big data analytics and value creation activities for enterprises and customers. This article presents a novel concept of big data reduction at the customer end in which early data reduction operations are performed to achieve multiple objectives, such as (a) lowering the service utilization cost, (b) enhancing the trust between customers and enterprises, (c) preserving privacy of customers, (d) enabling secure data sharing, and (e) delegating data sharing control to customers. We also propose a framework for early data reduction at customer end and present a business model for end-to-end data reduction in enterprise applications. The article further presents a business model canvas and maps the future application areas with its nine components. Finally, the article discusses the technology adoption challenges for value creation through big data reduction in enterprise applications.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.05.013","JIR":14.098,"SJR":2.77},{"Title":"The role of big data in smart city","author":"Ibrahim Abaker Targio Hashem and Victor Chang and Nor Badrul Anuar and Kayode Adewole and Ibrar Yaqoob and Abdullah Gani and Ejaz Ahmed and Haruna Chiroma","keywords":"Smart city, Big data, Internet of things, Smart environments, Cloud computing, Distributed computing","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The expansion of big data and the evolution of Internet of Things (IoT) technologies have played an important role in the feasibility of smart city initiatives. Big data offer the potential for cities to obtain valuable insights from a large amount of data collected through various sources, and the IoT allows the integration of sensors, radio-frequency identification, and Bluetooth in the real-world environment using highly networked services. The combination of the IoT and big data is an unexplored research area that has brought new and interesting challenges for achieving the goal of future smart cities. These new challenges focus primarily on problems related to business and technology that enable cities to actualize the vision, principles, and requirements of the applications of smart cities by realizing the main smart environment characteristics. In this paper, we describe the state-of-the-art communication technologies and smart-based applications used within the context of smart cities. The visions of big data analytics to support smart cities are discussed by focusing on how big data can fundamentally change urban populations at different levels. Moreover, a future business model of big data for smart cities is proposed, and the business and technological research challenges are identified. This study can serve as a benchmark for researchers and industries for the future progress and development of smart cities in the context of big data.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.05.002","JIR":14.098,"SJR":2.77},{"Title":"A hybrid IT framework for identifying high-quality physicians using big data analytics","author":"Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang","keywords":"Online healthcare communities, Physician identifying, Signaling theory, Machine learning, Topic modeling, Multi-criterion analysis","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Patients face difficulties identifying appropriate doctors owing to the sizeable quantity and uneven quality of information in online healthcare communities. In studying physician searches, researchers often focus on expertise similarity matches and sentiment analyses of reviews. However, the quality is often ignored. To address patients' information needs holistically, we propose a four-dimensional IT framework based on signaling theory. The model takes expertise knowledge, online reviews, profile descriptions (e.g., hospital reputation, number of patients, city) and service quality (e.g., response speed, interaction frequency, cost) as signals that distinguish high-quality physicians. It uses machine learning approaches to derive similarity matches and sentiment analysis. It also measures the relative importance of the signals by multi-criterion analysis and derives the physician rankings through the aggregated scores. Our study revealed that the proposed approach performs better compared with the other two recommend techniques. This research expands the boundary of signaling theory to healthcare management and enriches the literature on IT use and inter-organizational systems. The proposed IT model may improve patient care, alleviate the physician-patient relationship and reduce lawsuits against hospitals; it also has practical implications for healthcare management.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.01.005","JIR":14.098,"SJR":2.77},{"Title":"Data quality challenges in the UK social housing sector","author":"Caroline Duvier and Daniel Neagu and Crina Oltean-Dumbrava and Dave Dickens","keywords":"Social housing, Data quality","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The social housing sector has yet to realise the potential of high data quality. While other businesses, mainly in the private sector, reap the benefits of data quality, the social housing sector seems paralysed, as it is still struggling with recent government regulations and steep revenue reduction. This paper offers a succinct review of relevant literature on data quality and how it relates to social housing. The Housing and Development Board in Singapore offers a great example on how to integrate data quality initiatives in the social housing sector. Taking this example, the research presented in this paper is extrapolating cross-disciplinarily recommendations on how to implement data quality initiatives in social housing providers in the UK.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.09.008","JIR":14.098,"SJR":2.77},{"Title":"Analyzing data quality issues in research information systems via data profiling","author":"Otmane Azeroual and Gunter Saake and Eike Schallehn","keywords":"Current research information systems, CRIS, Research information systems, RIS, Research information, Data sources, Data quality, Extraction transformation load, ETL, Data analysis, Data profiling, Science system, Standardization","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The success or failure of a RIS in a scientific institution is largely related to the quality of the data available as a basis for the RIS applications. The most beautiful Business Intelligence (BI) tools (reporting, etc.) are worthless when displaying incorrect, incomplete, or inconsistent data. An integral part of every RIS is thus the integration of data from the operative systems. Before starting the integration process (ETL) of a source system, a rich analysis of source data is required. With the support of a data quality check, causes of quality problems can usually be detected. Corresponding analyzes are performed with data profiling to provide a good picture of the state of the data. In this paper, methods of data profiling are presented in order to gain an overview of the quality of the data in the source systems before their integration into the RIS. With the help of data profiling, the scientific institutions can not only evaluate their research information and provide information about their quality, but also examine the dependencies and redundancies between data fields and better correct them within their RIS.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2018.02.007","JIR":14.098,"SJR":2.77},{"Title":"Identifying influencers on social media","author":"Paul Harrigan and Timothy M. Daly and Kristof Coussement and Julie A. Lee and Geoffrey N. Soutar and Uwana Evers","keywords":"Influencers, Market mavens, Big data, Social media, Twitter","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The increased availability of social media big data has created a unique challenge for marketing decision-makers; turning this data into useful information. One of the significant areas of opportunity in digital marketing is influencer marketing, but identifying these influencers from big data sets is a continual challenge. This research illustrates how one type of influencer, the market maven, can be identified using big data. Using a mixed-method combination of both self-report survey data and publicly accessible big data, we gathered 556,150 tweets from 370 active Twitter users. We then proposed and tested a range of social-media-based metrics to identify market mavens. Findings show that market mavens (when compared to non-mavens) have more followers, post more often, have less readable posts, use more uppercase letters, use less distinct words, and use hashtags more often. These metrics are openly available from public Twitter accounts and could integrate into a broad-scale decision support system for marketing and information systems managers. These findings have the potential to improve influencer identification effectiveness and efficiency, and thus improve influencer marketing.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2020.102246","JIR":14.098,"SJR":2.77},{"Title":"Open data: Quality over quantity","author":"Shazia Sadiq and Marta Indulska","keywords":"Open data, Data quality","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Open data aims to unlock the innovation potential of businesses, governments, and entrepreneurs, yet it also harbours significant challenges for its effective use. While numerous innovation successes exist that are based on the open data paradigm, there is uncertainty over the data quality of such datasets. This data quality uncertainty is a threat to the value that can be generated from such data. Data quality has been studied extensively over many decades and many approaches to data quality management have been proposed. However, these approaches are typically based on datasets internal to organizations, with known metadata, and domain knowledge of the data semantics. Open data, on the other hand, are often unfamiliar to the user and may lack metadata. The aim of this research note is to outline the challenges in dealing with data quality of open datasets, and to set an agenda for future research to address this risk to deriving value from open data investments.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.01.003","JIR":14.098,"SJR":2.77},{"Title":"A review and future direction of agile, business intelligence, analytics and data science","author":"Deanne Larson and Victor Chang","keywords":"Agile methodologies, Business intelligence (BI), Analytics and big data, Lifecycle for BI and Big Data","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Agile methodologies were introduced in 2001. Since this time, practitioners have applied Agile methodologies to many delivery disciplines. This article explores the application of Agile methodologies and principles to business intelligence delivery and how Agile has changed with the evolution of business intelligence. Business intelligence has evolved because the amount of data generated through the internet and smart devices has grown exponentially altering how organizations and individuals use information. The practice of business intelligence delivery with an Agile methodology has matured; however, business intelligence has evolved altering the use of Agile principles and practices. The Big Data phenomenon, the volume, variety, and velocity of data, has impacted business intelligence and the use of information. New trends such as fast analytics and data science have emerged as part of business intelligence. This paper addresses how Agile principles and practices have evolved with business intelligence, as well as its challenges and future directions.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2016.04.013","JIR":14.098,"SJR":2.77},{"Title":"Analytics-based decision-making for service systems: A qualitative study and agenda for future research","author":"Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba} and Cyril Foropon and Thanos Papadopoulos","keywords":"Big data analytics, Decision-making, Service systems","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"While the use of big data tends to add value for business throughout the entire value chain, the integration of big data analytics (BDA) to the decision-making process remains a challenge. This study, based on a systematic literature review, thematic analysis and qualitative interview findings, proposes a set of six-steps to establish both rigor and relevance in the process of analytics-driven decision-making. Our findings illuminate the key steps in this decision process including problem definition, review of past findings, model development, data collection, data analysis as well as actions on insights in the context of service systems. Although findings have been discussed in a sequence of steps, the study identifies them as interdependent and iterative. The proposed six-step analytics-driven decision-making process, practical evidence from service systems, and future research agenda, provide altogether the foundation for future scholarly research and can serve as a step-wise guide for industry practitioners.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.01.020","JIR":14.098,"SJR":2.77},{"Title":"Data governance: A conceptual framework, structured review, and research agenda","author":"Rene Abraham and Johannes Schneider and Jan {vom Brocke}","keywords":"Data governance, Information governance, Conceptual framework, Literature review, Research agenda","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Data governance refers to the exercise of authority and control over the management of data. The purpose of data governance is to increase the value of data and minimize data-related cost and risk. Despite data governance gaining in importance in recent years, a holistic view on data governance, which could guide both practitioners and researchers, is missing. In this review paper, we aim to close this gap and develop a conceptual framework for data governance, synthesize the literature, and provide a research agenda. We base our work on a structured literature review including 145 research papers and practitioner publications published during 2001-2019. We identify the major building blocks of data governance and decompose them along six dimensions. The paper supports future research on data governance by identifying five research areas and displaying a total of 15 research questions. Furthermore, the conceptual framework provides an overview of antecedents, scoping parameters, and governance mechanisms to assist practitioners in approaching data governance in a structured manner.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.07.008","JIR":14.098,"SJR":2.77},{"Title":"30 years of intelligence models in management and business: A bibliometric review","author":"J.R. L\u00f3pez-Robles and J.R. Otegi-Olaso and I. {Porto G\u00f3mez} and M.J. Cobo","keywords":"Business Intelligence, Competitive Intelligence, Strategic Intelligence, Science information management, Mapping analysis","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"The critical factors in the big data era are collection, analysis, and dissemination of information to improve an organization\u2019s competitive position and enhance its products and services. In this scenario, it is imperative that organizations use Intelligence, which is understood as a process of gathering, analyzing, interpreting, and disseminating high-value data and information at the right time for use in the decision-making process. Earlier, the concept of Intelligence was associated with the military and national security sector; however, in present times, and as organizations evolve, Intelligence has been defined in several ways for the purposes of different applications. Given that the purpose of Intelligence is to obtain real value from data, information, and the dynamism of the organizations, the study of this discipline provides an opportunity to analyze the core trends related to data collection and processing, information management, decision-making process, and organizational capabilities. Therefore, the present study makes a conceptual analysis of the existing definitions of intelligence in the literature by quantifying the main bibliometric performance indicators, identifying the main authors and research areas, and evaluating the development of the field using SciMAT as a bibliometric analysis software.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.01.013","JIR":14.098,"SJR":2.77},{"Title":"Business analytics adoption process: An innovation diffusion perspective","author":"Dalwoo Nam and Junyeong Lee and Heeseok Lee","keywords":"Business analytics, Innovation diffusion, Adoption process, Data infrastructure, Data quality management, Analytics centralization","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Although business analytics (BA) have been increasingly adopted into businesses, there is limited empirical research examining the drivers of each stage of BA adoption in organizations. Drawing upon technological-organizational-environmental framework and innovation diffusion process, we developed an integrative model to examine BA adoption processes and tested with 170 Korean firms. The analysis shows data-related technological characteristics derive all stages of BA adoption: initiation, adoption and assimilation. While organizational characteristics are associated with adoption and assimilation stage, only competition intensity in environmental characteristics is associated with initiation stage. Our findings help practitioners and researchers to understand what factors can enable companies to adopt BA in each stage.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2019.07.017","JIR":14.098,"SJR":2.77},{"Title":"From data to value: A nine-factor framework for data-based value creation in information-intensive services","author":"Chiehyeon Lim and Ki-Hun Kim and Min-Jun Kim and Jun-Yeon Heo and Kwang-Jae Kim and Paul P. Maglio","keywords":"Big data, Data-based value creation, Information-intensive service, Factor, Data\u2013Value Chain","journal":"INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT","abstract":"Service is a key context for the application of IT, as IT digitizes information interactions in service and facilitates value creation, thereby contributing to service innovation. The recent proliferation of big data provides numerous opportunities for information-intensive services (IISs), in which information interactions exert the greatest effect on value creation. In the modern data-rich economy, understanding mechanisms and related factors of data-based value creation in IISs is essential for using IT to improve such services. This study identified nine key factors that characterize this data-based value creation: (1) data source, (2) data collection, (3) data, (4) data analysis, (5) information on the data source, (6) information delivery, (7) customer (information user), (8) value in information use, and (9) provider network. These factors were identified and defined through six action research projects with industry and government that used specific datasets to design new IISs and by analyzing data usage in 149 IIS cases. This paper demonstrates the usefulness of these factors for describing, analyzing, and designing the entire value creation chain, from data collection to value creation, in IISs. The main contribution of this study is to provide a simple yet comprehensive and empirically tested basis for the use and management of data to facilitate service value creation.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijinfomgt.2017.12.007","JIR":14.098,"SJR":2.77},{"Title":"Rethinking big data: A review on the data quality and usage issues","author":"Jianzheng Liu and Jie Li and Weifeng Li and Jiansheng Wu","keywords":"Big data, Data quality and error, Data ethnics, Spatial information sciences","journal":"ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING","abstract":"The recent explosive publications of big data studies have well documented the rise of big data and its ongoing prevalence. Different types of \u201cbig data\u201d have emerged and have greatly enriched spatial information sciences and related fields in terms of breadth and granularity. Studies that were difficult to conduct in the past time due to data availability can now be carried out. However, big data brings lots of \u201cbig errors\u201d in data quality and data usage, which cannot be used as a substitute for sound research design and solid theories. We indicated and summarized the problems faced by current big data studies with regard to data collection, processing and analysis: inauthentic data collection, information incompleteness and noise of big data, unrepresentativeness, consistency and reliability, and ethical issues. Cases of empirical studies are provided as evidences for each problem. We propose that big data research should closely follow good scientific practice to provide reliable and scientific \u201cstories\u201d, as well as explore and develop techniques and methods to mitigate or rectify those \u2018big-errors\u2019 brought by big data.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.isprsjprs.2015.11.006","JIR":8.979,"SJR":2.96},{"Title":"Geospatial big data handling theory and methods: A review and research challenges","author":"Songnian Li and Suzana Dragicevic and Francesc Ant\u00f3n Castro and Monika Sester and Stephan Winter and Arzu Coltekin and Christopher Pettit and Bin Jiang and James Haworth and Alfred Stein and Tao Cheng","keywords":"Big data, Geospatial, Data handling, Analytics, Spatial modeling, Review","journal":"ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING","abstract":"Big data has now become a strong focus of global interest that is increasingly attracting the attention of academia, industry, government and other organizations. Big data can be situated in the disciplinary area of traditional geospatial data handling theory and methods. The increasing volume and varying format of collected geospatial big data presents challenges in storing, managing, processing, analyzing, visualizing and verifying the quality of data. This has implications for the quality of decisions made with big data. Consequently, this position paper of the International Society for Photogrammetry and Remote Sensing (ISPRS) Technical Commission II (TC II) revisits the existing geospatial data handling methods and theories to determine if they are still capable of handling emerging geospatial big data. Further, the paper synthesises problems, major issues and challenges with current developments as well as recommending what needs to be developed further in the near future.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.isprsjprs.2015.10.012","JIR":8.979,"SJR":2.96},{"Title":"Improved land cover map of Iran using Sentinel imagery within Google Earth Engine and a novel automatic workflow for land cover classification using migrated training samples","author":"Arsalan Ghorbanian and Mohammad Kakooei and Meisam Amani and Sahel Mahdavi and Ali Mohammadzadeh and Mahdi Hasanlou","keywords":"Land cover classification, Sentinel, Google Earth Engine, Big data, Remote sensing, Iran","journal":"ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING","abstract":"Accurate information about the location, extent, and type of Land Cover (LC) is essential for various applications. The only recent available country-wide LC map of Iran was generated in 2016 by the Iranian Space Agency (ISA) using Moderate Resolution Imaging Spectroradiometer (MODIS) images with a considerably low accuracy. Therefore, the production of an up-to-date and accurate Iran-wide LC map using the most recent remote sensing, machine learning, and big data processing algorithms is required. Moreover, it is important to develop an efficient method for automatic LC generation for various time periods without the need to collect additional ground truth data from this immense country. Therefore, this study was conducted to fulfill two objectives. First, an improved Iranian LC map with 13 LC classes and a spatial resolution of 10\u00a0m was produced using multi-temporal synergy of Sentinel-1 and Sentinel-2 satellite datasets applied to an object-based Random forest (RF) algorithm. For this purpose, 2,869 Sentinel-1 and 11,994 Sentinel-2 scenes acquired in 2017 were processed and classified within the Google Earth Engine (GEE) cloud computing platform allowing big geospatial data analysis. The Overall Accuracy (OA) and Kappa Coefficient (KC) of the final Iran-wide LC map for 2017 was 95.6% and 0.95, respectively, indicating the considerable potential of the proposed big data processing method. Second, an efficient automatic method was developed based on Sentinel-2 images to migrate ground truth samples from a reference year to automatically generate an LC map for any target year. The OA and KC for the LC map produced for the target year 2019 were 91.35% and 0.91, respectively, demonstrating the efficiency of the proposed method for automatic LC mapping. Based on the obtained accuracies, this method can potentially be applied to other regions of interest for LC mapping without the need for ground truth data from the target year.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.isprsjprs.2020.07.013","JIR":8.979,"SJR":2.96},{"Title":"An integrated environmental analytics system (IDEAS) based on a DGGS","author":"Colin Robertson and Chiranjib Chaudhuri and Majid Hojati and Steven A. Roberts","keywords":"DGGS, Data model, Big data, Spatial data, Analytics, Environment","journal":"ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING","abstract":"Discrete global grid systems (DGGS) have been proposed as a data model for a digital earth framework. We introduce a new data model and analytics system called IDEAS \u2013 integrated discrete environmental analysis system to create an operational DGGS-based GIS which is suitable for large scale environmental modelling and analysis. Our analysis demonstrates that DGGS-based GIS is feasible within a relational database environment incorporating common data analytics tools. Common GIS operations implemented in our DGGS data model outperformed the same operations computed using traditional geospatial data types. A case study into wildfire modelling demonstrates the capability for data integration and supporting big data geospatial analytics. These results indicate that DGGS data models have significant capability to solve some of the key outstanding problems related to geospatial data analytics, providing a common representation upon which fast and scalable algorithms can be built.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.isprsjprs.2020.02.009","JIR":8.979,"SJR":2.96},{"Title":"Lidar sheds new light on plant phenomics for plant breeding and management: Recent advances and future prospects","author":"Shichao Jin and Xiliang Sun and Fangfang Wu and Yanjun Su and Yumei Li and Shiling Song and Kexin Xu and Qin Ma and Fr\u00e9d\u00e9ric Baret and Dong Jiang and Yanfeng Ding and Qinghua Guo","keywords":"Lidar, Traits, Phenomics, Breeding, Management, Multi-omics","journal":"ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING","abstract":"Plant phenomics is a new avenue for linking plant genomics and environmental studies, thereby improving plant breeding and management. Remote sensing techniques have improved high-throughput plant phenotyping. However, the accuracy, efficiency, and applicability of three-dimensional (3D) phenotyping are still challenging, especially in field environments. Light detection and ranging (lidar) provides a powerful new tool for 3D phenotyping with the rapid development of facilities and algorithms. Numerous efforts have been devoted to studying static and dynamic changes of structural and functional phenotypes using lidar in agriculture. These progresses also improve 3D plant modeling across different spatial\u2013temporal scales and disciplines, providing easier and less expensive association with genes and analysis of environmental practices and affords new insights into breeding and management. Beyond agriculture phenotyping, lidar shows great potential in forestry, horticultural, and grass phenotyping. Although lidar has resulted in remarkable improvements in plant phenotyping and modeling, the synthetization of lidar-based phenotyping for breeding and management has not been fully explored. We identify three main challenges in lidar-based phenotyping development: 1) developing low cost, high spatial\u2013temporal, and hyperspectral lidar facilities, 2) moving into multi-dimensional phenotyping with an endeavor to generate new algorithms and models, and 3) embracing open source and big data.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.isprsjprs.2020.11.006","JIR":8.979,"SJR":2.96},{"Title":"Is big data used by cities? Understanding the nature and antecedents of big data use by municipalities","author":"Hamza Ali and Ryad Titah","keywords":"Big data use, Big data use in municipalities, Smart cities, E-government, Municipal use of IT, Digital government","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"It is estimated that by 2050, 70% of the population will be urban (Nations Unies, 2014). This massive urbanization has created unprecedented challenges for cities and city managers which has led many of them to look for technological solutions to address them, including the use of Big Data, which is among the most considered technological support to help improve the overall operational and service delivery of cities. It is estimated that around 7 billion connected objects will soon be implemented in cities worldwide which will produce an unprecedented and massive amount of real-time data that will have to be managed, used, and analyzed effectively. If this massive amount of data is effectively managed and used, it can provide important benefits and produce real positive impacts on the functioning of cities. Nonetheless, despite these benefits, only a few cities are able to use and exploit big data, and some studies have shown that less than 0.5% of all the available data has been explored. The objective of this study is to understand the factors that influence cities to use big data and the nature of such use. Based on a field survey involving 106 municipalities, this study investigates the antecedents of big data use by cities and shows how different sets of antecedents influence three different types of big data use by cities.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2021.101600","JIR":7.279,"SJR":2.121},{"Title":"Big data analytics, resource orchestration, and digital sustainability: A case study of smart city development","author":"Dan Zhang and L.G. Pee and Shan L. Pan and Lili Cui","keywords":"Smart city, Big data, Digital sustainability, Resource orchestration, Socio-technical issues","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"Smart cities are expected to improve the efficiency and effectiveness of urban management, including public services, public security, and environmental protection, and to ultimately achieve Sustainable Development Goal (SDG) 11 for making cities inclusive, safe, resilient, and sustainable. Big data have been identified as a key enabler in the development of smart cities. However, our understanding of how different data sources should be managed and integrated remains limited. By analyzing data applications in the development of a sustainable smart city, this case study identified three phases of development, each requiring a different approach to orchestrating diverse data sources. A framework identifying the phases, data-related issues, data orchestration and its interaction with other resources, focal capabilities, and development approaches is developed. This study benefits both researchers and practitioners by making theoretical contributions and by offering practical insights in the fields of smart cities and big data.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2021.101626","JIR":7.279,"SJR":2.121},{"Title":"Techno-optimism and policy-pessimism in the public sector big data debate","author":"Simon Vydra and Bram Klievink","keywords":"Big data, Analytics, Government, Public administration, Policy-making, Decision-making, Science-policy interface, Network governance","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"Despite great potential, high hopes and big promises, the actual impact of big data on the public sector is not always as transformative as the literature would suggest. In this paper, we ascribe this predicament to an overly strong emphasis the current literature places on technical-rational factors at the expense of political decision-making factors. We express these two different emphases as two archetypical narratives and use those to illustrate that some political decision-making factors should be taken seriously by critiquing some of the core \u2018techno-optimist\u2019 tenets from a more \u2018policy-pessimist\u2019 angle. In the conclusion we have these two narratives meet \u2018eye-to-eye\u2019, facilitating a more systematized interrogation of big data promises and shortcomings in further research, paying appropriate attention to both technical-rational and political decision-making factors. We finish by offering a realist rejoinder of these two narratives, allowing for more context-specific scrutiny and balancing both technical-rational and political decision-making concerns, resulting in more realistic expectations about using big data for policymaking in practice.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2019.05.010","JIR":7.279,"SJR":2.121},{"Title":"Rationality and politics of algorithms. Will the promise of big data survive the dynamics of public decision making?","author":"H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer","keywords":"empty","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"Big data promises to transform public decision-making for the better by making it more responsive to actual needs and policy effects. However, much recent work on big data in public decision-making assumes a rational view of decision-making, which has been much criticized in the public administration debate. In this paper, we apply this view, and a more political one, to the context of big data and offer a qualitative study. We question the impact of big data on decision-making, realizing that big data \u2013 including its new methods and functions \u2013 must inevitably encounter existing political and managerial institutions. By studying two illustrative cases of big data use processes, we explore how these two worlds meet. Specifically, we look at the interaction between data analysts and decision makers. In this we distinguish between a rational view and a political view, and between an information logic and a decision logic. We find that big data provides ample opportunities for both analysts and decision makers to do a better job, but this doesn't necessarily imply better decision-making, because big data also provides opportunities for actors to pursue their own interests. Big data enables both data analysts and decision makers to act as autonomous agents rather than as links in a functional chain. Therefore, big data's impact cannot be interpreted only in terms of its functional promise; it must also be acknowledged as a phenomenon set to impact our policymaking institutions, including their legitimacy.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2018.10.011","JIR":7.279,"SJR":2.121},{"Title":"Customer agility and responsiveness through big data analytics for public value creation: A case study of Houston 311 on-demand services","author":"Akemi Takeoka Chatfield and Christopher G. Reddick","keywords":"On-demand services, Customer agility, Systemic use, Big data, Big data analytics, IT assimilation, Process-level strategic alignment, Digital infrastructures, 311 services, Government","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"A theoretical framework for big data analytics-enabled customer agility and responsiveness was developed from extant IS research. In on-demand service environments, customer agility involves dynamic capabilities in sensing and responding to citizens. Using this framework, a case study examined a large city government's 311 on-demand services which had leveraged big data analytics. While we found the localized big data analytics use by some of the 22 departments for enhanced customer agility and on-demand 311 services, city-wide systemic change in on-demand service delivery through big data analytics use was not evident. From the case study we identified key institutional mechanisms for linking customer agility to public value creation through 311 services. We posit how systemic use of big data analytics embedded into critical processes enables the government to co-create public values with citizens through 311 on-demand services, indicating the importance of creating a culture of analytics driven by strong political leadership.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2017.11.002","JIR":7.279,"SJR":2.121},{"Title":"A data quality approach to the identification of discrimination risk in automated decision making systems","author":"Antonio Vetr\u00f2 and Marco Torchiano and Mariachiara Mecati","keywords":"Automated decision making, Data ethics, Data quality, Data bias, Algorithm fairness, Digital policy, Digital governance","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"Automated decision-making (ADM) systems may affect multiple aspects of our lives. In particular, they can result in systematic discrimination of specific population groups, in violation of the EU Charter of Fundamental Rights. One of the potential causes of discriminative behavior, i.e., unfairness, lies in the quality of the data used to train such ADM systems. Using a data quality measurement approach combined with risk management, both defined in ISO standards, we focus on balance characteristics and we aim to understand how balance indexes (Gini, Simpson, Shannon, Imbalance Ratio) identify discrimination risk in six large datasets containing the classification output of ADM systems. The best result is achieved using the Imbalance Ratio index. Gini and Shannon indexes tend to assume high values and for this reason they have modest results in both aspects: further experimentation with different thresholds is needed. In terms of policies, the risk-based approach is a core element of the EU approach to regulate algorithmic systems: in this context, balance measures can be easily assumed as risk indicators of propagation \u2013 or even amplification \u2013 of bias in the input data of ADM systems.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2021.101619","JIR":7.279,"SJR":2.121},{"Title":"Knowledge areas, themes and future research on open data: A co-word analysis","author":"Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-Mar\u00eda Mora-Valent\u00edn","keywords":"Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge areas, Most-studied themes, Future trends","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"This paper aims to contribute to a better understanding of the literature on open data in three ways. The first is to develop a descriptive analysis of journals and authors to identify the knowledge areas in which open data are applied. The second is to analyse the conceptual structure of the field using a bibliometric technique. The co-word analysis enabled us to create a map of the main themes that have been studied, identifying their importance and relevance. These themes were analysed and grouped. The third is to propose future research trends. According to our results, the main knowledge areas are Engineering, Health, Public Administration, Management and Education. The main themes are big data, open-linked data and data reuse. Finally, several research questions are proposed according to knowledge area and theme.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2018.10.008","JIR":7.279,"SJR":2.121},{"Title":"Data science empowering the public: Data-driven dashboards for transparent and accountable decision-making in smart cities","author":"Ricardo Matheus and Marijn Janssen and Devender Maheshwari","keywords":"Data science, Dashboards, E-government, Open government, Open data, Big data, Smart City, Design principles, Transparency, Accountability, Trust, Policy-making, Decision-making","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"Dashboards visualize a consolidated set data for a certain purpose which enables users to see what is happening and to initiate actions. Dashboards can be used by governments to support their decision-making and policy processes or to communicate and interact with the public. The objective of this paper is to understand and to support the design of dashboards for creating transparency and accountability. Two smart city cases are investigated showing that dashboards can improve transparency and accountability, however, realizing these benefits was cumbersome and encountered various risks and challenges. Challenges include insufficient data quality, lack of understanding of data, poor analysis, wrong interpretation, confusion about the outcomes, and imposing a pre-defined view. These challenges can easily result in misconceptions, wrong decision-making, creating a blurred picture resulting in less transparency and accountability, and ultimately in even less trust in the government. Principles guiding the design of dashboards are presented. Dashboards need to be complemented by mechanisms supporting citizens' engagement, data interpretation, governance and institutional arrangements.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2018.01.006","JIR":7.279,"SJR":2.121},{"Title":"Data governance: Organizing data for trustworthy Artificial Intelligence","author":"Marijn Janssen and Paul Brous and Elsa Estevez and Luis S. Barbosa and Tomasz Janowski","keywords":"Big data, Data governance, AI, Algorithmic governance, Information sharing, Artificial Intelligence, Trusted frameworks","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"The rise of Big, Open and Linked Data (BOLD) enables Big Data Algorithmic Systems (BDAS) which are often based on machine learning, neural networks and other forms of Artificial Intelligence (AI). As such systems are increasingly requested to make decisions that are consequential to individuals, communities and society at large, their failures cannot be tolerated, and they are subject to stringent regulatory and ethical requirements. However, they all rely on data which is not only big, open and linked but varied, dynamic and streamed at high speeds in real-time. Managing such data is challenging. To overcome such challenges and utilize opportunities for BDAS, organizations are increasingly developing advanced data governance capabilities. This paper reviews challenges and approaches to data governance for such systems, and proposes a framework for data governance for trustworthy BDAS. The framework promotes the stewardship of data, processes and algorithms, the controlled opening of data and algorithms to enable external scrutiny, trusted information sharing within and between organizations, risk-based governance, system-level controls, and data control through shared ownership and self-sovereign identities. The framework is based on 13 design principles and is proposed incrementally, for a single organization and multiple networked organizations.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2020.101493","JIR":7.279,"SJR":2.121},{"Title":"Implementing challenges of artificial intelligence: Evidence from public manufacturing sector of an emerging economy","author":"Manu Sharma and Sunil Luthra and Sudhanshu Joshi and Anil Kumar","keywords":"Artificial intelligence, Implementing challenges, Public manufacturing sector, AI enabled systems, Emerging economies","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"The growing Artificial Intelligence (AI) age has been flooded with several innovations in algorithmic machine learning that may bring significant impacts to industries such as healthcare, agriculture, education, manufacturing, retail etc. But challenges such as data quality, privacy and lack of a skilled workforce limit the scope of AI implementation in emerging economies, particularly in the Public Manufacturing Sector (PMS). Therefore, to enhance the body of relevant literature, this study examines the existing challenges of AI implementation in PMS of India and explores the inter-relationships among them. The study has utilized the DEMATEL method for identification of the cause-and-effect group factors. The findings reveal that poor data quality, managers' lack of understanding of cognitive technologies, data privacy, problems in integrating cognitive projects and expensive technologies are the main challenges for AI implementation in PMS of India. Moreover, a model is proposed for industrial decision-makers and managers to take appropriate decisions to develop intelligent AI enabled systems for manufacturing organizations in emerging economies.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2021.101624","JIR":7.279,"SJR":2.121},{"Title":"Big and open linked data analytics ecosystem: Theoretical background and essential elements","author":"Martin Lnenicka and Jitka Komarkova","keywords":"Big and open linked data, Ecosystem approach, Dimensions, Data analytics lifecycle, Stakeholders, Conceptual framework","journal":"GOVERNMENT INFORMATION QUARTERLY","abstract":"Big and open linked data are often mentioned together because storing, processing, and publishing large amounts of these data play an increasingly important role in today's society. However, although this topic is described from the political, economic, and social points of view, a technical dimension, which is represented by big data analytics, is insufficient. The aim of this review article was to provide a theoretical background of big and open linked data analytics ecosystem and its essential elements. First, the key terms were introduced including related dimensions. Then, the key lifecycle phases were defined and involved stakeholders were identified. Finally, a conceptual framework was proposed. In contrast to previous research, the new ecosystem is formed by interactions of stakeholders in the following dimensions and their sub-dimensions: transparency, engagement, legal, technical, social, and economic. These relationships are characterized by the most important requisites and public policy choices affecting the data analytics ecosystem together with the key phases and activities of the data analytics lifecycle. The findings should contribute to relevant initiatives, strategies, and policies and their effective implementation.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.giq.2018.11.004","JIR":7.279,"SJR":2.121},{"Title":"Benefiting from big data in natural products: importance of preserving foundational skills and prioritizing data quality","author":"Nadja B. Cech and Marnix H. Medema and Jon Clardy","keywords":"empty","journal":"NATURAL PRODUCT REPORTS","abstract":"ABSTRACT\nSystematic, large-scale, studies at the genomic, metabolomic, and functional level have transformed the natural product sciences. Improvements in technology and reduction in cost for obtaining spectroscopic, chromatographic, and genomic data coupled with the creation of readily accessible curated and functionally annotated data sets have altered the practices of virtually all natural product research laboratories. Gone are the days when the natural products researchers were expected to devote themselves exclusively to the isolation, purification, and structure elucidation of small molecules. We now also engage with big data in taxonomic, genomic, proteomic, and\/or metabolomic collections, and use these data to generate and test hypotheses. While the oft stated aim for the use of large-scale -omics data in the natural products sciences is to achieve a rapid increase in the rate of discovery of new drugs, this has not yet come to pass. At the same time, new technologies have provided unexpected opportunities for natural products chemists to ask and answer new and different questions. With this viewpoint, we discuss the evolution of big data as a part of natural products research and provide a few examples of how discoveries have been enabled by access to big data. We also draw attention to some of the limitations in our existing engagement with large datasets and consider what would be necessary to overcome them.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1039\/d1np00061f","JIR":13.423,"SJR":2.703},{"Title":"Data quality for data science, predictive analytics, and big data in supply chain management: An introduction to the problem and suggestions for research and applications","author":"Benjamin T. Hazen and Christopher A. Boone and Jeremy D. Ezell and L. Allison Jones-Farmer","keywords":"Data quality, Statistical process control, Knowledge-based view, Organizational information processing view, Systems theory","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"Today\u05f3s supply chain professionals are inundated with data, motivating new ways of thinking about how data are produced, organized, and analyzed. This has provided an impetus for organizations to adopt and perfect data analytic functions (e.g. data science, predictive analytics, and big data) in order to enhance supply chain processes and, ultimately, performance. However, management decisions informed by the use of these data analytic methods are only as good as the data on which they are based. In this paper, we introduce the data quality problem in the context of supply chain management (SCM) and propose methods for monitoring and controlling data quality. In addition to advocating for the importance of addressing data quality in supply chain research and practice, we also highlight interdisciplinary research topics based on complementary theory.","year":2014,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2014.04.018","JIR":7.885,"SJR":2.406},{"Title":"A multidisciplinary perspective of big data in management research","author":"Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang","keywords":"Big data, Management research, Literature review","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"In recent years, big data has emerged as one of the prominent buzzwords in business and management. In spite of the mounting body of research on big data across the social science disciplines, scholars have offered little synthesis on the current state of knowledge. To take stock of academic research that contributes to the big data revolution, this paper tracks scholarly work's perspectives on big data in the management domain over the past decade. We identify key themes emerging in management studies and develop an integrated framework to link the multiple streams of research in fields of organisation, operations, marketing, information management and other relevant areas. Our analysis uncovers a growing awareness of big data's business values and managerial changes led by data-driven approach. Stemming from the review is the suggestion for research that both structured and unstructured big data should be harnessed to advance understanding of big data value in informing organisational decisions and enhancing firm competitiveness. To discover the full value, firms need to formulate and implement a data-driven strategy. In light of these, the study identifies and outlines the implications and directions for future research.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2017.06.006","JIR":7.885,"SJR":2.406},{"Title":"How \u2018big data\u2019 can make big impact: Findings from a systematic review and a longitudinal case study","author":"Samuel {Fosso Wamba} and Shahriar Akter and Andrew Edwards and Geoffrey Chopin and Denis Gnanzou","keywords":"\u2018Big data\u2019, Analytics, Business value, Issues, Case study, Emergency services, Literature review","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"Big data has the potential to revolutionize the art of management. Despite the high operational and strategic impacts, there is a paucity of empirical research to assess the business value of big data. Drawing on a systematic review and case study findings, this paper presents an interpretive framework that analyzes the definitional perspectives and the applications of big data. The paper also provides a general taxonomy that helps broaden the understanding of big data and its role in capturing business value. The synthesis of the diverse concepts within the literature on big data provides deeper insights into achieving value through big data strategy and implementation.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2014.12.031","JIR":7.885,"SJR":2.406},{"Title":"The value of Big Data in servitization","author":"David Opresnik and Marco Taisch","keywords":"Servitization, Big Data, Manufacturing, Competitive advantage, Value, Information","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"Servitization has become a pervasive business strategy among manufacturers, enabling them to undergird their competitive advantage. However, it has at least one weakness. While it is used worldwide also in economies with lower production costs, services in manufacturing are slowly becoming commoditized and will become a necessary, though not sufficient, condition for reaching an above average competitive advantage. Consequently, in this article we propose a new basis for competitive advantage for manufacturing enterprises called a Big Data Strategy in servitization. We scrutinize how manufacturers can exploit the opportunity arising from combined Big Data and servitization. Therefore, the concept of a Big Data Strategy framework in servitization is proposed. The findings are benchmarked against established frameworks in the Big Data and servitization literature. Its impact on competitive advantage is assessed through three theoretical perspectives that increase the validity of the results. The main finding is that, through the proposed strategy, new revenue streams can be created, while opening the possibility to decrease prices for product\u2013services. Through the proposed strategy manufacturers can differentiate themselves from the ones that are already servitizing. This article introduces the possibility of influencing the most important of the five \u201cVs\u201d in Big Data\u2013Value, in addition to the other four \u201cVs\u201d\u2014Volume, Variety, Velocity and Verification. As in regards to servitization, the article adds a third layer of added value\u2014 \u201cinformation\u201d, beside the two existing ones: product and service. The results have strategic implications for managers.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2014.12.036","JIR":7.885,"SJR":2.406},{"Title":"Harvesting big data to enhance supply chain innovation capabilities: An analytic infrastructure based on deduction graph","author":"Kim Hua Tan and YuanZhu Zhan and Guojun Ji and Fei Ye and Chingter Chang","keywords":"Big data, Analytic infrastructure, Competence set, Deduction graph, Supply chain innovation","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"Today, firms can access to big data (tweets, videos, click streams, and other unstructured sources) to extract new ideas or understanding about their products, customers, and markets. Thus, managers increasingly view data as an important driver of innovation and a significant source of value creation and competitive advantage. To get the most out of the big data (in combination with a firm\u05f3s existing data), a more sophisticated way of handling, managing, analysing and interpreting data is necessary. However, there is a lack of data analytics techniques to assist firms to capture the potential of innovation afforded by data and to gain competitive advantage. This research aims to address this gap by developing and testing an analytic infrastructure based on the deduction graph technique. The proposed approach provides an analytic infrastructure for firms to incorporate their own competence sets with other firms. Case studies results indicate that the proposed data analytic approach enable firms to utilise big data to gain competitive advantage by enhancing their supply chain innovation capabilities.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2014.12.034","JIR":7.885,"SJR":2.406},{"Title":"How to improve firm performance using big data analytics capability and business strategy alignment?","author":"Shahriar Akter and Samuel Fosso Wamba and Angappa Gunasekaran and Rameshwar Dubey and Stephen J. Childe","keywords":"Capabilities, Entanglement view, Big data analytics, Hierarchical modeling","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"The recent interest in big data has led many companies to develop big data analytics capability (BDAC) in order to enhance firm performance (FPER). However, BDAC pays off for some companies but not for others. It appears that very few have achieved a big impact through big data. To address this challenge, this study proposes a BDAC model drawing on the resource-based theory (RBT) and the entanglement view of sociomaterialism. The findings show BDAC as a hierarchical model, which consists of three primary dimensions (i.e., management, technology, and talent capability) and 11 subdimensions (i.e., planning, investment, coordination, control, connectivity, compatibility, modularity, technology management knowledge, technical knowledge, business knowledge and relational knowledge). The findings from two Delphi studies and 152 online surveys of business analysts in the U.S. confirm the value of the entanglement conceptualization of the higher-order BDAC model and its impact on FPER. The results also illuminate the significant moderating impact of analytics capability\u2013business strategy alignment on the BDAC\u2013FPER relationship.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2016.08.018","JIR":7.885,"SJR":2.406},{"Title":"Smart product service system hierarchical model in banking industry under uncertainties","author":"Ming-Lang Tseng and Tat-Dat Bui and Shulin Lan and Ming K. Lim and Abu Hashan Md Mashud","keywords":"Smart product-service systems, Digital technology, Sustainable innovation, Fuzzy delphi method, Decision-making trial and evaluation laboratory (DEMATEL), Diffusion of innovation theory","journal":"INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS","abstract":"This study adopts the diffusion of innovation theory as to develop the smart product service system model in banking industry due to prior studies are lacking in identifying the attributes. The smart product service system functions are bearing high uncertainty and system complexity; hence, the hybrid method of fuzzy Delphi method and fuzzy decision-making trial and evaluation laboratory to construct a valid hierarchical model and identified the causal interrelationships among the attributes. The smart product service system hierarchical model with eight aspects and 41 criteria are proposed enriching the existing literature and that identify appropriate strategies to achieve operational performance. The results show that seven aspects and 22 criteria are determined as the valid hierarchical model. The institutional compression, digital platform operation, and e-knowledge management are the causing aspects helps to form smart product service system operational performance in high uncertainty. For practices, the banking decision-makers should develop innovative actions relied on the forcible compression, cyber-physical systems, industrial big data, cloud service allocation and sharing, and transparency improvement as they are most importance criteria playing a decisive role in a successful SPSS. This provides guidelines for banking industry practice in Taiwan encouraging the miscellany of digital technology accomplishment for sustainable target.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijpe.2021.108244","JIR":7.885,"SJR":2.406},{"Title":"Digital health, big data and smart technologies for the care of patients with systemic autoimmune diseases: Where do we stand?","author":"Hugo Bergier and Lo\u00efc Duron and Christelle Sordet and Lou Kawka and Aur\u00e9lien Schlencker and Fran\u00e7ois Chasset and Laurent Arnaud","keywords":"Autoimmune diseases, Digital technology, Big data, Delivery of health care, Telemedicine","journal":"AUTOIMMUNITY REVIEWS","abstract":"The past decade has seen tremendous development in digital health, including in innovative new technologies such as Electronic Health Records, telemedicine, virtual visits, wearable technology and sophisticated analytical tools such as artificial intelligence (AI) and machine learning for the deep-integration of big data. In the field of rare connective tissue diseases (rCTDs), these opportunities include increased access to scarce and remote expertise, improved patient monitoring, increased participation and therapeutic adherence, better patient outcomes and patient empowerment. In this review, we discuss opportunities and key-barriers to improve application of digital health technologies in the field of autoimmune diseases. We also describe what could be the fully digital pathway of rCTD patients. Smart technologies can be used to provide real-world evidence about the natural history of rCTDs, to determine real-life drug utilization, advanced efficacy and safety data for rare diseases and highlight significant unmet needs. Yet, digitalization remains one of the most challenging issues faced by rCTD patients, their physicians and healthcare systems. Digital health technologies offer enormous potential to improve autoimmune rCTD care but this potential has so far been largely unrealized due to those significant obstacles. The need for robust assessments of the efficacy, affordability and scalability of AI in the context of digital health is crucial to improve the care of patients with rare autoimmune diseases.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.autrev.2021.102864","JIR":9.754,"SJR":2.621},{"Title":"Are batch effects still relevant in the age of big data?","author":"Wilson Wen Bin Goh and Chern Han Yong and Limsoon Wong","keywords":"artificial intelligence, batch effect, machine learning, RNA sequencing, single cell","journal":"TRENDS IN BIOTECHNOLOGY","abstract":"Batch effects (BEs) are technical biases that may confound analysis of high-throughput biotechnological data. BEs are complex and effective mitigation is highly context-dependent. In particular, the advent of high-resolution technologies such as single-cell RNA sequencing presents new challenges. We first cover how BE modeling differs between traditional datasets and the new data landscape. We also discuss new approaches for measuring and mitigating BEs, including whether a BE is significant enough to warrant correction. Even with the advent of machine learning and artificial intelligence, the increased complexity of next-generation biotechnological data means increased complexities in BE management. We forecast that BEs will not only remain relevant in the age of big data but will become even more important.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.tibtech.2022.02.005","JIR":19.536,"SJR":3.192},{"Title":"A digital twin-based big data virtual and real fusion learning reference framework supported by industrial internet towards smart manufacturing","author":"Pei Wang and Ming Luo","keywords":"Virtual and real fusion learning, Big data learning and analysis models, Digital twin, Industrial internet, Smart manufacturing","journal":"JOURNAL OF MANUFACTURING SYSTEMS","abstract":"Digital twin takes Industrial Internet as a carrier deeply coordinating and integrating virtual spaces with physical spaces, which effectively promotes smart factory development. Digital twin-based big data learning and analysis (BDLA) deepens virtual and real fusion, interaction and closed-loop iterative optimization in smart factories. This paper proposes a digital twin-based big data virtual and real fusion (DT-BDVRL) reference framework supported by Industrial Internet towards smart manufacturing. The reference framework is synthetically designed from three perspectives. The first one is an overall framework of DT-BDVRL supported by Industrial Internet. The second one is the establishment method and flow of BDLA models based on digital twin. The final one is digital thread of DT-BDVRL in virtual and real fusion analysis, iteration and closed-loop feedback in product full life cycle processes. For different virtual scenes, iterative optimization and verification methods and processes of BDLA models in virtual spaces are established. Moreover, the BDLA results can drive digital twin running in virtual spaces. By this, the BDLA results can be validated iteratively multiple times in virtual spaces. At same time, the BDLA results that run in virtual spaces are synchronized and executed in physical spaces through Industrial Internet platforms, effectively improving the physical execution effect of BDLA models. Finally, the above contents were applied and verified in the actual production case study of power switchgear equipment.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jmsy.2020.11.012","JIR":8.633,"SJR":2.31},{"Title":"Big data analytics based fault prediction for shop floor scheduling","author":"Wei Ji and Lihui Wang","keywords":"Big data analytics, Fault prediction, Shop floor, Scheduling","journal":"JOURNAL OF MANUFACTURING SYSTEMS","abstract":"The current task scheduling mainly concerns the availability of machining resources, rather than the potential errors after scheduling. To minimise such errors in advance, this paper presents a big data analytics based fault prediction approach for shop floor scheduling. Within the context, machining tasks, machining resources, and machining processes are represented by data attributes. Based on the available data on the shop floor, the potential fault\/error patterns, referring to machining errors, machine faults and maintenance states, are mined for unsuitable scheduling arrangements before machining as well as upcoming errors during machining. Comparing the data-represented tasks with the mined error patterns, their similarities or differences are calculated. Based on the calculated similarities, the fault probabilities of the scheduled tasks or the current machining tasks can be obtained, and they provide a reference of decision making for scheduling and rescheduling the tasks. By rescheduling high-risk tasks carefully, the potential errors can be avoided. In this paper, the architecture of the approach consisting of three steps in three levels is proposed. Furthermore, big data are considered in three levels, i.e. local data, local network data and cloud data. In order to implement this idea, several key techniques are illustrated in detail, e.g. data attribute, data cleansing, data integration of databases in different levels, and big data analytic algorithms. Finally, a simplified case study is described to show the prediction process of the proposed method.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jmsy.2017.03.008","JIR":8.633,"SJR":2.31},{"Title":"Industrial data management strategy towards an SME-oriented PHM","author":"N. Omri and Z. {Al Masry} and N. Mairot and S. Giampiccolo and N. Zerhouni","keywords":"Small and medium-sized enterprises, Data-driven PHM, Industrial data management, Data quality metrics, PHM implementation strategy","journal":"JOURNAL OF MANUFACTURING SYSTEMS","abstract":"The fourth industrial revolution is derived from advances in digitization and prognostic and health management (PHM) disciplines to make plants smarter and more efficient. However, an adapted approach for data-driven PHM process implementation in small and medium-sized enterprises (SMEs) has not been yet discussed. This research gap is due to the specificities of SMEs and the lack of documentation. In this paper, we examine existing standards for implementing PHM in the industrial field and discuss the limitations within SMEs. Based on that, a novel strategy to implement a data-driven PHM approach in SMEs is proposed. Accordingly, the data management process and the impact of data quality are reviewed to address some critical data problems in SMEs (e.g., data volume and data accuracy). A first set of simulations was carried out to study the impact of the data volume and percentage of missing data on classification problems in PHM. A general model of the evolution of the results accuracy in function of data volume and missing data is then generated, and an economic data volume notion is proposed for data infrastructure resizing. The proposed strategy and the developed models are then applied to the Scoder enterprise, which is a French SME. The feedback on the first results of this application is reported and discussed.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jmsy.2020.04.002","JIR":8.633,"SJR":2.31},{"Title":"Data science skills and domain knowledge requirements in the manufacturing industry: A gap analysis","author":"Guoyan Li and Chenxi Yuan and Sagar Kamarthi and Mohsen Moghaddam and Xiaoning Jin","keywords":"Industry 4.0, Labor market analysis, Skills gap, Data science","journal":"JOURNAL OF MANUFACTURING SYSTEMS","abstract":"Manufacturing has adopted technologies such as automation, robotics, industrial Internet of Things (IoT), and big data analytics to improve productivity, efficiency, and capabilities in the production environment. Modern manufacturing workers not only need to be adept at the traditional manufacturing technologies but also ought to be trained in the advanced data-rich computer-automated technologies. This study analyzes the data science and analytics (DSA) skills gap in today's manufacturing workforce to identify the critical technical skills and domain knowledge required for data science and intelligent manufacturing-related jobs that are highly in-demand in today's manufacturing industry. The gap analysis conducted in this paper on Emsi job posting and profile data provides insights into the trends in manufacturing jobs that leverage data science, automation, cyber, and sensor technologies. These insights will be helpful for educators and industry to train the next generation manufacturing workforce. The main contribution of this paper includes (1) presenting the overall trend in manufacturing job postings in the U.S., (2) summarizing the critical skills and domain knowledge in demand in the manufacturing sector, (3) summarizing skills and domain knowledge reported by manufacturing job seekers, (4) identifying the gaps between demand and supply of skills and domain knowledge, and (5) recognize opportunities for training and upskilling workforce to address the widening skills and knowledge gap.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jmsy.2021.07.007","JIR":8.633,"SJR":2.31},{"Title":"Urban big data fusion based on deep learning: An overview","author":"Jia Liu and Tianrui Li and Peng Xie and Shengdong Du and Fei Teng and Xin Yang","keywords":"Urban computing, Big data, Data fusion, Deep learning","journal":"INFORMATION FUSION","abstract":"Urban big data fusion creates huge values for urban computing in solving urban problems. In recent years, various models and algorithms based on deep learning have been proposed to unlock the power of knowledge from urban big data. To clarify the methodologies of urban big data fusion based on deep learning (DL), this paper classifies them into three categories: DL-output-based fusion, DL-input-based fusion and DL-double-stage-based fusion. These methods use deep learning to learn feature representation from multi-source big data. Then each category of fusion methods is introduced and some examples are shown. The difficulties and ideas of dealing with urban big data will also be discussed.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2019.06.016","JIR":12.975,"SJR":2.776},{"Title":"A survey on big data-driven digital phenotyping of mental health","author":"Yunji Liang and Xiaolong Zheng and Daniel D. Zeng","keywords":"Digital phenotyping, Big data, Mental health, Data mining, Information fusion","journal":"INFORMATION FUSION","abstract":"The landscape of mental health has undergone tremendous changes within the last two decades, but the research on mental health is still at the initial stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays, big data and artificial intelligence offer new opportunities for the screening and prediction of mental problems. In this review paper, we outline the vision of digital phenotyping of mental health (DPMH) by fusing the enriched data from ubiquitous sensors, social media and healthcare systems, and present a broad overview of DPMH from sensing and computing perspectives. We first conduct a systematical literature review and propose the research framework, which highlights the key aspects related with mental health, and discuss the challenges elicited by the enriched data for digital phenotyping. Next, five key research strands including affect recognition, cognitive analytics, behavioral anomaly detection, social analytics, and biomarker analytics are unfolded in the psychiatric context. Finally, we discuss various open issues and the corresponding solutions to underpin the digital phenotyping of mental health.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2019.04.001","JIR":12.975,"SJR":2.776},{"Title":"A survey on deep learning for big data","author":"Qingchen Zhang and Laurence T. Yang and Zhikui Chen and Peng Li","keywords":"Deep learning, Big data, Stacked auto-encoders, Deep belief networks, Convolutional neural networks, Recurrent neural networks","journal":"INFORMATION FUSION","abstract":"Deep learning, as one of the most currently remarkable machine learning techniques, has achieved great success in many applications such as image analysis, speech recognition and text understanding. It uses supervised and unsupervised strategies to learn multi-level representations and features in hierarchical architectures for the tasks of classification and pattern recognition. Recent development in sensor networks and communication technologies has enabled the collection of big data. Although big data provides great opportunities for a broad of areas including e-commerce, industrial control and smart medical, it poses many challenging issues on data mining and information processing due to its characteristics of large volume, large variety, large velocity and large veracity. In the past few years, deep learning has played an important role in big data analytic solutions. In this paper, we review the emerging researches of deep learning models for big data feature learning. Furthermore, we point out the remaining challenges of big data deep learning and discuss the future topics.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2017.10.006","JIR":12.975,"SJR":2.776},{"Title":"Social big data: Recent achievements and new challenges","author":"Gema Bello-Orgaz and Jason J. Jung and David Camacho","keywords":"Big data, Data mining, Social media, Social networks, Social-based frameworks and applications","journal":"INFORMATION FUSION","abstract":"Big data has become an important issue for a large number of research areas such as data mining, machine learning, computational intelligence, information fusion, the semantic Web, and social networks. The rise of different big data frameworks such as Apache Hadoop and, more recently, Spark, for massive data processing based on the MapReduce paradigm has allowed for the efficient utilisation of data mining methods and machine learning algorithms in different domains. A number of libraries such as Mahout and SparkMLib have been designed to develop new efficient applications based on machine learning algorithms. The combination of big data technologies and traditional machine learning algorithms has generated new and interesting challenges in other areas as social media and social networks. These new challenges are focused mainly on problems such as data processing, data storage, data representation, and how data can be used for pattern mining, analysing user behaviours, and visualizing and tracking data, among others. In this paper, we present a revision of the new methodologies that is designed to allow for efficient data mining and information fusion from social media and of the new applications and frameworks that are currently appearing under the \u201cumbrella\u201d of the social networks, social media and big data paradigms.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2015.08.005","JIR":12.975,"SJR":2.776},{"Title":"A survey of data fusion in smart city applications","author":"Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan","keywords":"Data fusion, Sensor fusion, Smart city, Big data, Internet of things, Multi-perspectives classification","journal":"INFORMATION FUSION","abstract":"The advancement of various research sectors such as Internet of Things (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has shed some light in transforming an urban city integrating the aforementioned techniques to a commonly known term - Smart City. With the emergence of smart city, plethora of data sources have been made available for wide variety of applications. The common technique for handling multiple data sources is data fusion, where it improves data output quality or extracts knowledge from the raw data. In order to cater evergrowing highly complicated applications, studies in smart city have to utilize data from various sources and evaluate their performance based on multiple aspects. To this end, we introduce a multi-perspectives classification of the data fusion to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives classification to evaluate selected applications in each domain of the smart city. We conclude the paper by discussing potential future direction and challenges of data fusion integration.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.inffus.2019.05.004","JIR":12.975,"SJR":2.776},{"Title":"A multi-dimension framework for value creation through Big Data","author":"Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante","keywords":"Big Data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation","journal":"INDUSTRIAL MARKETING MANAGEMENT","abstract":"Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2020.03.015","JIR":6.96,"SJR":2.022},{"Title":"Understanding market agility for new product success with big data analytics","author":"Nick Hajli and Mina Tajvidi and Ayantunji Gbadamosi and Waqar Nadeem","keywords":"Big data analytics, Customer agility, Effective use of data, New product success","journal":"INDUSTRIAL MARKETING MANAGEMENT","abstract":"The complexity that characterises the dynamic nature of the various environmental factors makes it very compelling for firms to be capable of addressing the changing customers' needs. The current study examines the role of big data in new product success. We develop a qualitative research with case study approach to look at this. Specifically, we look at multiple cases to get in-depth understanding of customer agility for new product success with big data analytics. The findings of the study provide insight into the role of customer agility in new product success. This study unpacks the interconnectedness of the effective use of data aggregation tools, the effectiveness of data analysis tools and customer agility. It also explores the link between all of these factors and new product success. The study is reasonably telling in that it shows that the effective use of data aggregation and data analysis tools results in customer agility which in itself explains how an organisation senses and responds speedily to opportunities for innovation in the competitive marketing environment. The current study provides significant theoretical contributions by providing evidence for the role of big data analytics, big data aggregation tools, customer agility, organisational slack and environmental turbulence in new product success.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2019.09.010","JIR":6.96,"SJR":2.022},{"Title":"A multi-dimension framework for value creation through big data","author":"Gianluca Elia and Gloria Polimeno and Gianluca Solazzo and Giuseppina Passiante","keywords":"Big data analytics, Cognitive computing, Framework, Model, Systematic literature review, Value creation","journal":"INDUSTRIAL MARKETING MANAGEMENT","abstract":"Big Data represents a promising area for value creation and frontier research. The potential to extract actionable insights from Big Data has gained increasing attention of both academics and practitioners operating in several industries. Marketing domain has become from the start a field for experiments with Big Data approaches, even if the adoption of Big Data solutions does not always generate effective value for the adopters. Therefore, the gap existing between the potential of value creation embedded in the Big Data paradigm and the current limited exploitation of this value represents an area of investigation that this paper aims to explore. In particular, by following a systematic literature review, this study aims at presenting a framework that outlines the multiple value directions that the Big Data paradigm can generate for the adopting organizations. Eleven distinct value directions have been identified and then grouped in five dimensions (Informational, Transactional, Transformational, Strategic, Infrastructural Value), which constitute the pillars of the proposed framework. Finally, the framework has been also preliminarily applied in three case studies conducted within three Italian based companies operating in different industries (e-commerce, fast-moving consumer goods, and banking) in the final aim to see its applicability in real business scenarios.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2019.08.004","JIR":6.96,"SJR":2.022},{"Title":"Linking big data analytical intelligence to customer relationship management performance","author":"Chubing Zhang and Xinchun Wang and Annie Peng Cui and Shenghao Han","keywords":"Big data, Data-driven culture, Competitive pressures, Mass customization, Marketing capability, CRM performance","journal":"INDUSTRIAL MARKETING MANAGEMENT","abstract":"This study investigates the driving forces of a firm's assimilation of big data analytical intelligence (BDAI) and how the assimilation of BDAI improve customer relationship management (CRM) performance. Drawing on the resource-based view, this study argues that a firm's data-driven culture and the competitive pressure it faces in the industry motivate a firm's assimilation of BDAI. As a firm resource, BDAI enables an organization to develop superior mass-customization capability, which in turn positively influences its CRM performance. In addition, this study proposes that a firm's marketing capability can moderate the impact of BDAI assimilation on its mass-customization capability. Using survey data collected from 147 business-to-business companies, this study finds support for most of the hypotheses. The findings of this study uncover compelling insights about the dynamics involved in the process of using BDAI to improve CRM performance.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2020.10.012","JIR":6.96,"SJR":2.022},{"Title":"Telling stories that sell: The role of storytelling and big data analytics in smart service sales","author":"Valeriia Boldosova","keywords":"Storytelling, Big data analytics, Smart service, Customer reference, Customer-supplier relationships","journal":"INDUSTRIAL MARKETING MANAGEMENT","abstract":"The emergence of digitally connected products and big data analytics (BDA) in industrial marketing has attracted academic and managerial interest in smart services. However, suppliers' provision of smart services and customers' adoption of these services have received scarce attention in the literature, demonstrating the need to address the changing nature of customer-supplier interactions in the digital era. Responding to prior research calls, this study utilizes ethnographic research and a storytelling lens to advance our knowledge of how stories and BDA can enhance customers' attitudes toward suppliers' smart services, their behavioral intentions and their actual adoption of smart services. The study's findings demonstrate that storytelling is a collective sensemaking and sensegiving process that occurs in interactions between customers and suppliers in which both parties contribute to the story development. The use of BDA in storytelling enhances customer sensemaking of smart services by highlighting the business value extracted from the digitized data of a reference customer. By synthesizing insights from servitization, storytelling, BDA and the customer reference literature, this study offers managers practical guidance regarding how to increase smart service sales. An example of a story used to facilitate customer adoption of a supplier's smart services in the manufacturing sector is provided.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2019.12.004","JIR":6.96,"SJR":2.022},{"Title":"Using artificial intelligence to detect crisis related to events: Decision making in B2B by artificial intelligence","author":"Aydin Farrokhi and Farid Shirazi and Nick Hajli and Mina Tajvidi","keywords":"Big data, Artificial intelligence, Machine learning, Data mining, Sentiment analytics","journal":"INDUSTRIAL MARKETING MANAGEMENT","abstract":"Artificial Intelligence (AI) could be an important foundation of competitive advantage in the market for firms. As such, firms use AI to achieve deep market engagement when the firm's data are employed to make informed decisions. This study examines the role of computer-mediated AI agents in detecting crises related to events in a firm. A crisis threatens organizational performance; therefore, a data-driven strategy will result in an efficient and timely reflection, which increases the success of crisis management. The study extends the situational crisis communication theory (SCCT) and Attribution theory frameworks built on big data and machine learning capabilities for early detection of crises in the market. This research proposes a structural model composed of a statistical and sentimental big data analytics approach. The findings of our empirical research suggest that knowledge extracted from day-to-day data communications such as email communications of a firm can lead to the sensing of critical events related to business activities. To test our model, we use a publicly available dataset containing 517,401 items belonging to 150 users, mostly senior managers of Enron during 1999 through the 2001 crisis. The findings suggest that the model is plausible in the early detection of Enron's critical events, which can support decision making in the market.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.indmarman.2020.09.015","JIR":6.96,"SJR":2.022},{"Title":"Perception model of surrounding rock geological conditions based on TBM operational big data and combined unsupervised-supervised learning","author":"Xin Yin and Quansheng Liu and Xing Huang and Yucong Pan","keywords":"TBM, Surrounding rock class, Perception model, Unsupervised learning, Supervised learning","journal":"TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY","abstract":"The perception of surrounding rock geological conditions ahead the tunnel face is essential for TBM safe and efficient tunnelling. This paper developed a perception approach of surrounding rock class based on TBM operational big data and combined unsupervised-supervised learning. In data preprocessing, four data mining techniques (i.e., Z-score, K-NN, Kalman filtering, and wavelet packet decomposition) were used to detect outliers, substitute outliers, suppress noise, and extract features, respectively. Then, GMM was used to revise the original surrounding rock class through clustering TBM load parameters and performance parameters in view of the shortcomings of the HC method in the TBM-excavated tunnel. After that, five various ensemble learning classification models were constructed to identify the surrounding rock class, in which model hyper-parameters were automatically tuned by Bayes optimization. In order to evaluate model performance, balanced accuracy, Kappa, F1-score, and training time were taken into account, and a novel multi-metric comprehensive ranking system was designed. Engineering application results indicated that LightGBM achieved the most superior performance with the highest comprehensive score of 6.9066, followed by GBDT (5.9228), XGBoost (5.4964), RF (3.7581), and AdaBoost (0.9946). Through the weighted purity reduction algorithm, the contributions of input features on the five models were quantitatively analyzed. Finally, the impact of class imbalance on model performance was discussed using the ADASYN algorithm, showing that eliminating class imbalance can further improve the model's perception ability.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.tust.2021.104285","JIR":5.915,"SJR":2.172},{"Title":"Tunnel boring machines (TBM) performance prediction: A case study using big data and deep learning","author":"Shangxin Feng and Zuyu Chen and Hua Luo and Shanyong Wang and Yufei Zhao and Lipeng Liu and Daosheng Ling and Liujie Jing","keywords":"TBM performance prediction, Deep belief network (DBN), Yingsong Water Diversion Project, Field penetration index prediction","journal":"TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY","abstract":"This work explores the potential for predicting TBM performance using deep learning. It focuses on a 17.5-km-long tunnel excavated for the Yingsong Water Diversion Project in Northeastern China with its 728\u00a0days\u2019 continuous monitoring of mechanical data. The prediction uses the deep belief network (DBN) proposed by Hinton et al. (2006),on the penetration rate, cutter rotation speed, torque, and thrust force. Field Penetration Index (FPI) is introduced to quantify TBM performance in the field. The DBN algorithm trains on nth number of preceding elements and predicts the performance of the n\u00a0+\u00a01th element. Prior to the implementation of the DBN, a pilot test was performed to find the optimal values for the network structural parameters (number of input nodes, number of hidden layers, number of nodes in the hidden layers, and learning rate). Predictions on FPIs in all the three rock types were then proceeded with good agreement with the field measured data. The mean relative errors for the predicted measured FPIs are generally less than 0.15 and the correlation coefficients (R) can be higher than 0.78. The predicted and measured FPI values along the length of the tunnel graphically follow the same trends. These results confirm the usefulness of big data and the deep learning in predicting TBM performance.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.tust.2020.103636","JIR":5.915,"SJR":2.172},{"Title":"Perspectives of big experimental database and artificial intelligence in tunnel fire research","author":"Xiaoning Zhang and Xiqiang Wu and Younggi Park and Tianhang Zhang and Xinyan Huang and Fu Xiao and Asif Usmani","keywords":"Big data, Empirical model, Deep learning, Critical event, Smart firefighting","journal":"TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY","abstract":"Tunnel fire is one of the most severe global fire hazards and causes a significant amount of economic losses and casualties every year. Over the last 50\u00a0years, numerous full-scale and reduced-scale tunnel fire tests, as well as numerical simulations have been conducted to quantify the critical fire events and key parameters to guide the fire safety design of the tunnel. In light of the recent advances in big data and artificial intelligence, this paper aims to establish a database that contains all existing experimental data of tunnel fire, based on an extensive literature review on tunnel fire tests. This tunnel-fire database summarizes seven key parameters of flame, ventilation, and smoke in that is open access at a GitHub site: https:\/\/github.com\/PolyUFire\/Tunnel_Fire_Database. The test conditions, experimental phenomena, and data of each literature work were organized and categorized in a standard format that could be conveniently accessed and continuously updated. Based on this database, machine learning is applied to predict the critical ventilation velocity of a tunnel fire as a demonstration. The review of the current database not only reveals more valuable information and hidden problems in the conventional collection of test data, but also provides new directions in future tunnel fire research. The established database and methodology help promote the application of artificial intelligence and smart firefighting in tunnel fire safety.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.tust.2020.103691","JIR":5.915,"SJR":2.172},{"Title":"CBI4.0: A cross-layer approach for big data gathering for active monitoring and maintenance in the manufacturing industry 4.0","author":"Muhammad Faheem and Rizwan Aslam Butt and Rashid Ali and Basit Raza and Md. Asri Ngadi and Vehbi Cagri Gungor","keywords":"Internet of things, Industry 4.0, Big data, Multi-channel communication, Wireless sensor network","journal":"JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION","abstract":"Industry 4.0 (I4.0) defines a new paradigm to produce high-quality products at the low cost by reacting quickly and effectively to changing demands in the highly volatile global markets. In Industry 4.0, the adoption of Internet of Things (IoT)-enabled Wireless Sensors (WSs) in the manufacturing processes, such as equipment, machining, assembly, material handling, inspection, etc., generates a huge volume of data known as Industrial Big Data (IBD). However, the reliable and efficient gathering and transmission of this big data from the source sensors to the floor inspection system for the real-time monitoring of unexpected changes in the production and quality control processes is the biggest challenge for Industrial Wireless Sensor Networks (IWSNs). This is because of the harsh nature of the indoor industrial environment that causes high noise, signal fading, multipath effects, heat and electromagnetic interference, which reduces the transmission quality and trigger errors in the IWSNs. Therefore, this paper proposes a novel cross-layer data gathering approach called CBI4.0 for active monitoring and control of manufacturing processes in the Industry 4.0. The key aim of the proposed CBI4.0 scheme is to exploit the multi-channel and multi-radio architecture of the sensor network to guarantee quality of service (QoS) requirements, such as higher data rates, throughput, and low packet loss, corrupted packets, and latency by dynamically switching between different frequency bands in the Multichannel Wireless Sensor Networks (MWSNs). By performing several simulation experiments through EstiNet 9.0 simulator, the performance of the proposed CBI4.0 scheme is compared against existing studies in the automobile Industry 4.0. The experimental outcomes show that the proposed scheme outperforms existing schemes and is suitable for effective control and monitoring of various events in the automobile Industry 4.0.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jii.2021.100236","JIR":10.063,"SJR":2.042},{"Title":"Industrial applications of big data in disruptive innovations supporting environmental reporting","author":"Esa H\u00e4m\u00e4l\u00e4inen and Tommi Inkinen","keywords":"Big data, Disruption, Responsible, Process industry, Economic efficiency, Economic geography","journal":"JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION","abstract":"Disruptive innovations are usually identified as ideas that are created \u2018outside the box\u2019. They are expected to fundamentally change existing business models and processes founded on technological applications. Disruptive innovations can be challenging to define. Information technology (IT) solutions focus on collecting, processing, and reporting different types of data. Commonly, is the solutions are expected (in cybernetics or self-regulating processes) to provide feedback to original processes and to steer them based on the data. To achieve continuous improvement with regard to environmental responsibility and profitability, new thinking and, in particular, accurate and reliable data are needed for decision-making. Very large data storages, known as big data, contain an increasing mass of different types of homogenous and non-homogenous information, as well as extensive time-series. New, innovative algorithms are required to reveal relevant information and opportunities hidden in these data storages. Global environmental challenges and zero-emission responsible production issues can only be solved using relevant and reliable continuous data as the basis. The final goal should be the creation of scalable environmental solutions based on disruptive innovations and accurate data. The aim of this paper is to determine the explicit steps for replacing silo-based reporting with company-wide, refined information, which enables decision-makers in all industries the chance to make responsible choices.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jii.2019.100105","JIR":10.063,"SJR":2.042},{"Title":"Data and knowledge mining with big data towards smart production","author":"Ying Cheng and Ken Chen and Hemeng Sun and Yongping Zhang and Fei Tao","keywords":"Big data, Data mining techniques (DMTs), Production management, Smart manufacturing, Statistical analysis, Knowledge discovery","journal":"JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION","abstract":"Driven by the innovative improvement of information and communication technologies (ICTs) and their applications into manufacturing industry, the big data era in manufacturing is correspondingly arising, and the developing data mining techniques (DMTs) pave the way for pursuing the aims of smart production with the real-time, dynamic, self-adaptive and precise control. However, lots of factors in the ever-changing environment of manufacturing industry, such as, various of complex production processes, larger scale and uncertainties, more complicated constrains, coupling of operational performance, and so on, make production management face with more and more big challenges. The dynamic inflow of a large number of raw data which is collected from the physical manufacturing sites or generated in various related information systems, caused the heavy information overload problems. Indeed, most of traditional DMTs are not yet sufficient to process such big data for smart production management. Therefore, this paper reviews the development of DMTs in the big data era, and makes discussion on the applications of DMTs in production management, by selecting and analyzing the relevant papers since 2010. In the meantime, we point out limitations and put forward some suggestions about the smartness and further applications of DMTs used in production management.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jii.2017.08.001","JIR":10.063,"SJR":2.042},{"Title":"Organizational process maturity model for IoT data quality management","author":"Sunho Kim and Ricardo P\u00e9rez-Castillo and Ismael Caballero and Downgwoo Lee","keywords":"Data quality, Data quality management, IoT, ISO 8000, Process-centric, Process reference model, Maturity, Process maturity, process attribute","journal":"JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION","abstract":"Data quality management (DQM) is one of the most critical aspects to ensure successful applications of the Internet of Things (IoT). So far, most of the approaches for assuring data quality are typically data-centric, i.e., mainly focus on fixing data issues for specific values. However, organizations can also benefit from improving their capabilities of their DQM processes by developing organizational best DQM practices. In this regard, our investigation addresses how well organizations perform their DQM processes in the IoT domain. The main contribution of this study is to establish a framework for IoT DQM maturity. This framework is compliant with ISO 8000-61 (DQM: process reference model) and ISO 8000-62 (DQM: organizational process maturity assessment) and can be used to assess and improve the capabilities of the DQM processes for IoT data. The framework is composed of two elements. First, a process reference model (PRM) for IoT DQM is proposed by extending the PRM for DQM defined in ISO 8000-61, tailoring some existing processes and adding new ones. Second, a maturity model suitable for IoT data is proposed based on the PRM for IoT DQM. The maturity model, named IoT DQM3, is proposed by extending the maturity model defined in ISO 8000-62. However, in order to increase the usability of IoT DQM3, we consider adequate the proposition of a simplification of the IoT DQM3, by introducing a lightweight version to reduce assessment indicators and facilitate its industrial adoption. A simplified method to measure the capability of a process is also suggested considering the relationship of process attributes with the measurement stack defined in ISO 8000-63. The empirical validation of the maturity model is twofold. First, the appropriateness of the two models is surveyed with data quality experts who are currently working in various organizations around the world. Second, in order to demonstrate the feasibility of the proposal, the light-weight version is applied to a manufacturing company as a case study.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jii.2021.100256","JIR":10.063,"SJR":2.042},{"Title":"Improving early OSV design robustness by applying \u2018Multivariate Big Data Analytics\u2019 on a ship's life cycle","author":"Niki Sadat Abbasian and Afshin Salajegheh and Henrique Gaspar and Per Olaf Brett","keywords":"External data, Internal data, Abnormality, Missing data, Outliers, Randomness, Multivariate analysis, Data integration, Clustering","journal":"JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION","abstract":"Typically, only a smaller portion of the monitorable operational data (e.g. from sensors and environment) from Offshore Support Vessels (OSVs) are used at present. Operational data, in addition to equipment performance data, design and construction data, creates large volumes of data with high veracity and variety. In most cases, such data richness is not well understood as to how to utilize it better during design and operation. It is, very often, too time consuming and resource demanding to estimate the final operational performance of vessel concept design solution in early design by applying simulations and model tests. This paper argues that there is a significant potential to integrate ship lifecycle data from different phases of its operation in large data repository for deliberate aims and evaluations. It is disputed discretely in the paper, evaluating performance of real similar type vessels during early stages of the design process, helps substantially improving and fine-tuning the performance criterion of the next generations of vessel design solutions. Producing learning from such a ship lifecycle data repository to find useful patterns and relationships among design parameters and existing fleet real performance data, requires the implementation of modern data mining techniques, such as big data and clustering concepts, which are introduced and applied in this paper. The analytics model introduced suggests and reviews all relevant steps of data knowledge discovery, including pre-processing (integration, feature selection and cleaning), processing (data analyzing) and post processing (evaluating and validating results) in this context.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jii.2018.02.002","JIR":10.063,"SJR":2.042},{"Title":"Garbage in garbage out: The precarious link between IoT and blockchain in food supply chains","author":"Warwick Powell and Marcus Foth and Shoufeng Cao and Val\u00e9ri Natanelov","keywords":"Blockchain, Data quality, Distributed ledger technology, Meat industry, Internet of things, Food supply chains","journal":"JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION","abstract":"The application of blockchain in food supply chains does not resolve conventional IoT data quality issues. Data on a blockchain may simply be immutable garbage. In response, this paper reports our observations and learnings from an ongoing beef supply chain project that integrates Blockchain and IoT for supply chain event tracking and beef provenance assurance and proposes two solutions for data integrity and trust in the Blockchain and IoT-enabled food supply chain. Rather than aiming for absolute truth, we explain how applying the notion of \u2018common knowledge\u2019 fundamentally changes oracle identity and data validity practices. Based on the learnings derived from leading an IoT supply chain project with a focus on beef exports from Australia to China, our findings unshackle IoT and Blockchain from being used merely to collect lag indicators of past states and liberate their potential as lead indicators of desired future states. This contributes: (a) to limit the possibility of capricious claims on IoT data performance, and; (b) to utilise mechanism design as an approach by which supply chain behaviours that increase the probability of desired future states being realised can be encouraged.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jii.2021.100261","JIR":10.063,"SJR":2.042},{"Title":"Physics-informed deep learning for signal compression and reconstruction of big data in industrial condition monitoring","author":"Matthew Russell and Peng Wang","keywords":"Physics-informed deep learning, Prognostics and health management, Data compression, Big data","journal":"MECHANICAL SYSTEMS AND SIGNAL PROCESSING","abstract":"The onset of the Internet of Things enables machines to be outfitted with always-on sensors that can provide health information to cloud-based monitoring systems for prognostics and health management (PHM), which greatly improves reliability and avoids downtime of machines and processes on the shop floor. On the other hand, real-time monitoring produces large amounts of data, leading to significant challenges for efficient and effective data transmission (from the shop floor to the cloud) and analysis (in the cloud). Restricted by industrial hardware capability, especially Internet bandwidth, most solutions approach data transmission from the perspective of data compression (before transmission, at local computing devices) coupled with data reconstruction (after transmission, in the cloud). However, existing data compression techniques may not adapt to domain-specific characteristics of data, and hence have limitations in addressing high compression ratios where full restoration of signal details is important for revealing machine conditions. This study integrates Deep Convolutional Autoencoders (DCAE) with local structure and physics-informed loss terms that incorporate PHM domain knowledge such as the importance of frequency content for machine fault diagnosis. Furthermore, Fault Division Autoencoder Multiplexing (FDAM) is proposed to mitigate the negative effects of multiple disjoint operating conditions on reconstruction fidelity. The proposed methods are evaluated on two case studies, and autocorrelation-based noise analysis provides insight into the relative performance across machine health and operating conditions. Results indicate that physically-informed DCAE compression outperforms prevalent data compression approaches, such as compressed sensing, Principal Component Analysis (PCA), Discrete Cosine Transform (DCT), and DCAE with a standard loss function. FDAM can further improve the data reconstruction quality for certain machine conditions.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ymssp.2021.108709","JIR":6.823,"SJR":2.275},{"Title":"Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm and data pre-processing technology","author":"Shuangqi Li and Hongwen He and Jianwei Li","keywords":"Electric vehicles, Battery energy storage, Temperature-dependent model, Battery management system, Big data, Deep learning","journal":"APPLIED ENERGY","abstract":"As one of the bottleneck technologies of electric vehicles (EVs), the battery hosts complex and hardly observable internal chemical reactions. Therefore, a precise mathematical model is crucial for the battery management system (BMS) to ensure the secure and stable operation of the battery in a multi-variable environment. First, a Cloud-based BMS (C-BMS) is established based on a database containing complete battery status information. Next, a data cleaning method based on machine learning is applied to the big data of batteries. Meanwhile, to improve the model stability under dynamic conditions, an F-divergence-based data distribution quality assessment method and a sampling-based data preprocess method is designed. Then, a lithium-ion battery temperature-dependent model is built based on Stacked Denoising Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training method combined with data preprocessing is also proposed to improve the model accuracy. Finally, to improve reliability, a conjunction working mode between the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied case of the model. Using the battery data extracted from electric buses, the effectiveness and accuracy of the model are validated. The error of the estimated battery terminal voltage is within 2%, and the error of the estimated State of Charge (SoC) is within 3%.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.apenergy.2019.03.154","JIR":9.746,"SJR":3.035},{"Title":"Prospects for energy economy modelling with big data: Hype, eliminating blind spots, or revolutionising the state of the art?","author":"Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan","keywords":"Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy data, Big data","journal":"APPLIED ENERGY","abstract":"Energy economy models are central to decision making on energy and climate issues in the 21st century, such as informing the design of deep decarbonisation strategies under the Paris Agreement. Designing policies that are aimed at achieving such radical transitions in the energy system will require ever more in-depth modelling of end-use demand, efficiency and fuel switching, as well as an increasing need for regional, sectoral, and agent disaggregation to capture technological, jurisdictional and policy detail. Building and using these models entails complex trade-offs between the level of detail, the size of the system boundary, and the available computing resources. The availability of data to characterise key energy system sectors and interactions is also a key driver of model structure and parameterisation, and there are many blind spots and design compromises that are caused by data scarcity. We may soon, however, live in a world of data abundance, potentially enabling previously impossible levels of resolution and coverage in energy economy models. But while big data concepts and platforms have already begun to be used in a number of selected energy research applications, their potential to improve or even completely revolutionise energy economy modelling has been almost completely overlooked in the existing literature. In this paper, we explore the challenges and possibilities of this emerging frontier. We identify critical gaps and opportunities for the field, as well as developing foundational concepts for guiding the future application of big data to energy economy modelling, with reference to the existing literature on decision making under uncertainty, scenario analysis and the philosophy of science.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.apenergy.2019.02.002","JIR":9.746,"SJR":3.035},{"Title":"Investments in data quality: Evaluating impacts of faulty data on asset management in power systems","author":"Sylvie Koziel and Patrik Hilber and Per Westerlund and Ebrahim Shayesteh","keywords":"Asset management, Component replacement, Data quality costs, Electric power distribution, Optimization, Trade-off","journal":"APPLIED ENERGY","abstract":"Data play an essential role in asset management decisions. The amount of data is increasing through accumulating historical data records, new measuring devices, and communication technology, notably with the evolution toward smart grids. Consequently, the management of data quantity and quality is becoming even more relevant for asset managers to meet efficiency and reliability requirements for power grids. In this work, we propose an innovative data quality management framework enabling asset managers (i) to quantify the impact of poor data quality, and (ii) to determine the conditions under which an investment in data quality improvement is required. To this end, an algorithm is used to determine the optimal year for component replacement based on three scenarios, a Reference scenario, an Imperfect information scenario, and an Investment in higher data quality scenario. Our results indicate that (i) the impact on the optimal year of replacement is the highest for middle-aged components; (ii) the profitability of investments in data quality improvement depends on various factors, including data quality, and the cost of investment in data quality improvement. Finally, we discuss the implementation of the proposed models to control data quality in practice, while taking into account real-world technological and economic limitations.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.apenergy.2020.116057","JIR":9.746,"SJR":3.035},{"Title":"Big-data for building energy performance: Lessons from assembling a very large national database of building energy use","author":"Paul A. Mathew and Laurel N. Dunn and Michael D. Sohn and Andrea Mercado and Claudine Custudio and Travis Walter","keywords":"Buildings Performance Database, Building performance, Big data, Building data collection, Data-driven decision support","journal":"APPLIED ENERGY","abstract":"Building energy data has been used for decades to understand energy flows in buildings and plan for future energy demand. Recent market, technology and policy drivers have resulted in widespread data collection by stakeholders across the buildings industry. Consolidation of independently collected and maintained datasets presents a cost-effective opportunity to build a database of unprecedented size. Applications of the data include peer group analysis to evaluate building performance, and data-driven algorithms that use empirical data to estimate energy savings associated with building retrofits. This paper discusses technical considerations in compiling such a database using the DOE Buildings Performance Database (BPD) as a case study. We gathered data on over 750,000 residential and commercial buildings. We describe the process and challenges of mapping and cleansing data from disparate sources. We analyze the distributions of buildings in the BPD relative to the Commercial Building Energy Consumption Survey (CBECS) and Residential Energy Consumption Survey (RECS), evaluating peer groups of buildings that are well or poorly represented, and discussing how differences in the distributions of the three datasets impact use-cases of the data. Finally, we discuss the usefulness and limitations of the current dataset and the outlook for increasing its size and applications.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.apenergy.2014.11.042","JIR":9.746,"SJR":3.035},{"Title":"Big data GIS analysis for novel approaches in building stock modelling","author":"Ren\u00e9 Buffat and Andreas Froemelt and Niko Heeren and Martin Raubal and Stefanie Hellweg","keywords":"Building heat demand, Big data, Large scale modelling, Bottom-up modelling, GIS, Climate data, Spatio-temporal modelling","journal":"APPLIED ENERGY","abstract":"Building heat demand is responsible for a significant share of the total global final energy consumption. Building stock models with a high spatio-temporal resolution are a powerful tool to investigate the effects of new building policies aimed at increasing energy efficiency, the introduction of new heating technologies or the integration of buildings within an energy system based on renewable energy sources. Therefore, building stock models have to be able to model the improvements and variation of used materials in buildings. In this paper, we propose a method based on generalized large-scale geographic information system (GIS) to model building heat demand of large regions with a high temporal resolution. In contrast to existing building stock models, our approach allows to derive the envelope of all buildings from digital elevation models and to model location dependent effects such as shadowing due to the topography and climate conditions. We integrate spatio-temporal climate data for temperature and solar radiation to model climate effects of complex terrain. The model is validated against a database containing the measured energy demand of 1845 buildings of the city of St. Gallen, Switzerland and 120 buildings of the Alpine village of Zernez, Switzerland. The proposed model is able to assess and investigate large regions by using spatial data describing natural and anthropogenic land features. The validation resulted in an average goodness of fit (R2) of 0.6.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.apenergy.2017.10.041","JIR":9.746,"SJR":3.035},{"Title":"Big data in tourism research: A literature review","author":"Jingjing Li and Lizhi Xu and Ling Tang and Shouyang Wang and Ling Li","keywords":"Tourism research, Big data, Literature review, Tourism management, Tourist behavior","journal":"TOURISM MANAGEMENT","abstract":"Even at an early stage, diverse big data have been applied to tourism research and made an amazing improvement. This paper might be the first attempt to present a comprehensive literature review on different types of big data in tourism research. By data sources, the tourism-related big data fall into three primary categories: UGC data (generated by users), including online textual data and online photo data; device data (by devices), including GPS data, mobile roaming data, Bluetooth data, etc.; transaction data (by operations), including web search data, webpage visiting data, online booking data, etc. Carrying different information, different data types address different tourism issues. For each type, a systematical analysis is conducted from the perspectives of research focuses, data characteristics, analytic techniques, major challenges and further directions. This survey facilitates a thorough understanding of this sunrise research and offers valuable insights into its future prospects.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.tourman.2018.03.009","JIR":10.967,"SJR":3.328},{"Title":"Delineation of Nitrogen Signaling Networks: Computational Approaches in the Big Data Era","author":"Yoshiaki Ueda and Shuichi Yanagisawa","keywords":"empty","journal":"MOLECULAR PLANT","abstract":"empty","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.molp.2019.01.008","JIR":13.164,"SJR":4.588},{"Title":"The Role of Big Data and Predictive Analytics in Retailing","author":"Eric T. Bradlow and Manish Gangwar and Praveen Kopalle and Sudhir Voleti","keywords":"Big data, Predictive analytics, Retailing, Pricing","journal":"JOURNAL OF RETAILING","abstract":"The paper examines the opportunities in and possibilities arising from big data in retailing, particularly along five major data dimensions\u2014data pertaining to customers, products, time, (geo-spatial) location and channel. Much of the increase in data quality and application possibilities comes from a mix of new data sources, a smart application of statistical tools and domain knowledge combined with theoretical insights. The importance of theory in guiding any systematic search for answers to retailing questions, as well as for streamlining analysis remains undiminished, even as the role of big data and predictive analytics in retailing is set to rise in importance, aided by newer sources of data and large-scale correlational techniques. The Statistical issues discussed include a particular focus on the relevance and uses of Bayesian analysis techniques (data borrowing, updating, augmentation and hierarchical modeling), predictive analytics using big data and a field experiment, all in a retailing context. Finally, the ethical and privacy issues that may arise from the use of big data in retailing are also highlighted.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jretai.2016.12.004","JIR":5.245,"SJR":3.184},{"Title":"Real-world evaluation of enhanced recovery after surgery: big data under the microscope","author":"Daniel I. McIsaac","keywords":"big data, enhanced recovery, epidemiology, orthopedic surgery, postoperative outcome, study design","journal":"BRITISH JOURNAL OF ANAESTHESIA","abstract":"empty","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.bja.2020.01.012","JIR":9.166,"SJR":2.589},{"Title":"Big data and visual analytics in anaesthesia and health care\u2020","author":"A.F. Simpao and L.M. Ahumada and M.A. Rehman","keywords":"decision support systems, clinical, electronic health records, integrated advanced information management systems, medical informatics","journal":"BRITISH JOURNAL OF ANAESTHESIA","abstract":"Advances in computer technology, patient monitoring systems, and electronic health record systems have enabled rapid accumulation of patient data in electronic form (i.e. big data). Organizations such as the Anesthesia Quality Institute and Multicenter Perioperative Outcomes Group have spearheaded large-scale efforts to collect anaesthesia big data for outcomes research and quality improvement. Analytics\u2014the systematic use of data combined with quantitative and qualitative analysis to make decisions\u2014can be applied to big data for quality and performance improvements, such as predictive risk assessment, clinical decision support, and resource management. Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces, and it can facilitate performance of cognitive activities involving big data. Ongoing integration of big data and analytics within anaesthesia and health care will increase demand for anaesthesia professionals who are well versed in both the medical and the information sciences.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1093\/bja\/aeu552","JIR":9.166,"SJR":2.589},{"Title":"Using big data from air quality monitors to evaluate indoor PM2.5 exposure in buildings: Case study in Beijing","author":"JinXing Zuo and Wei Ji and YuJie Ben and Muhammad Azher Hassan and WenHong Fan and Liam Bates and ZhaoMin Dong","keywords":"Indoor PM, Infiltration factor, Indoor\/outdoor ratio, Beijing","journal":"ENVIRONMENTAL POLLUTION","abstract":"Due to time- and expense- consuming of conventional indoor PM2.5 (particulate matter with aerodynamic diameter of less than 2.5\u202f\u03bcm) sampling, the sample size in previous studies was generally small, which leaded to high heterogeneity in indoor PM2.5 exposure assessment. Based on 4403 indoor air monitors in Beijing, this study evaluated indoor PM2.5 exposure from 15th March 2016 to 14th March 2017. Indoor PM2.5 concentration in Beijing was estimated to be 38.6\u202f\u00b1\u202f18.4\u202f\u03bcg\/m3. Specifically, the concentration in non-heating season was 34.9\u202f\u00b1\u202f15.8\u202f\u03bcg\/m3, which was 24% lower than that in heating season (46.1\u202f\u00b1\u202f21.2\u202f\u03bcg\/m3). A significant correlation between indoor and ambient PM2.5 (p\u202f<\u202f0.05) was evident with an infiltration factor of 0.21, and the ambient PM2.5 contributed approximately 52% and 42% to indoor PM2.5 for non-heating and heating seasons, respectively. Meanwhile, the mean indoor\/outdoor (I\/O) ratio was estimated to be 0.73\u202f\u00b1\u202f0.54. Finally, the adjusted PM2.5 exposure level integrating the indoor and outdoor impact was calculated to be 46.8\u202f\u00b1\u202f27.4\u202f\u03bcg\/m3, which was approximately 42% lower than estimation only relied on ambient PM2.5 concentration. This study is the first attempt to employ big data from commercial air monitors to evaluate indoor PM2.5 exposure and risk in Beijing, which may be instrumental to indoor PM2.5 pollution control.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.envpol.2018.05.030","JIR":8.071,"SJR":2.136},{"Title":"A spatio-temporally weighted hybrid model to improve estimates of personal PM2.5 exposure: Incorporating big data from multiple data sources","author":"YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong","keywords":"Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai","journal":"ENVIRONMENTAL POLLUTION","abstract":"An accurate estimation of population exposure to particulate matter with an aerodynamic diameter <2.5\u202f\u03bcm (PM2.5) is crucial to hazard assessment and epidemiology. This study integrated annual data from 1146 in-home air monitors, air quality monitoring network, public applications, and traffic smart cards to determine the pattern of PM2.5 concentrations and activities in different microenvironments (including outdoors, indoors, subways, buses, and cars). By combining massive amounts of signaling data from cell phones, this study applied a spatio-temporally weighted model to improve the estimation of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor PM2.5 concentration was estimated to be 29.3\u202f\u00b1\u202f27.1\u202f\u03bcg\/m3 (n\u202f=\u202f365), with an average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure was estimated to be 32.1\u202f\u00b1\u202f13.9\u202f\u03bcg\/m3 (n\u202f=\u202f365), with indoor PM2.5 contributing the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai. A heatmap of PM2.5 exposure indicated that the inner-city exposure index was significantly higher than that of the outskirts city, which demonstrated that the importance of spatial differences in population exposure estimation.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.envpol.2019.07.034","JIR":8.071,"SJR":2.136},{"Title":"Dynamic assessment of PM2.5 exposure and health risk using remote sensing and geo-spatial big data","author":"Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed Mahmood","keywords":"Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data, Environmental health","journal":"ENVIRONMENTAL POLLUTION","abstract":"In the past few decades, extensive epidemiological studies have focused on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic diameters less than 2.5\u202f\u03bcm) on public health. However, most of them failed to consider the dynamic changes of population distribution adequately and were limited by the accuracy of PM2.5 estimations. Therefore, in this study, location-based service (LBS) data from social media and satellite-derived high-quality PM2.5 concentrations were collected to perform highly spatiotemporal exposure assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China. The city-scale exposure levels and the corresponding health outcomes were first estimated. Then the uncertainties in exposure risk assessments were quantified based on in-situ PM2.5 observations and static population data. The results showed that approximately half of the population living in the BTH region were exposed to monthly mean PM2.5 concentration greater than 80\u202f\u03bcg\/m3 in 2015, and the highest risk was observed in December. In terms of all-cause, cardiovascular, and respiratory disease, the premature deaths attributed to PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative analysis between five different exposure models further illustrated that the dynamic population distribution and accurate PM2.5 estimations showed great influence on environmental exposure and health assessments and need be carefully considered. Otherwise, the results would be considerably over- or under-estimated.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.envpol.2019.06.057","JIR":8.071,"SJR":2.136},{"Title":"Fungal biodiversity and conservation mycology in light of new technology, big data, and changing attitudes","author":"Lotus A. Lofgren and Jason E. Stajich","keywords":"empty","journal":"CURRENT BIOLOGY","abstract":"Summary\nFungi have successfully established themselves across seemingly every possible niche, substrate, and biome. They are fundamental to biogeochemical cycling, interspecies interactions, food production, and drug bioprocessing, as well as playing less heroic roles as difficult to treat human infections and devastating plant pathogens. Despite community efforts to estimate and catalog fungal diversity, we have only named and described a minute fraction of the fungal world. The identification, characterization, and conservation of fungal diversity is paramount to preserving fungal bioresources, and to understanding and predicting ecosystem cycling and the evolution and epidemiology of fungal disease. Although species and ecosystem conservation are necessarily the foundation of preserving this diversity, there is value in expanding our definition of conservation to include the protection of biological collections, ecological metadata, genetic and genomic data, and the methods and code used for our analyses. These definitions of conservation are interdependent. For example, we need metadata on host specificity and biogeography to understand rarity and set priorities for conservation. To aid in these efforts, we need to draw expertise from diverse fields to tie traditional taxonomic knowledge to data obtained from modern -omics-based approaches, and support the advancement of diverse research perspectives. We also need new tools, including an updated framework for describing and tracking species known only from DNA, and the continued integration of functional predictions to link genetic diversity to functional and ecological diversity. Here, we review the state of fungal diversity research as shaped by recent technological advancements, and how changing viewpoints in taxonomy, -omics, and systematics can be integrated to advance mycological research and preserve fungal biodiversity.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.cub.2021.06.083","JIR":10.834,"SJR":3.822},{"Title":"Data as an asset: What the oil and gas sector can learn from other industries about \u201cBig Data\u201d","author":"Robert K. Perrons and Jesse W. Jensen","keywords":"Big data, Oil and gas, Information technologies, Data","journal":"ENERGY POLICY","abstract":"The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but \u201cBig Data\u201d is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset\u2014but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.enpol.2015.02.020","JIR":6.142,"SJR":2.093},{"Title":"Data as an asset: What the oil and gas sector can learn from other industries about \u201cBig Data\u201d","author":"Robert K. Perrons and Jesse W. Jensen","keywords":"Big data, Oil and gas, Information technologies, Data","journal":"ENERGY POLICY","abstract":"The upstream oil and gas industry has been contending with massive data sets and monolithic files for many years, but \u201cBig Data\u201d is a relatively new concept that has the potential to significantly re-shape the industry. Despite the impressive amount of value that is being realized by Big Data technologies in other parts of the marketplace, however, much of the data collected within the oil and gas sector tends to be discarded, ignored, or analyzed in a very cursory way. This viewpoint examines existing data management practices in the upstream oil and gas industry, and compares them to practices and philosophies that have emerged in organizations that are leading the way in Big Data. The comparison shows that, in companies that are widely considered to be leaders in Big Data analytics, data is regarded as a valuable asset\u2014but this is usually not true within the oil and gas industry insofar as data is frequently regarded there as descriptive information about a physical asset rather than something that is valuable in and of itself. The paper then discusses how the industry could potentially extract more value from data, and concludes with a series of policy-related questions to this end.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.enpol.2015.02.020","JIR":6.142,"SJR":2.093},{"Title":"Energy performance certificates \u2014 New opportunities for data-enabled urban energy policy instruments?","author":"Oleksii Pasichnyi and J\u00f6rgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas","keywords":"Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden","journal":"ENERGY POLICY","abstract":"Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.enpol.2018.11.051","JIR":6.142,"SJR":2.093},{"Title":"Energy performance certificates \u2014 New opportunities for data-enabled urban energy policy instruments?","author":"Oleksii Pasichnyi and J\u00f6rgen Wallin and Fabian Levihn and Hossein Shahrokni and Olga Kordas","keywords":"Energy performance certificate (EPC), Building energy efficiency, Data applications, Data quality, Sweden","journal":"ENERGY POLICY","abstract":"Energy performance certificates (EPC) were introduced in European Union to support reaching energy efficiency targets by informing actors in the building sector about energy efficiency in buildings. While EPC have become a core source of information about building energy, the domains of its applications have not been studied systematically. This partly explains the limitation of conventional EPC data quality studies that fail to expose the essential problems and secure effective use of the data. This study reviews existing applications of EPC data and proposes a new method for assessing the quality of EPCs using data analytics. Thirteen application domains were identified from systematic mapping of 79 papers, revealing increases in the number and complexity of studies and advances in applied data analysis techniques. The proposed data quality assurance method based on six validation levels was tested using four samples of EPC dataset for the case of Sweden. The analysis showed that EPC data can be improved through adding or revising the EPC features and assuring interoperability of EPC datasets. In conclusion, EPC data have wider applications than initially intended by the EPC policy instrument, placing stronger requirements on the quality and content of the data.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.enpol.2018.11.051","JIR":6.142,"SJR":2.093},{"Title":"How big data enriches maritime research \u2013 a critical review of Automatic Identification System (AIS) data applications","author":"Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X. Li","keywords":"AIS data, data mining, navigation safety, ship behaviour analysis, environmental evaluation, advanced applications of AIS data","journal":"TRANSPORT REVIEWS","abstract":"ABSTRACT\nThe information-rich vessel movement data provided by the Automatic Identification System (AIS) has gained much popularity over the past decade, during which the employment of satellite-based receivers has enabled wide coverage and improved data quality. The application of AIS data has developed from simply navigation-oriented research to now include trade flow estimation, emission accounting, and vessel performance monitoring. The AIS now provides high frequency, real-time positioning and sailing patterns for almost the whole world's commercial fleet, and therefore, in combination with supplementary databases and analyses, AIS data has arguably kickstarted the era of digitisation in the shipping industry. In this study, we conduct a comprehensive review of the literature regarding AIS applications by dividing it into three development stages, namely, basic application, extended application, and advanced application. Each stage contains two to three application fields, and in total we identified seven application fields, including (1) AIS data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic shipping. We found that the original application of AIS data to navigation safety has, with the improvement of data accessibility, evolved into diverse applications in various directions. Moreover, we summarised the major methodologies in the literature into four categories, these being (1) data processing and mining, (2) index measurement, (3) causality analysis, and (4) operational research. Undoubtedly, the applications of AIS data will be further expanded in the foreseeable future. This will not only provide a more comprehensive understanding of voyage performance and allow researchers to examine shipping market dynamics from the micro level, but also the abundance of AIS data may also open up the rather opaque aspect of how shipping companies release information to external authorities, including the International Maritime Organization, port states, scientists and researchers. It is expected that more multi-disciplinary AIS studies will emerge in the coming years. We believe that this study will shed further light on the future development of AIS studies.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1080\/01441647.2019.1649315","JIR":9.643,"SJR":3.046},{"Title":"How Big Data Informs Us About Cataract Surgery: The LXXII Edward Jackson Memorial Lecture","author":"Anne Louise Coleman","keywords":"empty","journal":"AMERICAN JOURNAL OF OPHTHALMOLOGY","abstract":"Purpose\nTo characterize the role of Big Data in evaluating quality of care in ophthalmology, to highlight opportunities for studying quality improvement using data available in the American Academy of Ophthalmology Intelligent Research in Sight (IRIS) Registry, and to show how Big Data informs us about rare events such as endophthalmitis after cataract surgery.\nDesign\nReview of published studies, analysis of public-use Medicare claims files from 2010 to 2013, and analysis of IRIS Registry from 2013 to 2014.\nMethods\nStatistical analysis of observational data.\nResults\nThe overall rate of endophthalmitis after cataract surgery was 0.14% in 216 703 individuals in the Medicare database. In the IRIS Registry the endophthalmitis rate after cataract surgery was 0.08% among 511 182 individuals. Endophthalmitis rates tended to be higher in eyes with combined cataract surgery and anterior vitrectomy (P\u00a0= .051), although only 0.08% of eyes had this combined procedure. Visual acuity (VA) in the IRIS Registry in eyes with and without postoperative endophthalmitis measured 1\u20137\u00a0days postoperatively were logMAR 0.58 (standard deviation [SD]: 0.84) (approximately Snellen acuity of 20\/80) and logMAR 0.31 (SD: 0.34) (approximately Snellen acuity of 20\/40), respectively. In 33 547 eyes with postoperative VA after cataract surgery, 18.3% had 1-month-postoperative VA worse than 20\/40.\nConclusions\nBig Data drawing on Medicare claims and IRIS Registry records can help identify additional areas for quality improvement, such as in the 18.3% of eyes in the IRIS Registry having 1-month-postoperative VA worse than 20\/40. The ability to track patient outcomes in Big Data sets provides opportunities for further research on rare complications such as postoperative endophthalmitis and outcomes from uncommon procedures such as cataract surgery combined with anterior vitrectomy. But privacy and data-security concerns associated with Big Data should not be taken lightly.","year":2015,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ajo.2015.09.028","JIR":5.258,"SJR":2.704},{"Title":"Big data sharing and analysis to advance research in post-traumatic epilepsy","author":"Dominique Duncan and Paul Vespa and Asla Pitk\u00e4nen and Adebayo Braimah and Niina Lapinlampi and Arthur W. Toga","keywords":"Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging, TBI","journal":"NEUROBIOLOGY OF DISEASE","abstract":"We describe the infrastructure and functionality for a centralized preclinical and clinical data repository and analytic platform to support importing heterogeneous multi-modal data, automatically and manually linking data across modalities and sites, and searching content. We have developed and applied innovative image and electrophysiology processing methods to identify candidate biomarkers from MRI, EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel analytic tools designed to study epileptogenesis in animal model and human with the goal of tracking the probability of developing epilepsy over time.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.nbd.2018.05.026","JIR":5.996,"SJR":2.205},{"Title":"Use of \u201cbig data\u201d in drug discovery and clinical trials","author":"Guillaume Taglang and David B. Jackson","keywords":"Big data, Drug discovery, Clinical trials, Precision medicine, Biomarkers","journal":"GYNECOLOGIC ONCOLOGY","abstract":"Oncology is undergoing a data-driven metamorphosis. Armed with new and ever more efficient molecular and information technologies, we have entered an era where data is helping us spearhead the fight against cancer. This technology driven data explosion, often referred to as \u201cbig data\u201d, is not only expediting biomedical discovery, but it is also rapidly transforming the practice of oncology into an information science. This evolution is critical, as results to-date have revealed the immense complexity and genetic heterogeneity of patients and their tumors, a sobering reminder of the challenge facing every patient and their oncologist. This can only be addressed through development of clinico-molecular data analytics that provide a deeper understanding of the mechanisms controlling the biological and clinical response to available therapeutic options. Beyond the exciting implications for improved patient care, such advancements in predictive and evidence-based analytics stand to profoundly affect the processes of cancer drug discovery and associated clinical trials.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ygyno.2016.02.022","JIR":5.482,"SJR":2.105},{"Title":"Big data and ophthalmic research","author":"Antony Clark and Jonathon Q. Ng and Nigel Morlet and James B. Semmens","keywords":"data linkage, clinical registry, health services research, ophthalmic epidemiology, big data","journal":"SURVEY OF OPHTHALMOLOGY","abstract":"Large population-based health administrative databases, clinical registries, and data linkage systems are a rapidly expanding resource for health research. Ophthalmic research has benefited from the use of these databases in expanding the breadth of knowledge in areas such as disease surveillance, disease etiology, health services utilization, and health outcomes. Furthermore, the quantity of data available for research has increased exponentially in recent times, particularly as e-health initiatives come online in health systems across the globe. We review some big data concepts, the databases and data linkage systems used in eye research\u2014including their advantages and limitations, the types of studies previously undertaken, and the future direction for big data in eye research.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.survophthal.2016.01.003","JIR":6.048,"SJR":2.131},{"Title":"Big Data and machine learning in radiation oncology: State of the art and future prospects","author":"Jean-Emmanuel Bibault and Philippe Giraud and Anita Burgun","keywords":"Radiation oncology, Big Data, Predictive model, Machine learning","journal":"CANCER LETTERS","abstract":"Precision medicine relies on an increasing amount of heterogeneous data. Advances in radiation oncology, through the use of CT Scan, dosimetry and imaging performed before each fraction, have generated a considerable flow of data that needs to be integrated. In the same time, Electronic Health Records now provide phenotypic profiles of large cohorts of patients that could be correlated to this information. In this review, we describe methods that could be used to create integrative predictive models in radiation oncology. Potential uses of machine learning methods such as support vector machine, artificial neural networks, and deep learning are also discussed.","year":2016,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.canlet.2016.05.033","JIR":8.679,"SJR":2.47},{"Title":"The China Kidney Disease Network (CK-NET): \u201cBig Data\u2014Big Dreams\u201d","author":"Rajiv Saran and Diane Steffick and Jennifer Bragg-Gresham","keywords":"empty","journal":"AMERICAN JOURNAL OF KIDNEY DISEASES","abstract":"empty","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1053\/j.ajkd.2017.04.008","JIR":8.86,"SJR":2.677},{"Title":"Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices","author":"Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch","keywords":"Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson\u2013Siegel model","journal":"JOURNAL OF ECONOMETRICS","abstract":"Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jeconom.2019.04.019","JIR":2.388,"SJR":3.769},{"Title":"Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices","author":"Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch","keywords":"Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson\u2013Siegel model","journal":"JOURNAL OF ECONOMETRICS","abstract":"Nearly all studies that analyze the term structure of interest rates take a two-step approach. First, actual bond prices are summarized by interpolated synthetic zero-coupon yields, and second, some of these yields are used as the source data for further empirical examination. In contrast, we consider the advantages of a one-step approach that directly analyzes the universe of bond prices. To illustrate the feasibility and desirability of the one-step approach, we compare arbitrage-free dynamic term structure models estimated using both approaches. We also provide a simulation study showing that a one-step approach can extract the information in large panels of bond prices and avoid any arbitrary noise introduced from a first-stage interpolation of yields.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jeconom.2019.04.019","JIR":2.388,"SJR":3.769},{"Title":"Chapter Five - Big Data in Drug Discovery","author":"Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard","keywords":"Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials","journal":"JOURNAL OF ECONOMETRICS","abstract":"Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.","year":2018,"type_publication":"incollection","doi":"https:\/\/doi.org\/10.1016\/bs.pmch.2017.12.003","JIR":2.388,"SJR":3.769},{"Title":"Chapter Five - Big Data in Drug Discovery","author":"Nathan Brown and Jean Cambruzzi and Peter J. Cox and Mark Davies and James Dunbar and Dean Plumbley and Matthew A. Sellwood and Aaron Sim and Bryn I. Williams-Jones and Magdalena Zwierzyna and David W. Sheppard","keywords":"Big Data, Artificial intelligence, Drug discovery, Biology, Chemistry, Clinical trials","journal":"JOURNAL OF ECONOMETRICS","abstract":"Interpretation of Big Data in the drug discovery community should enhance project timelines and reduce clinical attrition through improved early decision making. The issues we encounter start with the sheer volume of data and how we first ingest it before building an infrastructure to house it to make use of the data in an efficient and productive way. There are many problems associated with the data itself including general reproducibility, but often, it is the context surrounding an experiment that is critical to success. Help, in the form of artificial intelligence (AI), is required to understand and translate the context. On the back of natural language processing pipelines, AI is also used to prospectively generate new hypotheses by linking data together. We explain Big Data from the context of biology, chemistry and clinical trials, showcasing some of the impressive public domain sources and initiatives now available for interrogation.","year":2018,"type_publication":"incollection","doi":"https:\/\/doi.org\/10.1016\/bs.pmch.2017.12.003","JIR":2.388,"SJR":3.769},{"Title":"What's driving the diffusion of next-generation digital technologies?","author":"Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik","keywords":"empty","journal":"TECHNOVATION","abstract":"The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.technovation.2022.102477","JIR":6.606,"SJR":2.3},{"Title":"What's driving the diffusion of next-generation digital technologies?","author":"Jaehan Cho and Timothy DeStefano and Hanhin Kim and Inchul Kim and Jin Hyun Paik","keywords":"empty","journal":"TECHNOVATION","abstract":"The recent development and diffusion of next-generation digital technologies (NGDTs) such as artificial intelligence, the Internet of Things, big data, 3D printing, and so on are expected to have an immense impact on businesses, innovation, and society. While we know from extant research that a firm's R&D investment, intangible assets, and productivity are factors that influence technology use more generally, to date there is little known about the factors that determine how these emerging tools are used, and by who. Using Probit and OLS modeling on a survey of 12,579 South Korean firms in 2017, we conduct one of the first comprehensive examinations highlighting various firm characteristics that drive NGDT implementation. While much of the literature assesses the use of individual technologies, our research attempts to unveil the extent to which firms implement NGDTs in bundles. Our investigation shows that more than half of the firms that use NGDTs deployed multiple technologies simultaneously. One of the insightful complementarities identified in this research exists amongst technologies that generate, facilitate and demand large sums of data, including big data, IoT, cloud computing and AI. Such technologies also appear important for innovative tools such as 3D printing and robotics.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.technovation.2022.102477","JIR":6.606,"SJR":2.3},{"Title":"Data quality over data quantity in computational cognitive neuroscience","author":"Antonio Kolossa and Bruno Kopp","keywords":"Computational modeling, Functional brain imaging, Signal-to-noise ratio, Reliability, Replicability","journal":"NEUROIMAGE","abstract":"We analyzed factors that may hamper the advancement of computational cognitive neuroscience (CCN). These factors include a particular statistical mindset, which paves the way for the dominance of statistical power theory and a preoccupation with statistical replicability in the behavioral and neural sciences. Exclusive statistical concerns about sampling error occur at the cost of an inadequate representation of the problem of measurement error. We contrasted the manipulation of data quantity (sampling error, by varying the number of subjects) against the manipulation of data quality (measurement error, by varying the number of data per subject) in a simulated Bayesian model identifiability study. The results were clear-cut in showing that - across all levels of signal-to-noise ratios - varying the number of subjects was completely inconsequential, whereas the number of data per subject exerted massive effects on model identifiability. These results emphasize data quality over data quantity, and they call for the integration of statistics and measurement theory.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.neuroimage.2018.01.005","JIR":6.556,"SJR":3.259},{"Title":"Discovering dynamic brain networks from big data in rest and task","author":"Diego Vidaurre and Romesh Abeysuriya and Robert Becker and Andrew J. Quinn and Fidel Alfaro-Almagro and Stephen M. Smith and Mark W. Woolrich","keywords":"empty","journal":"NEUROIMAGE","abstract":"Brain activity is a dynamic combination of the responses to sensory inputs and its own spontaneous processing. Consequently, such brain activity is continuously changing whether or not one is focusing on an externally imposed task. Previously, we have introduced an analysis method that allows us, using Hidden Markov Models (HMM), to model task or rest brain activity as a dynamic sequence of distinct brain networks, overcoming many of the limitations posed by sliding window approaches. Here, we present an advance that enables the HMM to handle very large amounts of data, making possible the inference of very reproducible and interpretable dynamic brain networks in a range of different datasets, including task, rest, MEG and fMRI, with potentially thousands of subjects. We anticipate that the generation of large and publicly available datasets from initiatives such as the Human Connectome Project and UK Biobank, in combination with computational methods that can work at this scale, will bring a breakthrough in our understanding of brain function in both health and disease.","year":2018,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.neuroimage.2017.06.077","JIR":6.556,"SJR":3.259},{"Title":"Construction of a service quality scale for the online food delivery industry","author":"Ching-Chan Cheng and Ya-Yuan Chang and Cheng-Ta Chen","keywords":"Online food delivery, Service quality, Big data analytic, OFD service quality scale","journal":"INTERNATIONAL JOURNAL OF HOSPITALITY MANAGEMENT","abstract":"The main purpose of this study is based on qualitative and quantitative research procedures, and integrates the key service factors for the online food delivery (OFD) industry extracted by Internet Big Data Analytics (IBDA) to construct a OFD service quality scale (OFD-SERV). This study takes OFD customers in Taipei City as the objects. The results show that 20 key service factors for the OFD industry are extracted through IBDA. The OFD-SERV scale contains six dimensions including reliability, maintenance of meal quality and hygiene, assurance, security, system operation and traceability, a total of 28 items. The results from the structural equation modeling showed that the reliability, assurance and system operation have a positive impact on customer satisfaction. Finally, the findings provide knowledge and inspiration for the current OFD, and enable OFD operators and future researchers to more accurately identify the deficiency of service quality.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ijhm.2021.102938","JIR":9.237,"SJR":2.321},{"Title":"Management challenges in creating value from business analytics","author":"Richard Vidgen and Sarah Shaw and David B. Grant","keywords":"Analytics, Delphi, Management challenges, Value creation, Ecosystem","journal":"EUROPEAN JOURNAL OF OPERATIONAL RESEARCH","abstract":"The popularity of big data and business analytics has increased tremendously in the last decade and a key challenge for organizations is in understanding how to leverage them to create business value. However, while the literature acknowledges the importance of these topics little work has addressed them from the organization's point of view. This paper investigates the challenges faced by organizational managers seeking to become more data and information-driven in order to create value. Empirical research comprised a mixed methods approach using (1) a Delphi study with practitioners through various forums and (2) interviews with business analytics managers in three case organizations. The case studies reinforced the Delphi findings and highlighted several challenge focal areas: organizations need a clear data and analytics strategy, the right people to effect a data-driven cultural change, and to consider data and information ethics when using data for competitive advantage. Further, becoming data-driven is not merely a technical issue and demands that organizations firstly organize their business analytics departments to comprise business analysts, data scientists, and IT personnel, and secondly align that business analytics capability with their business strategy in order to tackle the analytics challenge in a systemic and joined-up way. As a result, this paper presents a business analytics ecosystem for organizations that contributes to the body of scholarly knowledge by identifying key business areas and functions to address to achieve this transformation.","year":2017,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.ejor.2017.02.023","JIR":5.334,"SJR":2.161},{"Title":"Data-driven performance analyses of wastewater treatment plants: A review","author":"Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y. Cath","keywords":"Wastewater treatment, Big data, Statistical process control, Process optimization, Monitoring","journal":"WATER RESEARCH","abstract":"Recent advancements in data-driven process control and performance analysis could provide the wastewater treatment industry with an opportunity to reduce costs and improve operations. However, big data in wastewater treatment plants (WWTP) is widely underutilized, due in part to a workforce that lacks background knowledge of data science required to fully analyze the unique characteristics of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated, and co-correlated behavior that (i) is very difficult to model using first principals and (ii) must be considered when implementing data-driven methods. This review provides an overview of data-driven methods of achieving fault detection, variable prediction, and advanced control of WWTP. We present how big data has been used in the context of WWTP, and much of the discussion can also be applied to water treatment. Due to the assumptions inherent in different data-driven modeling approaches (e.g., control charts, statistical process control, model predictive control, neural networks, transfer functions, fuzzy logic), not all methods are appropriate for every goal or every dataset. Practical guidance is given for matching a desired goal with a particular methodology along with considerations regarding the assumed data structure. References for further reading are provided, and an overall analysis framework is presented.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.watres.2019.03.030","JIR":11.236,"SJR":3.099},{"Title":"Impact of water source mixture and population changes on the Al residue in megalopolitan drinking water","author":"Chenhao Tian and Chenghong Feng and Lei Chen and Qixuan Wang","keywords":"Al residue, Mixed water sources, Big data analysis, Megalopolitan, Drinking water","journal":"WATER RESEARCH","abstract":"This study establishes a new understanding of the contributions of Al residue in a megalopolitan drinking water supply system with mixed water sources. The different influences and contributions of foreign water source, resident migration and season changing to Al residue in drinking water were investigated. Especially, the role of Southern water transferred over 1200 km via the South-to-North Water Diversion Project in the Al residue of drinking water supply system of a northern megalopolitan were revealed for the first time. Comparisons of big data on Al residue in the water supply system with sole and mixed water sources showed that the introduction of Southern water enhanced the Al residue in drinking water by over 35%. The world's largest annual residents\u2019 migration during Chinese Lunar New Year and the changes of season affect the water pipework hydrodynamics, which were embodied as the periodic changes of particulate aluminium and the relations with resident's temporal-spatial distribution in the megalopolitan. Because of the differences in water quality, Southern water promotes the release of historically deposited Al and facilitates the cleaning of old pipes.","year":2020,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.watres.2020.116335","JIR":11.236,"SJR":3.099},{"Title":"Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art Review","author":"Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish Shrestha and Partho P. Sengupta and Thomas H. Marwick","keywords":"artificial intelligence, cardiovascular imaging, deep learning, machine learning","journal":"JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY","abstract":"Data science is likely to lead to major changes in cardiovascular imaging. Problems with timing, efficiency, and missed diagnoses occur at all stages of the imaging chain. The application of artificial intelligence (AI) is dependent on robust data; the application of appropriate computational approaches and tools; and validation of its clinical application to image segmentation, automated measurements, and eventually, automated diagnosis. AI may reduce cost and improve value at the stages of image acquisition, interpretation, and decision-making. Moreover, the precision now possible with cardiovascular imaging, combined with \u201cbig data\u201d from the electronic health record and pathology, is likely to better characterize disease and personalize therapy. This review summarizes recent promising applications of AI in cardiology and cardiac imaging, which potentially add value to patient care.","year":2019,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jacc.2018.12.054","JIR":24.094,"SJR":10.315},{"Title":"Advances in optical phenotyping of cereal crops","author":"Dawei Sun and Kelly Robbins and Nicolas Morales and Qingyao Shu and Haiyan Cen","keywords":"cereal crops, high-throughput phenotyping, optical sensors, traits","journal":"TRENDS IN PLANT SCIENCE","abstract":"Optical sensors and sensing-based phenotyping techniques have become mainstream approaches in high-throughput phenotyping for improving trait selection and genetic gains in crops. We review recent progress and contemporary applications of optical sensing-based phenotyping (OSP) techniques in cereal crops and highlight optical sensing principles for spectral response and sensor specifications. Further, we group phenotypic traits determined by OSP into four categories \u2013 morphological, biochemical, physiological, and performance traits \u2013 and illustrate appropriate sensors for each extraction. In addition to the current status, we discuss the challenges of OSP and provide possible solutions. We propose that optical sensing-based traits need to be explored further, and that standardization of the language of phenotyping and worldwide collaboration between phenotyping researchers and other fields need to be established.","year":2022,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.tplants.2021.07.015","JIR":18.313,"SJR":4.587},{"Title":"The potential of remote sensing and artificial intelligence as tools to improve the resilience of agriculture production systems","author":"Jinha Jung and Murilo Maeda and Anjin Chang and Mahendra Bhandari and Akash Ashapure and Juan Landivar-Bowles","keywords":"empty","journal":"CURRENT OPINION IN BIOTECHNOLOGY","abstract":"Modern agriculture and food production systems are facing increasing pressures from climate change, land and water availability, and, more recently, a pandemic. These factors are threatening the environmental and economic sustainability of current and future food supply systems. Scientific and technological innovations are needed more than ever to secure enough food for a fast-growing global population. Scientific advances have led to a better understanding of how various components of the agricultural system interact, from the cell to the field level. Despite incredible advances in genetic tools over the past few decades, our ability to accurately assess crop status in the field, at scale, has been severely lacking until recently. Thanks to recent advances in remote sensing and Artificial Intelligence (AI), we can now quantify field scale phenotypic information accurately and integrate the big data into predictive and prescriptive management tools. This review focuses on the use of recent technological advances in remote sensing and AI to improve the resilience of agricultural systems, and we will present a unique opportunity for the development of prescriptive tools needed to address the next decade\u2019s agricultural and human nutrition challenges.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.copbio.2020.09.003","JIR":9.74,"SJR":2.843},{"Title":"Developing and evaluating a pediatric asthma severity computable phenotype derived from electronic health records","author":"Komal Peer and William G. Adams and Aaron Legler and Megan Sandel and Jonathan I. Levy and Ren\u00e9e Boynton-Jarrett and Chanmin Kim and Jessica H. Leibler and M. Patricia Fabian","keywords":"Asthma, electronic health records, big data, respiratory function tests, selection bias, health care disparities, delivery of health care, observer variation, National Heart, Lung, and Blood Institute (US), pediatrics","journal":"JOURNAL OF ALLERGY AND CLINICAL IMMUNOLOGY","abstract":"Background\nExtensive data available in electronic health records (EHRs) have the potential to improve asthma care and understanding of factors influencing asthma outcomes. However, this work can be accomplished only when the EHR data allow for accurate measures of severity, which at present are complex and inconsistent.\nObjective\nOur aims were to create and evaluate a standardized pediatric asthma severity phenotype based in clinical asthma guidelines for use in EHR-based health initiatives and studies and also to examine the presence and absence of these data in relation to patient characteristics.\nMethods\nWe developed an asthma severity computable phenotype and compared the concordance of different severity components contributing to the phenotype to trends in the literature. We used multivariable logistic regression to assess the presence of EHR data relevant to asthma severity.\nResults\nThe asthma severity computable phenotype performs as expected in comparison with national statistics and the literature. Severity classification for a child is maximized when based on the long-term medication regimen component and minimized when based only on the symptom data component. Use of the severity phenotype results in better, clinically grounded classification. Children for whom severity could be ascertained from these EHR data were more likely to be seen for asthma in the outpatient setting and less likely to be older or Hispanic. Black children were less likely to have lung function testing data present.\nConclusion\nWe developed a pragmatic computable phenotype for pediatric asthma severity that is transportable to other EHRs.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.jaci.2020.11.045","JIR":10.793,"SJR":3.281},{"Title":"Challenges in modeling the emergence of novel pathogens","author":"Emma E. Glennon and Marjolein Bruijning and Justin Lessler and Ian F. Miller and Benjamin L. Rice and Robin N. Thompson and Konstans Wells and C. Jessica E. Metcalf","keywords":"Immune landscape, Genotype to phenotype map, Big data, Data integration, Fundamental theory, Health system functioning","journal":"EPIDEMICS","abstract":"The emergence of infectious agents with pandemic potential present scientific challenges from detection to data interpretation to understanding determinants of risk and forecasts. Mathematical models could play an essential role in how we prepare for future emergent pathogens. Here, we describe core directions for expansion of the existing tools and knowledge base, including: using mathematical models to identify critical directions and paths for strengthening data collection to detect and respond to outbreaks of novel pathogens; expanding basic theory to identify infectious agents and contexts that present the greatest risks, over both the short and longer term; by strengthening estimation tools that make the most use of the likely range and uncertainties in existing data; and by ensuring modelling applications are carefully communicated and developed within diverse and equitable collaborations for increased public health benefit.","year":2021,"type_publication":"article","doi":"https:\/\/doi.org\/10.1016\/j.epidem.2021.100516","JIR":4.396,"SJR":2.023},{"Title":"An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques","author":"Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed","keywords":"Big Data,Internet of Things,Intelligent sensors,Data analysis,Cloud computing,Smart manufacturing,Sensor phenomena and characterization,Big data,health state monitoring,Internet of Things (IoT),noisy data cleaning,real-time systems,sensor selection","journal":"IEEE INTERNET OF THINGS JOURNAL","abstract":"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.","year":2022,"type_publication":"article","doi":"10.1109\/JIOT.2021.3096637","JIR":9.471,"SJR":2.075},{"Title":"An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques","author":"Yu, Wenjin and Liu, Yuehua and Dillon, Tharam and Rahayu, Wenny and Mostafa, Fahed","keywords":"Big Data,Internet of Things,Intelligent sensors,Data analysis,Cloud computing,Smart manufacturing,Sensor phenomena and characterization,Big data,health state monitoring,Internet of Things (IoT),noisy data cleaning,real-time systems,sensor selection","journal":"IEEE INTERNET OF THINGS JOURNAL","abstract":"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.","year":2022,"type_publication":"article","doi":"10.1109\/JIOT.2021.3096637","JIR":9.471,"SJR":2.075},{"Title":"An Incorrect Data Detection Method for Big Data Cleaning of Machinery Condition Monitoring","author":"Xu, Xuefang and Lei, Yaguo and Li, Zeda","keywords":"Big Data,Machinery,Feature extraction,Condition monitoring,Data integrity,Fault diagnosis,Cleaning,Condition-monitoring big data,data cleaning,data quality,incorrect data,local outlier factor (LOF)","journal":"IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS","abstract":"The presence of incorrect data leads to the decrease of condition-monitoring big data quality. As a result, unreliable or misleading results are probably obtained by analyzing these poor-quality data. In this paper, to improve the data quality, an incorrect data detection method based on an improved local outlier factor (LOF) is proposed for data cleaning. First, a sliding window technique is used to divide data into different segments. These segments are considered as different objects and their attributes consist of time-domain statistical features extracted from each segment, such as mean, maximum and peak-to-peak value. Second, a kernel-based LOF (KLOF) is calculated using these attributes to evaluate the degree of each segment being incorrect data. Third, according to these KLOF values and a threshold value, incorrect data are detected. Finally, a simulation of vibration data generated by a defective rolling element bearing and three real cases concerning a fixed-axle gearbox, a wind turbine, and a planetary gearbox are used to verify the effectiveness of the proposed method, respectively. The results demonstrate that the proposed method is able to detect both missing segments and abnormal segments, which are two typical incorrect data, effectively, and thus is helpful for big data cleaning of machinery condition monitoring.","year":2020,"type_publication":"article","doi":"10.1109\/TIE.2019.2903774","JIR":8.236,"SJR":2.393},{"Title":"Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode Processes","author":"Yao, Le and Ge, Zhiqiang","keywords":"Data models,Big Data,Predictive models,Inference algorithms,Prediction algorithms,Semisupervised learning,Computational modeling,Big data,Gaussian mixture model (GMM),multimode process modeling,quality prediction,semisupervised modeling,stochastic variational inference (SVI)","journal":"IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS","abstract":"In this paper, a novel variational inference semisupervised Gaussian mixture model (VI-S2GMM) model is first proposed for semisupervised predictive modeling in multimode processes. Parameters of Gaussian components are identified more accurately with extra unlabeled samples, which improve the prediction performance of the regression model. Since all labeled and unlabeled data samples are involved in each iteration of parameter updating, intractable computing problems occur when facing high-dimension datasets. To tackle this problem, a scalable stochastic VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic gradient optimization algorithm to maximize the evidence of lower bound, the VI-based algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples is randomly selected to update parameters in each iteration, which is more efficient than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data modeling issue. In this way, a large number of unlabeled data can be useful in the modeling, which will further benefit the prediction performance. The SVI-S2GMM is then exploited for the prediction of a quality-related key performance index. Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.","year":2019,"type_publication":"article","doi":"10.1109\/TIE.2018.2856200","JIR":8.236,"SJR":2.393},{"Title":"Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning Over 5G Multimedia Big Data Wireless Networks","author":"Zhang, Xi and Zhu, Qixuan","keywords":"Big Data,Quality of service,Wireless networks,5G mobile communication,Resource management,Wireless sensor networks,5G multimedia big data wireless networks,ICN,NFV,SDN,optimal transmit power,statistical delay-bounded QoS,effective capacity,relay selection","journal":"IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS","abstract":"The multimedia transmission represents a typical big data application in the fifth-generation (5G) wireless networks. However, supporting multimedia big data transmission over 5G wireless networks imposes many new and open challenges because multimedia big data services are both time-sensitive and bandwidth-intensive over time-varying wireless channels with constrained wireless resources. To overcome these difficulties, in this paper we propose the information-centric virtualization architectures for software-defined statistical delay-bounded quality of service (QoS) provisioning over 5G multimedia big data wireless networks. In particular, our proposed schemes integrate the three 5G-promising candidate techniques to guarantee the statistical delay-bounded QoS for multimedia big data transmissions: 1) information-centric network (ICN), to derive the optimal in-network caching locations for multimedia big data; 2) network functions virtualization (NFV), to abstract the PHY-layer infrastructures into several virtualized networks to derive the optimal multimedia data contents delivery paths; and 3) software-defined networks (SDNs), to dynamically reconfigure wireless resources allocation architectures through the SDN-control plane. Under our proposed architectures, to jointly optimize the implementations of NFV and SDN techniques under ICN architectures, we develop the three virtual network selection and transmit-power allocation schemes to: 1) maximize single user's effective capacity; 2) jointly optimize the aggregate effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative gaming among all users, respectively. By simulations and numerical analyses, we show that our proposed architectures and schemes significantly outperform the other existing schemes in supporting the statistical delay-bounded QoS provisioning over the 5G multimedia big data wireless networks.","year":2019,"type_publication":"article","doi":"10.1109\/JSAC.2019.2927088","JIR":9.144,"SJR":2.986}]