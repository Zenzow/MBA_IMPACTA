- Abstract: In a world increasingly connected, and in which information flows quickly
    and affects a very large number of people, sentiment analysis has seen a spectacular
    development over the past ten years. This is due to the fact that the explosion
    of social networks has allowed anyone with internet access to publicly express
    his opinion. Moreover, the emergence of big data has brought enormous opportunities
    and powerful storage and analytics tools to the field of sentiment analysis. However,
    big data introduces new variables and constraints that could radically affect
    the traditional models of sentiment analysis. Therefore, new concerns, such as
    big data quality, have to be addressed to get the most out of big data. To the
    best of our knowledge, no contributions have been published so far which address
    big data quality in SA throughout its different processes. In this paper, we first
    highlight the most important big data quality metrics to consider in any big data
    project. Then, we show how these metrics could be specifically considered in SA
    approaches and this for each phase in the big data value chain.
  Author: El Alaoui, Imane and Gahi, Youssef and Messoussi, Rochdi
  Book Title/Journal: Proceedings of the 2019 International Conference on Big Data
    Engineering
  DOI: 10.1145/3341620.3341629
  JCS_FACTOR: 0.0
  Keywords: Big data, Sentiment analysis, Big data quality metrics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON BIG DATA ENGINEERING
  Title: Big Data Quality Metrics for Sentiment Analysis Approaches
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: With the advent of the information age, big data technology came into
    being. The wide application of big data brings new opportunities and challenges
    to the construction of national defense and military information. Under the background
    of information-based joint operations characterized by large complex systems,
    how to scientifically and rationally plan the construction of large complex systems,
    and maximize the effectiveness of the complex system has become a key concern
    for system construction decision makers and researchers. This paper combines the
    application of big data in the construction of large complex systems, and focuses
    on the evaluation of the effectiveness of large complex systems based on big data,
    which can be used for reference by relevant researchers.
  Author: Zhi-peng, Sun and Gui-ming, Chen and Hui, Zhang
  Book Title/Journal: Proceedings of the 2019 4th International Conference on Big
    Data and Computing
  DOI: 10.1145/3335484.3335545
  JCS_FACTOR: 0.0
  Keywords: effectiveness, large-scale complex systems, evaluation, big data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON BIG DATA AND
    COMPUTING
  Title: Evaluation of Large-Scale Complex Systems Effectiveness Based on Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Big Data presents promising technological and economical opportunities.
    In fact, it has become the raw material of production for many organizations.
    Data is available in large quantities, and it continues generating abundantly.
    However, not all the data will have valuable knowledge. Unreliable sources provide
    misleading and biased information, and even reliable sources could suffer from
    low data quality.In this paper, we propose a novel methodology for the selectability
    of data sources, by both considering the presence and the absence of users' preferences.
    The proposed model integrates multiple factors that affect the reliability of
    data sources, including their quality, gain, cost and coverage. Experimental results
    on real world data-sets, show its capability to find the subset of relevant and
    reliable sources with the lowest cost.
  Author: Safhi, Hicham Moad and Frikh, Bouchra and Ouhbi, Brahim
  Book Title/Journal: Proceedings of the 21st International Conference on Information
    Integration and Web-Based Applications &amp; Services
  DOI: 10.1145/3366030.3366121
  JCS_FACTOR: 0.0
  Keywords: Big Data integration, Big Data Source Selection, Data quality, Source
    reliability
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INFORMATION INTEGRATION
    AND WEB-BASED APPLICATIONS &AMP; SERVICES
  Title: Data Source Selection in Big Data Context
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: University of Colorado Anschutz Medical Campus' Data Science to Patient
    Value Program and 2040 Partners for Health sought to create open learning materials
    for engaged citizens and community leaders regarding big data and big data methods
    to support their collaboration in patient-centered and participatorybased community
    research and evaluation. 2040 is a local nonprofit organization that cultivates
    partnerships in Aurora, Colorado neighborhoods to tackle critical health needs.
    Our goal was to co-design and co-create a series of big data learning modules
    accessible to community laypeople, so they might better understand big data topics
    and be empowered more actively engage in health research and evaluation that uses
    big data methods.
  Author: Schilling, Lisa M. and Pena-Jackson, Griselda and Russell, Seth and Corral,
    Janet and Kwan, Bethany and Ressalam, Julie
  Book Title/Journal: SIGKDD Explor. Newsl.
  DOI: 10.1145/3331651.3331659
  JCS_FACTOR: 0.0
  Keywords: education, co-design, big data, community engagement
  SCI_FACTOR: 0.0
  TITLE_UPPER: SIGKDD EXPLOR. NEWSL.
  Title: Co-Designing Learning Materials to Empower Laypersons to Better Understand
    Big Data and Big Data Methods
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Proliferation of IoT (Internet of Things) and sensor technology has expedited
    the realization of Smart City. To enable necessary functions, sensors distributed
    across the city generate a huge volume of stream data that are crucial for controlling
    Smart City devices. However, due to conditions such as wears and tears, battery
    drain, or malicious attacks, not all data are reliable even when they are accurately
    measured. These data could lead to invalid and devastating consequences (e.g.,
    failed utility or transportation services). The assessment of data reliability
    is necessary and challenging especially for Smart City, as it has to keep up with
    velocity of big data stream to provide up-to-date results. Most research on data
    reliability has focused on data fusion and anomaly detection that lack a quantified
    measure of how much the data over a period of time are adequately reliable for
    decision-makings. This paper alleviates these issues and presents an online approach
    to assessing Big stream data reliability in a timely manner. By employing a well-studied
    evidence-based theory, our approach provides a computational framework that assesses
    data reliability in terms of belief likelihoods. The framework is lightweight
    and easy to scale, deeming fit for streaming data. We evaluate the approach using
    a real application of light sensing data of 1,323,298 instances. The preliminary
    results are consistent with logical rationales, confirming validity of the approach.
  Author: Puangpontip, Supadchaya and Hewett, Rattikorn
  Book Title/Journal: Proceedings of the 2019 3rd International Conference on Big
    Data Research
  DOI: 10.1145/3372454.3372478
  JCS_FACTOR: 0.0
  Keywords: IoT, Theory of evidence, Smart City, Data Reliability
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 3RD INTERNATIONAL CONFERENCE ON BIG DATA RESEARCH
  Title: Assessing Reliability of Big Data Stream for Smart City
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In the era of big data, cloud computing and the Internet of Things, the
    quality of data has tremendous impact on our everyday life. Moreover, the increasing
    velocity, volume and variety of data requires new approaches for quality assessment.
    In this paper, a new approach for quality assessment is presented that applies
    metamorphic testing to data quality. The exemplary application of the approach
    on a big data application shows promising results for the suitability of the approach.
  Author: Auer, Florian and Felderer, Michael
  Book Title/Journal: Proceedings of the 4th International Workshop on Metamorphic
    Testing
  DOI: 10.1109/MET.2019.00019
  JCS_FACTOR: 0.0
  Keywords: metamorphic testing, data quality, quality assessment, metamorphic data
    relations, big data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL WORKSHOP ON METAMORPHIC TESTING
  Title: Addressing Data Quality Problems with Metamorphic Data Relations
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The benefits of deriving useful insights from avalanche of data available
    everywhere cannot be overemphasized. Big Data analytics can revolutionize the
    healthcare industry. It can also ensure functional productivity, help forecast
    and suggest feedbacks to disease outbreaks, enhance clinical practice, and optimize
    healthcare expenditure which cuts across all stakeholders in healthcare sectors.
    Notwithstanding these immense capabilities available in the general application
    of big data; studies on derivation of useful insights from healthcare data that
    can enhance medical practice have received little academic attention. Therefore,
    this study highlighted the possibility of making very insightful healthcare outcomes
    with big data through a simple classification problem which classifies the tendency
    of individuals towards specific drugs based on personality measures. Our model
    though trained with less than 2000 samples and with a simple neural network architecture
    achieved mean accuracies of 76.87% (sd=0.0097) and 75.86% (sd=0.0123) for the
    0.15 and 0.05 validation sets respectively. The relatively acceptable performance
    recorded by our model despite the small dataset could largely be attributed to
    number of attributes in our dataset. It is essential to uncover some of the many
    complexities in our societies in relations to healthcare; and through many machine
    learning architectures like the neural networks these complex relationships can
    be discovered
  Author: Adenuga, Kayode I. and Muniru, Idris O. and Sadiq, Fatai I. and Adenuga,
    Rahmat O. and Solihudeen, Muhammad J.
  Book Title/Journal: Proceedings of the 2019 8th International Conference on Software
    and Information Engineering
  DOI: 10.1145/3328833.3328841
  JCS_FACTOR: 0.0
  Keywords: Challenges, Analytics, Big Data, Benefits
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 8TH INTERNATIONAL CONFERENCE ON SOFTWARE AND
    INFORMATION ENGINEERING
  Title: 'Big Data in Healthcare: Are We Getting Useful Insights from This Avalanche
    of Data?'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Today we experience a data-driven society. All human activities, industrial
    processes and research lead to data generation of unprecedented scale, spurring
    new products, services and businesses. Big Data and its application have been
    a target for European Commission -- with more than 100 FP7 and about 50 H2020
    funded projects under Big Data domain. GATE project aims to establish and sustain
    in the long run a Centre of Excellence as collaborative environment for conducting
    Big Data research and innovation, facilitated by GATE platform and Innovation
    Labs. This paper proposes a conceptual architecture of GATE platform, that is
    holistic, symbiotic, open, evolving and data-integrated. It is also modular and
    with component-based design that allows to position a mix of products and tools
    from different providers. GATE platform will enable start-ups, SMEs and large
    enterprises, as well as other organizations in a wide range of sectors, to build
    advanced Data driven services and applications. The usability of the proposed
    architecture is proven through a development of a sample time series data visualization
    application. Its architecture follows the proposed one through implementation
    of required components using open technology stack.
  Author: Petrova-Antonova, Dessislava and Krasteva, Iva and Ilieva, Sylvia and Pavlova,
    Irena
  Book Title/Journal: Proceedings of the 20th International Conference on Computer
    Systems and Technologies
  DOI: 10.1145/3345252.3345282
  JCS_FACTOR: 0.0
  Keywords: Big Data Value Chain, Emerging Architectures, Smart City, Big Data, GATE
    Platform
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 20TH INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS
    AND TECHNOLOGIES
  Title: Conceptual Architecture of GATE Big Data Platform
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In this paper, the Big Data challenges and the processing is analyzed,
    recently great attention has been paid to the challenges for great data, largely
    due to the wide spread of applications and systems used in real life, such as
    presentation, modeling, processing and large (often unlimited) data storage. Mass
    Data Survey, OLAP Mass Data, Mass Data Dissemination and Mass Data Protection.
    Consequently, we focus on further research trends and, as a default, we will explore
    a future research challenge research project in this area of research.
  Author: Mohammed, Tareq Abed and Ghareeb, Ahmed and Al-bayaty, Hussein and Aljawarneh,
    Shadi
  Book Title/Journal: Proceedings of the Second International Conference on Data Science,
    E-Learning and Information Systems
  DOI: 10.1145/3368691.3368717
  JCS_FACTOR: 0.0
  Keywords: OLAP, data mining, machin learning, big data, data processing
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON DATA SCIENCE,
    E-LEARNING AND INFORMATION SYSTEMS
  Title: 'Big Data Challenges and Achievements: Applications on Smart Cities and Energy
    Sector'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Currently Big Data is the biggest buzzword, and definitely, we believe
    that Big Data is changing the world. Some researchers say Big Data will be even
    bigger buzzword than the Internet. With fast-growing computing resources, information
    and knowledge a new digital globe has emerged. Information is being created and
    stored at a fast rate and is being accessed by a vast range of applications through
    scientific computing, commercial workloads, and social media. In 2018, over 28
    billion devices globally, are connected to the internet. In 2020, more than 50
    billion smart appliances will be connected worldwide and internet traffic flow
    will be 92 times greater than it was in 2005. The usage of such a massive number
    of connected devices not only increase the data volume but also the velocity of
    data addition with speed of light on fiber optic and various wireless networks.
    This fast generation of enormous data creates numerous threats and challenges.
    There exist various approaches that are addressing issues and challenges of Big
    Data with the theory of Vs such as 3 V's, 5 V's, 7 V's etc. The objective of this
    work is to explore and investigate the status of the current Big Data domain.
    Further, a comprehensive overview of Big Data, its characteristics, opportunities,
    issues, and challenges have been explored and described with the help of 51 V's.
    The outcome of this research will help in understanding the Big Data in a systematic
    way.
  Author: Khan, Nawsher and Naim, Arshi and Hussain, Mohammad Rashid and Naveed, Quadri
    Noorulhasan and Ahmad, Naim and Qamar, Shamimul
  Book Title/Journal: Proceedings of the International Conference on Omni-Layer Intelligent
    Systems
  DOI: 10.1145/3312614.3312623
  JCS_FACTOR: 0.0
  Keywords: Big Data, data storage, data generation, data characteristics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON OMNI-LAYER INTELLIGENT
    SYSTEMS
  Title: 'The 51 V''s Of Big Data: Survey, Technologies, Characteristics, Opportunities,
    Issues and Challenges'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: A data revolution has been leading by big data, which have the extremely
    profound influence on the economic, social development and public life. This paper
    introduces the meaning of big data, and discusses the innovation and opportunity
    of enterprise under the perspective of big data. According to the information
    architecture, this paper supplies the basic construction of enterprise big data
    analysis platform, and suggests the strategy of application, which have certain
    realistic directive significance.
  Author: Shen, Shaoyi and Li, Bin and Li, Situo
  Book Title/Journal: Proceedings of the 2019 3rd International Conference on Computer
    Science and Artificial Intelligence
  DOI: 10.1145/3374587.3374650
  JCS_FACTOR: 0.0
  Keywords: Shared, Data asset, Distributed, Analysis of big data, Construction
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE
    AND ARTIFICIAL INTELLIGENCE
  Title: Construction and Application of Big Data Analysis Platform for Enterprise
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'With the development of technology in the era of digital big data in
    the network and the promotion of network technology, big data is simultaneously
    integrated into different industry sectors to achieve Internet performance management,
    and enhance the new perspective of enterprise human resources performance management
    activities. Today''s Internet, cloud computing, Internet of Things and other industrial
    technologies have undergone repeated changes, showing an unprecedented picture.
    At present, the subjective awareness of enterprise human resources performance
    management is too strong, lack of objective data understanding, and the theoretical
    framework of big data human resource management is not fully applied. This paper
    reconstructs the data system from four aspects: data source, collection, integration
    and analysis. Innovate the human resources performance management method from
    the system to provide more scientific and specific ideas for human resource performance
    management.'
  Author: Kun-fa, Li and Jing-chun, Chen and Yan-xi, Wang
  Book Title/Journal: Proceedings of the 2019 2nd International Conference on Information
    Management and Management Sciences
  DOI: 10.1145/3357292.3357302
  JCS_FACTOR: 0.0
  Keywords: Big data, human resources, performance management
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 2ND INTERNATIONAL CONFERENCE ON INFORMATION
    MANAGEMENT AND MANAGEMENT SCIENCES
  Title: Big Data Informatization Applied to Optimization of Human Resource Performance
    Management
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is an important aspect in many fields. In citizen science
    application databases, data quality is often found lacking, which is why there
    needs to be a method of integrating data quality into the design. This paper tackles
    the problem by dividing data quality into separate characteristics according to
    the ISO / IEC 25012 standard. These characteristics are integrated into a conceptual
    model of the system and data model for citizen science applications. Furthermore,
    the paper describes a way to measure data quality using the data quality characteristics.
    The models and measuring methods are theoretical and can be adapted into case
    specific designs.
  Author: Musto, Jiri and Dahanayake, Ajantha
  Book Title/Journal: Proceedings of the 11th International Conference on Management
    of Digital EcoSystems
  DOI: 10.1145/3297662.3365797
  JCS_FACTOR: 0.0
  Keywords: Conceptual model, Data Quality requirements, Citizen science, Data quality,
    Data Quality Characteristics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 11TH INTERNATIONAL CONFERENCE ON MANAGEMENT OF DIGITAL
    ECOSYSTEMS
  Title: Integrating Data Quality Requirements to Citizen Science Application Design
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: As a new type of asset, the value of big data resources can only be realized
    in the transaction circulation. Establishing and improving the big data trading
    platform market system is a systematic project to transform data from resources
    into assets. Through the comparative analysis of several typical big data trading
    platform construction practices in China, this research finds some problems, such
    as unclear positioning of some platforms leading to overlapping functions, extensive
    data transaction, lack of unified data pricing methods, unclear data ownership.
    In addition, there is no difference between the data escrow transaction mode and
    the aggregate transaction mode, and the rights of the data supply parties cannot
    be guaranteed. And it also discusses how to promote the construction and improvement
    of China's big data trading market more systematically, normally and institutionally.
    Finally, it proposes to build local big data trading platforms according to local
    conditions, establish a data transaction system based on blockchain, establish
    a big data transaction pricing index system, and establish a big data standard
    system.
  Author: Yu, Bangbo and Zhao, Haijun
  Book Title/Journal: Proceedings of the 2019 4th International Conference on Intelligent
    Information Technology
  DOI: 10.1145/3321454.3321474
  JCS_FACTOR: 0.0
  Keywords: regulatory construction, big data trading platform, Data assets
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON INTELLIGENT
    INFORMATION TECHNOLOGY
  Title: Research on the Construction of Big Data Trading Platform in China
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data too sensitive to be "open" for analysis and re-purposing typically
    remains "closed" as proprietary information. This dichotomy undermines efforts
    to make algorithmic systems more fair, transparent, and accountable. Access to
    proprietary data in particular is needed by government agencies to enforce policy,
    researchers to evaluate methods, and the public to hold agencies accountable;
    all of these needs must be met while preserving individual privacy and firm competitiveness.
    In this paper, we describe an integrated legal-technical approach provided by
    a third-party public-private data trust designed to balance these competing interests.
    Basic membership allows firms and agencies to enable low-risk access to data for
    compliance reporting and core methods research, while modular data sharing agreements
    support a wide array of projects and use cases. Unless specifically stated otherwise
    in an agreement, all data access is initially provided to end users through customized
    synthetic datasets that offer a) strong privacy guarantees, b) removal of signals
    that could expose competitive advantage, and c) removal of biases that could reinforce
    discriminatory policies, all while maintaining fidelity to the original data.
    We find that using synthetic data in conjunction with strong legal protections
    over raw data strikes a balance between transparency, proprietorship, privacy,
    and research objectives. This legal-technical framework can form the basis for
    data trusts in a variety of contexts.
  Author: Young, Meg and Rodriguez, Luke and Keller, Emily and Sun, Feiyang and Sa,
    Boyang and Whittington, Jan and Howe, Bill
  Book Title/Journal: Proceedings of the Conference on Fairness, Accountability, and
    Transparency
  DOI: 10.1145/3287560.3287577
  JCS_FACTOR: 0.0
  Keywords: data ethics, data governance, data sharing, algorithmic bias, privacy
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY
  Title: 'Beyond Open vs. Closed: Balancing Individual Privacy and Public Accountability
    in Data Sharing'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "The way people walk is a strong correlate of their identity. Several\
    \ studies have shown that both humans and machines can recognize individuals just\
    \ by their gait, given that proper measurements of the observed motion patterns\
    \ are available. For surveillance applications, gait is also attractive, because\
    \ it does not require active collaboration from users and is hard to fake. However,\
    \ the acquisition of good-quality measures of a person\xE2\u20AC\u2122s motion\
    \ patterns in unconstrained environments, (e.g., in person re-identification applications)\
    \ has proved very challenging in practice. Existing technology (video cameras)\
    \ suffer from changes in viewpoint, daylight, clothing, accessories, and other\
    \ variations in the person\xE2\u20AC\u2122s appearance. Novel three-dimensional\
    \ sensors are bringing new promises to the field, but still many research issues\
    \ are open. This article presents a survey of the work done in gait analysis for\
    \ re-identification in the past decade, looking at the main approaches, datasets,\
    \ and evaluation methodologies. We identify several relevant dimensions of the\
    \ problem and provide a taxonomic analysis of the current state of the art. Finally,\
    \ we discuss the levels of performance achievable with the current technology\
    \ and give a perspective of the most challenging and promising directions of research\
    \ for the future."
  Author: Nambiar, Athira and Bernardino, Alexandre and Nascimento, Jacinto C.
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3243043
  JCS_FACTOR: 0.0
  Keywords: person re-identification, Video surveillance, gait analysis, biometrics,
    machine learning, computer vision
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: 'Gait-Based Person Re-Identification: A Survey'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Data Deduplication
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: Advancements in digital civics have enabled leaders to engage and gather
    input from a broader spectrum of the public. However, less is known about the
    analysis process around community input and the challenges faced by civic leaders
    as engagement practices scale up. To understand these challenges, we conducted
    21 interviews with leaders on civic-oriented projects. We found that at a small-scale,
    civic leaders manage to facilitate sensemaking through collaborative or individual
    approaches. However, as civic leaders scale engagement practices to account for
    more diverse perspectives, making sense of the large quantity of qualitative data
    becomes a challenge. Civic leaders could benefit from training in qualitative
    data analysis and simple, scalable collaborative analysis tools that would help
    the community form a shared understanding. Drawing from these insights, we discuss
    opportunities for designing tools that could improve civic leaders' ability to
    utilize and reflect public input in decisions.
  Author: Mahyar, Narges and Nguyen, Diana V. and Chan, Maggie and Zheng, Jiayi and
    Dow, Steven P.
  Book Title/Journal: Proceedings of the 2019 on Designing Interactive Systems Conference
  DOI: 10.1145/3322276.3322354
  JCS_FACTOR: 0.0
  Keywords: qualitative dataanalysis, community engagement, public inpu, digital civics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 ON DESIGNING INTERACTIVE SYSTEMS CONFERENCE
  Title: 'The Civic Data Deluge: Understanding the Challenges of Analyzing Large-Scale
    Community Input'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'A map of potential prevalence of Chagas disease (ChD) with high spatial
    disaggregation is presented. It aims to detect areas outside the Gran Chaco ecoregion
    (hyperendemic for the ChD), characterized by high affinity with ChD and high health
    vulnerability.To quantify potential prevalence, we developed several indicators:
    an Affinity Index which quantifies the degree of linkage between endemic areas
    of ChD and the rest of the country. We also studied favorable habitability conditions
    for Triatoma infestans, looking for areas where the predominant materials of floors,
    roofs and internal ceilings favor the presence of the disease vector.We studied
    determinants of a more general nature that can be encompassed under the concept
    of Health Vulnerability Index. These determinants are associated with access to
    health providers and the socio-economic level of different segments of the population.Finally
    we constructed a Chagas Potential Prevalence Index (ChPPI) which combines the
    affinity index, the health vulnerability index, and the population density. We
    show and discuss the maps obtained. These maps are intended to assist public health
    specialists, decision makers of public health policies and public officials in
    the development of cost-effective strategies to improve access to diagnosis and
    treatment of ChD.'
  Author: Vazquez Brust, Antonio and Olego, Tom\'{a}s and Rosati, Germ\'{a}n and Lang,
    Carolina and Bozzoli, Guillermo and Weinberg, Diego and Chuit, Roberto and Minnoni,
    Martin and Sarraute, Carlos
  Book Title/Journal: Companion Proceedings of The 2019 World Wide Web Conference
  DOI: 10.1145/3308560.3316485
  JCS_FACTOR: 0.0
  Keywords: epidemics, call detail records, migrations, health vulnerability, social
    network analysis, neglected tropical diseases, Chagas disease
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMPANION PROCEEDINGS OF THE 2019 WORLD WIDE WEB CONFERENCE
  Title: Detecting Areas of Potential High Prevalence of Chagas in Argentina
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Much existing knowledge about global consumption of peer-produced information
    goods is supported by data on Wikipedia page view counts and surveys. In 2017,
    the Wikimedia Foundation began measuring the time readers spend on a given page
    view (dwell time), enabling a more detailed understanding of such reading patterns.
    In this paper, we validate and model this new data source and, building on existing
    findings, use regression analysis to test hypotheses about how patterns in reading
    time vary between global contexts. Consistent with prior findings from self-report
    data, our complementary analysis of behavioral data provides evidence that Global
    South readers are more likely to use Wikipedia to gain in-depth understanding
    of a topic. We find that Global South readers spend more time per page view and
    that this difference is amplified on desktop devices, which are thought to be
    better suited for in-depth information seeking tasks.
  Author: TeBlunthuis, Nathan and Bayer, Tilman and Vasileva, Olga
  Book Title/Journal: Proceedings of the 15th International Symposium on Open Collaboration
  DOI: 10.1145/3306446.3340829
  JCS_FACTOR: 0.0
  Keywords: readership, digital divides, peer production, web analytics, dwell time,
    Wikipedia, quantitative methods
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 15TH INTERNATIONAL SYMPOSIUM ON OPEN COLLABORATION
  Title: 'Dwelling on Wikipedia: Investigating Time Spent by Global Encyclopedia Readers'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The number of applications being developed that require access to knowledge
    about the real world has increased rapidly over the past two decades. Domain ontologies,
    which formalize the terms being used in a discipline, have become essential for
    research in areas such as Machine Learning, the Internet of Things, Robotics,
    and Natural Language Processing, because they enable separate systems to exchange
    information. The quality of these domain ontologies, however, must be ensured
    for meaningful communication. Assessing the quality of domain ontologies for their
    suitability to potential applications remains difficult, even though a variety
    of frameworks and metrics have been developed for doing so. This article reviews
    domain ontology assessment efforts to highlight the work that has been carried
    out and to clarify the important issues that remain. These assessment efforts
    are classified into five distinct evaluation approaches and the state of the art
    of each described. Challenges associated with domain ontology assessment are outlined
    and recommendations are made for future research and applications.
  Author: McDaniel, Melinda and Storey, Veda C.
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3329124
  JCS_FACTOR: 0.0
  Keywords: ontology development, ontology application, applied ontology, Ontology,
    metrics, evaluation, task-ontology fit, domain ontology, assessment
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: 'Evaluating Domain Ontologies: Clarification, Classification, and Challenges'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Detecting erroneous values is a key step in data cleaning. Error detection
    algorithms usually require a user to provide input configurations in the form
    of rules or statistical parameters. However, providing a complete, yet correct,
    set of configurations for each new dataset is not trivial, as the user has to
    know about both the dataset and the error detection algorithms upfront. In this
    paper, we present Raha, a new configuration-free error detection system. By generating
    a limited number of configurations for error detection algorithms that cover various
    types of data errors, we can generate an expressive feature vector for each tuple
    value. Leveraging these feature vectors, we propose a novel sampling and classification
    scheme that effectively chooses the most representative values for training. Furthermore,
    our system can exploit historical data to filter out irrelevant error detection
    algorithms and configurations. In our experiments, Raha outperforms the state-of-the-art
    error detection techniques with no more than 20 labeled tuples on each dataset.
  Author: Mahdavi, Mohammad and Abedjan, Ziawasch and Castro Fernandez, Raul and Madden,
    Samuel and Ouzzani, Mourad and Stonebraker, Michael and Tang, Nan
  Book Title/Journal: Proceedings of the 2019 International Conference on Management
    of Data
  DOI: 10.1145/3299869.3324956
  JCS_FACTOR: 0.0
  Keywords: label propagation, clustering, semi-supervised learning, data cleaning,
    historical data, classification, machine learning, error detection
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA
  Title: 'Raha: A Configuration-Free Error Detection System'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "Context: End-user service composition (EUSC) is a service-oriented paradigm\
    \ that aims to empower end users and allow them to compose their own web applications\
    \ from reusable service components. User studies have been used to evaluate EUSC\
    \ tools and processes. Such an approach should benefit software development, because\
    \ incorporating end users\xE2\u20AC\u2122 feedback into software development should\
    \ make software more useful and usable. Problem: There is a gap in our understanding\
    \ of what constitutes a user study and how a good user study should be designed,\
    \ conducted, and reported. Goal: This article aims to address this gap. Method:\
    \ The article presents a systematic review of 47 selected user studies for EUSC.\
    \ Guided by a review framework, the article systematically and consistently assesses\
    \ the focus, methodology and cohesion of each of these studies. Results: The article\
    \ concludes that the focus of these studies is clear, but their methodology is\
    \ incomplete and inadequate, their overall cohesion is poor. The findings lead\
    \ to the development of a design framework and a set of questions for the design,\
    \ reporting, and review of good user studies for EUSC. The detailed analysis and\
    \ the insights obtained from the analysis should be applicable to the design of\
    \ user studies for service-oriented systems as well and indeed for any user studies\
    \ related to software artifacts."
  Author: Zhao, Liping and Loucopoulos, Pericles and Kavakli, Evangelia and Letsholo,
    Keletso J.
  Book Title/Journal: ACM Trans. Web
  DOI: 10.1145/3340294
  JCS_FACTOR: 0.0
  Keywords: web services, mapshups, User studies, qualitative studies, review framework,
    design guideline, service-oriented computing, empirical studies, systematic review,
    end-user service composition
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM TRANS. WEB
  Title: 'User Studies on End-User Service Composition: A Literature Review and a
    Design Framework'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: This article proposes a class of dependencies for graphs, referred to
    as graph entity dependencies (GEDs). A GED is defined as a combination of a graph
    pattern and an attribute dependency. In a uniform format, GEDs can express graph
    functional dependencies with constant literals to catch inconsistencies, and keys
    carrying id literals to identify entities (vertices) in a graph. We revise the
    chase for GEDs and prove its Church-Rosser property. We characterize GED satisfiability
    and implication, and establish the complexity of these problems and the validation
    problem for GEDs, in the presence and absence of constant literals and id literals.
    We also develop a sound, complete and independent axiom system for finite implication
    of GEDs. In addition, we extend GEDs with built-in predicates or disjunctions,
    to strike a balance between the expressive power and complexity. We settle the
    complexity of the satisfiability, implication, and validation problems for these
    extensions.
  Author: Fan, Wenfei and Lu, Ping
  Book Title/Journal: ACM Trans. Database Syst.
  DOI: 10.1145/3287285
  JCS_FACTOR: 0.0
  Keywords: validation, Graph dependencies, implication, axiom system, satisfiability,
    EGDs, conditional functional dependencies, disjunction, TGDs, keys, built-in predicates
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM TRANS. DATABASE SYST.
  Title: Dependencies for Graphs
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Scripts are widely used to design and run scientific experiments. Scripting
    languages are easy to learn and use, and they allow complex tasks to be specified
    and executed in fewer steps than with traditional programming languages. However,
    they also have important limitations for reproducibility and data management.
    As experiments are iteratively refined, it is challenging to reason about each
    experiment run (or trial), to keep track of the association between trials and
    experiment instances as well as the differences across trials, and to connect
    results to specific input data and parameters. Approaches have been proposed that
    address these limitations by collecting, managing, and analyzing the provenance
    of scripts. In this article, we survey the state of the art in provenance for
    scripts. We have identified the approaches by following an exhaustive protocol
    of forward and backward literature snowballing. Based on a detailed study, we
    propose a taxonomy and classify the approaches using this taxonomy.
  Author: Pimentel, Jo\~{a}o Felipe and Freire, Juliana and Murta, Leonardo and Braganholo,
    Vanessa
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3311955
  JCS_FACTOR: 0.0
  Keywords: analyzing, managing, Provenance, collecting, survey, scripts
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: A Survey on Collecting, Managing, and Analyzing Provenance from Scripts
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: It is our great pleasure to welcome you to &lt;I&gt;The Web Conference
    2019&lt;/I&gt;. The Web Conference is the premier venue focused on understanding
    the current state and the evolution of the Web through the lens of computer science,
    computational social science, economics, policy, and many other disciplines. The
    2019 edition of the conference is a reflection point as we celebrate the 30th
    anniversary of the Web.
  Author: empty
  Book Title/Journal: empty
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: EMPTY
  Title: 'WWW ''19: Companion Proceedings of The 2019 World Wide Web Conference'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: proceedings
  Year: 2019
- Abstract: The Handbook of Multimodal-Multisensor Interfaces provides the first authoritative
    resource on what has become the dominant paradigm for new computer interfaces---user
    input involving new media (speech, multi-touch, hand and body gestures, facial
    expressions, writing) embedded in multimodal-multisensor interfaces.This three-volume
    handbook is written by international experts and pioneers in the field. It provides
    a textbook, reference, and technology roadmap for professionals working in this
    and related areas.This third volume focuses on state-of-the-art multimodal language
    and dialogue processing, including semantic integration of modalities. The development
    of increasingly expressive embodied agents and robots has become an active test-bed
    for coordinating multimodal dialogue input and output, including processing of
    language and nonverbal communication. In addition, major application areas are
    featured for commercializing multimodal-multisensor systems, including automotive,
    robotic, manufacturing, machine translation, banking, communications, and others.
    These systems rely heavily on software tools, data resources, and international
    standards to facilitate their development. For insights into the future, emerging
    multimodal-multisensor technology trends are highlighted for medicine, robotics,
    interaction with smart spaces, and similar topics. Finally, this volume discusses
    the societal impact of more widespread adoption of these systems, such as privacy
    risks and how to mitigate them. The handbook chapters provide a number of walk-through
    examples of system design and processing, information on practical resources for
    developing and evaluating new systems, and terminology and tutorial support for
    mastering this emerging field. In the final section of this volume, experts exchange
    views on a timely and controversial challenge topic, and how they believe multimodal-multisensor
    interfaces need to be equipped to most effectively advance human performance during
    the next decade.
  Author: empty
  Book Title/Journal: empty
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: EMPTY
  Title: 'The Handbook of Multimodal-Multisensor Interfaces: Language Processing,
    Software, Commercialization, and Emerging Directions'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: book
  Year: 2019
- Abstract: Based on understanding the application of big data and the research status
    of crop germplasm resources, this paper proposes a system architecture that is
    suitable for crop germplasm resources big data. Among them, the overall architecture
    of germplasm resources is elaborated through six functional modules, including
    data source, data integration, data processing, data application, big data operation
    and maintenance platform, and data management and security. The logical functional
    architecture specification was formulated and the technical implementation and
    selection are defined. The technical implementation framework describes the technical
    implementation of germplasm resources big data, and jointly supports the construction
    and operation of germplasm resources big data. Finally, a verification system
    is established to verify the feasibility of the big data system framework for
    germplasm resources.
  Author: Jing, Furong and Cao, Yongsheng and Fang, Wei and Chen, Yanqing
  Book Title/Journal: Proceedings of the 3rd International Conference on Computer
    Science and Application Engineering
  DOI: 10.1145/3331453.3361308
  JCS_FACTOR: 0.0
  Keywords: Data analysis, Crop germplasm resources, Data management, Big data architecture
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE
    AND APPLICATION ENGINEERING
  Title: Construction and Implementation of Big Data Framework for Crop Germplasm
    Resources
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Agricultural meteorological disasters, including floods, droughts, dry
    hot winds, low temperature chills, typhoons, hail and continuous rain, can lead
    to significant reduction in agricultural output. Big data platform for early warning
    of agricultural meteorological disaster is the basis of business operation system
    for early warning of agricultural meteorological disasters, and the data quality
    is an important guarantee for success of the early warning. Quality control of
    big data for early warning of agricultural meteorological disaster involves names
    of data sets, metadata, data documents and content of data sets. The quality control
    for contents of data sets is divided into quality control of attribute data and
    that of spatial data, and quality control of spatial data is divided into quality
    control of vector data and that of raster data. Methods for data quality control
    are divided into fully automatic, semi-automatic and full manual control methods.
  Author: Li, Jiale and Liao, Shunbao
  Book Title/Journal: Proceedings of the 2019 International Conference on Artificial
    Intelligence and Computer Science
  DOI: 10.1145/3349341.3349371
  JCS_FACTOR: 0.0
  Keywords: framework, quality control, big data, agro-meteorological disasters, early
    warning
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
    AND COMPUTER SCIENCE
  Title: Quality Control Framework of Big Data for Early Warning of Agricultural Meteorological
    Disasters
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: This paper focuses on big data management and analytics in intelligent
    smart environments, with particular regards to intelligent transportation and
    logistics systems, and provides relevant research directions that may represent
    a milestone for future years.
  Author: Cuzzocrea, Alfredo
  Book Title/Journal: Proceedings of the 21st International Conference on Information
    Integration and Web-Based Applications &amp; Services
  DOI: 10.1145/3366030.3366044
  JCS_FACTOR: 0.0
  Keywords: Big data management, Intelligent smart environments, Big data analytics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 21ST INTERNATIONAL CONFERENCE ON INFORMATION INTEGRATION
    AND WEB-BASED APPLICATIONS &AMP; SERVICES
  Title: 'Big Data Management and Analytics in Intelligent Smart Environments: State-of-the-Art
    Analysis and Future Research Directions'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The integration and application of aerospace product quality data resources
    is an important way to carry out quality improvement, quality evaluation and precise
    management. Standardization is the basis for promoting quality data resources
    integration. The unified and normative standard system is the guarantee for efficient
    development of integration standards. Firstly, we analyzed the features of quality
    data resources according to the status quo of integration. Integration structure
    of quality data resources in terms of vertical and horizontal integration was
    proposed by adopting the methods of "decomposition-integration" and "classification-association".
    Secondly, we constructed a three-dimensions architecture of quality data resource
    integration using the method of system engineering methodology, from the layer
    dimension (basis, common, special), technical dimension (description, collection,
    storage, transmission, processing, comprehensive management) and category dimension
    (rocket, spacecraft). Thirdly, we worked out 20 lists about basis and common standard
    by adopting the top-down approach. Some standard development suggestions are proposed
    based on the characteristics of quality data resources and standard research strategies.
    Finally, we applied the quality problem data resource standard construction and
    application to verify the proposed method.
  Author: Jia, Fengsheng and Gao, Yang and Wang, Yuming
  Book Title/Journal: Proceedings of the 2019 International Conference on Big Data
    Engineering
  DOI: 10.1145/3341620.3341624
  JCS_FACTOR: 0.0
  Keywords: system planning, standard system, quality data resource integration, aerospace
    products
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON BIG DATA ENGINEERING
  Title: Study on Standard System of Aerospace Quality Data Resources Integration
    under the Background of Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: At the space launch site, the big data of the launch support system comes
    from the construction of the launch site, the ground service, the comprehensive
    support process, and launch mission organization and command. The big data is
    extensive sources, various types, large scale, and rapid growth. The big data
    application can improve the data processing and management efficiency for the
    launch support system. Then the application can enhance the support capability
    of flight mission and success rate. This paper analyzes the existing data application
    of launch support system. The challenges and requirements of big data application
    are studied by the construction of intelligent launch site. The application pattern
    and target are put forward from four aspects of launch mission organization and
    command, mission application, comprehensive support, and information security.
    The classification of big data is proposed for a launch support system. The architecture
    of big data application system is designed, which meets the application pattern
    and target. It lays a foundation for the future big data project at the launch
    site.
  Author: Dong, Wei and Xiao, Litian and Niu, Shengfen and Niu, Jianjun and Wang,
    Fei
  Book Title/Journal: Proceedings of the 3rd International Conference on Computer
    Science and Application Engineering
  DOI: 10.1145/3331453.3360973
  JCS_FACTOR: 0.0
  Keywords: Space launch site, Application research, Launch support system, Big data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE
    AND APPLICATION ENGINEERING
  Title: Application Research of Big Data for Launch Support System at Space Launch
    Site
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality assessment plays an important role in electricity consumption
    big data. It can help business people master the overall data situation, which
    can provide a strong guarantee for subsequent data improvement, analysis and decision.
    According to the electrical data quality issues, we design a rule-based data quality
    assessment architecture for electrical big data. It includes six types of data
    quality assessment indexes (such as comprehensiveness, accuracy, completeness),
    and the related data quality rules (such as non-empty rule and range rule), which
    can be used to guide the electrical data quality inspection. Meanwhile, for the
    accuracy, we propose an outlier detection method based on time time-relevant k-means,
    which is used to detect the voltage, curve and power data issues in electricity
    data. The experimental and simulation results show that the proposed architecture
    and method can work well for the comprehensive data quality assessment of electrical
    data.
  Author: Liu, He and Wang, Xiaohui and Lei, Shuya and Zhang, Xi and Liu, Weiwei and
    Qin, Ming
  Book Title/Journal: Proceedings of the International Conference on Artificial Intelligence,
    Information Processing and Cloud Computing
  DOI: 10.1145/3371425.3371435
  JCS_FACTOR: 0.0
  Keywords: data quality, electrical data, outlier, quality assessment
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE,
    INFORMATION PROCESSING AND CLOUD COMPUTING
  Title: A Rule Based Data Quality Assessment Architecture and Application for Electrical
    Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Under the background of the rapid development of big data technology,
    the construction of college informationization teaching service system is the
    basis of the informationization teaching in colleges and universities. That is
    very important for the success of the informatization in Universities what is
    service realization model, business logic, architecture and platform conform to
    the whole development strategy of universities. Information management organization
    supports the planning, implementation, operation, maintenance and management of
    business information system. This paper analyzes the reform mode of college education
    information service system supported by big data technology. Based on the analysis
    of the reform mode of college informationization teaching service system supported
    by big data technology, this paper puts forward the design idea of post system
    based on big data. At the same time, with the case of "big data assisted employment",
    the post design and adjustment were carried out. The results show that big data
    assisted employment has greatly improved the efficiency and quality of the school's
    employment department, providing students with better employment security. Finally,
    the problems that need to be solved in the informatization teaching service are
    sorted out.
  Author: Pengxi, Li
  Book Title/Journal: Proceedings of the 2019 3rd High Performance Computing and Cluster
    Technologies Conference
  DOI: 10.1145/3341069.3341086
  JCS_FACTOR: 0.0
  Keywords: Teaching informatization, Service system, Big data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 3RD HIGH PERFORMANCE COMPUTING AND CLUSTER
    TECHNOLOGIES CONFERENCE
  Title: The Construction Study of College Informationization Teaching Service System
    under the Background of Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In recent years, the outbreak of foodborne diseases has been on an upward
    trend clearly. It is of great significance for us to predict the outbreak of foodborne
    diseases accurately and conduct quantitative risk assessment timely. Traditional
    prediction methods based on a single data source have drawbacks such as complex
    prediction processes and inaccurate prediction results. In this article, we figure
    out the scientific issues of how to improve the temporal and spatial accuracy
    of foodborne disease outbreak risk prediction in Beijing. Firstly, we analyze
    the different foodborne disease risk factors caused by the spread of water pollution
    in Beijing, and study the methods of collecting and preprocessing multi-source
    data. Then, through the comparison of different regression models and parameters
    tuning, we propose the use of Gradient Boosting Regression (GBR) model as a multi-source
    data fusion model to predict the outbreak of foodborne disease. Finally, we use
    the risk map to detect and predict foodborne disease outbreak in different business
    districts of Beijing based on visualization techniques, aiming to provide prevention
    and control assessment for decision-makers quickly and precisely.
  Author: Zhang, Mingke and Guo, Danhuai and Hu, Jinyong and Jin, Wei
  Book Title/Journal: Proceedings of the 5th ACM SIGSPATIAL International Workshop
    on the Use of GIS in Emergency Management
  DOI: 10.1145/3356998.3365776
  JCS_FACTOR: 0.0
  Keywords: foodborne disease, risk assessment, big data, machine learning
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 5TH ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON THE
    USE OF GIS IN EMERGENCY MANAGEMENT
  Title: Risk Prediction and Assessment of Foodborne Disease Based on Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Fuzzy join is an important primitive for data cleaning. The ability to
    customize fuzzy join is crucial to allow applications to address domain-specific
    data quality issues such as synonyms and abbreviations. While efficient indexing
    techniques exist for single-node implementations of customizable fuzzy join, the
    state-of-the-art scale-out techniques do not support customization, and exhibit
    poor performance and scalability characteristics. We describe the design of a
    scale-out fuzzy join operator that supports customization. We use a locality-sensitive-hashing
    (LSH) based signature scheme, and introduce optimizations that result in significant
    speed up with negligible impact on recall. We evaluate our implementation on the
    Azure Databricks version of Spark using several real-world and synthetic data
    sets. We observe speedups exceeding 50X compared to the best-known prior scale-out
    technique, and close to linear scalability with data size and number of nodes.
  Author: Chen, Zhimin and Wang, Yue and Narasayya, Vivek and Chaudhuri, Surajit
  Book Title/Journal: Proc. VLDB Endow.
  DOI: 10.14778/3352063.3352128
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROC. VLDB ENDOW.
  Title: Customizable and Scalable Fuzzy Join for Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Firmani, Donatella and Tanca, Letizia and Torlone, Riccardo
  Book Title/Journal: J. Data and Information Quality
  DOI: 10.1145/3362121
  JCS_FACTOR: 0.0
  Keywords: knowledge extraction, source selection, Data integration
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. DATA AND INFORMATION QUALITY
  Title: Ethical Dimensions for Data Quality
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Mappings of first name to gender have been widely recognized as a critical\
    \ tool for the completion, study, and validation of data records in a range of\
    \ areas. In this study, we investigate how organizations with large databases\
    \ of existing entities can create their own mappings between first names and gender\
    \ and how these mappings can be improved and utilized. Therefore, we first explore\
    \ a dataset with demographic information on more than 4 million people, which\
    \ was provided by a car insurance company. Then, we study how naming conventions\
    \ have changed over time and how they differ by nationality. Next, we build a\
    \ probabilistic first-name-to-gender mapping and augment the mapping by adding\
    \ nationality and decade of birth to improve the mapping's performance. We test\
    \ our mapping in two-label and three-label settings and further validate our mapping\
    \ by categorizing patent filings by gender of the inventor. We compare the results\
    \ with previous studies\xE2\u20AC\u2122 outcomes and find that our mapping produces\
    \ high-precision results. We validate that the additional information of nationality\
    \ and year of birth improve the precision scores of name-to-gender mappings. Therefore,\
    \ the proposed approach constitutes an efficient process for improving the data\
    \ quality of organizations\xE2\u20AC\u2122 records, if the gender attribute is\
    \ missing or unreliable."
  Author: M\"{u}ller, Daniel and Jain, Pratiksha and Te, Yieh-Funk
  Book Title/Journal: J. Data and Information Quality
  DOI: 10.1145/3297720
  JCS_FACTOR: 0.0
  Keywords: gender name mapping, Data quality improvement, record completion, patenting
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. DATA AND INFORMATION QUALITY
  Title: Augmenting Data Quality through High-Precision Gender Categorization
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: In recent years, improvements in high-speed Analog-to-Digital Converters
    (ADC) and sensor technology has encouraged researchers to improve the performance
    of Data Acquisition (DAQ) systems for scientific experiments which require high
    speed and continuous data measurements --- in particular, measuring the electronic
    and magnetic properties of materials using pump-probe experiments at high repetition
    rates. Experiments at TELBE are capable of acquiring almost 100 Gigabytes of raw
    data every ten minutes. The DAQ system used at TELBE partitions the raw data into
    various subdirectories for further parallel processing utilizing the multicore
    structure of modern CPUs.Furthermore, several other types of processors that accelerate
    data processing like the GPU and FPGA have emerged to solve the challenges of
    processing the massive amount of raw data. However, the memory and network bottlenecks
    become a significant challenge in big data processing, and new scalable programming
    techniques are needed to solve these challenges. In this contribution, we will
    outline the design and implementation of our practical software approach for efficient
    parallel processing of our large data sets at the TELBE user facility.
  Author: Bawatna, Mohammed and Green, Bertram and Kovalev, Sergey and Deinert, Jan-Christoph
    and Knodel, Oliver and Spallek, Rainer G.
  Book Title/Journal: Proceedings of the 2019 Summer Simulation Conference
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: big data, data analytics, data processing pipeline, data acquisition systems,
    signal processing
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 SUMMER SIMULATION CONFERENCE
  Title: Research and Implementation of Efficient Parallel Processing of Big Data
    at TELBE User Facility
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Data Quality Rule Definition and Discovery
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: Under the background of "Wisdom Drug Rehabilitation", we introduced "Artificial
    Intelligence and Big Data" into "exercise rehabilitation" work of drug addicts
    in judicial administrative system. It is a practical innovation of drug treatment
    in China. This article will elaborate this innovation of the construction and
    application of "Exercise Rehabilitation" intelligence platform system. This system
    will improve mental status, alleviate physical and psychological symptoms, ensure
    safety in places, lighten the burden of professional police officers, make rapid
    analysis, make accurate decisions and improve the integrity rate of the addict.
  Author: Jia, Dong-Ming and Yuan, Cun-Feng and Guo, Song and Jiang, Zu-Zhen and Xu,
    Ding and Wang, Da-An
  Book Title/Journal: Proceedings of the 2019 International Conference on Artificial
    Intelligence and Advanced Manufacturing
  DOI: 10.1145/3358331.3358336
  JCS_FACTOR: 0.0
  Keywords: artificial intelligence, big data, rehabilitation training, Sports rehabilitation,
    judicial administrative
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
    AND ADVANCED MANUFACTURING
  Title: Application of "Artificial Intelligence and Big Data" in Sports Rehabilitation
    for Chinese Judicial Administrative Drug Addicts
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Education is an important element towards learning and human development,
    thus, it is the key towards identifying competencies and better productivity for
    the workforce. As part of the Commission on Higher Education's (CHED) thrust for
    improving efficiency and effectiveness by simplifying the collection process for
    all the stakeholders, the developed system will drastically improve the availability
    of data for making informed decisions and efficient generation of reports.This
    research outlines opportunities and challenges associated with the implementation
    and governance of Big Data in higher education through development and implementation
    of data analytics tool.
  Author: Cabanban-Casem, Christianne Lynnette
  Book Title/Journal: Proceedings of the 2019 Asia Pacific Information Technology
    Conference
  DOI: 10.1145/3314527.3314537
  JCS_FACTOR: 0.0
  Keywords: Data Science, Knowledge Management, Higher Education Data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 ASIA PACIFIC INFORMATION TECHNOLOGY CONFERENCE
  Title: Analytical Visualization of Higher Education Institutions' Big Data for Decision
    Making
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Today's herd management undergoes a major transformation triggered by
    the penetration of cheap sensor solutions into cattle farms, and the promise of
    predictive analytics to detect animal health issues and product-related problems
    before they occur. The latter is particularly important to prevent disease spread,
    ensure animal health, animal welfare and product quality. Sensor businesses entering
    the market tend to build their solutions as end-to-end pipelines spanning sensors,
    proprietary algorithms, cloud services, and mobile apps. Since data privacy is
    an important issue in this industry, as a result, disconnected data silos, heterogeneity
    of APIs, and lack of common standards limit the value the sensor technologies
    could provide for herd management. In the last few years, researchers and communities
    proposed a number of data integration architectures to enable exchange between
    streams of sensor data. This paper surveys the existing efforts and outlines the
    opportunities they fail to address by treating sensor data as a black box. We
    discuss alternative solutions to the problem based on privacy-preserving collaborative
    learning, and provide a set of scenarios to show their benefits for both farmers
    and businesses.
  Author: Papst, Franz and Saukh, Olga and R\"{o}mer, Kay and Grandl, Florian and
    Jakovljevic, Igor and Steininger, Franz and Mayerhofer, Martin and Duda, J\"{u}rgen
    and Egger-Danner, Christa
  Book Title/Journal: Proceedings of the 9th International Conference on the Internet
    of Things
  DOI: 10.1145/3365871.3365900
  JCS_FACTOR: 0.0
  Keywords: privacy-preserving data analysis, agriculture, data privacy
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON THE INTERNET OF
    THINGS
  Title: Embracing Opportunities of Livestock Big Data Integration with Privacy Constraints
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: As part of the open government movement, an increasing number of 311 call
    centers have made their datasets available to the public. Studies have found that
    311 request patterns are associated with personal attributes and living conditions.
    Most of these studies used New York City 311 data. In this study, we use 311 data
    from the City of Miami, a smaller local government, as a case study. This study
    contributes to digital government research and practices by making suggestions
    on best practices regarding the use of big data analytics on 311 data. In addition,
    we discuss limitations of 311 data and analytics results. Finally, we expect our
    results to inform decision making within the City of Miami government and other
    local governments.
  Author: Hagen, Loni and Seon Yi, Hye and Pietri, Siana and E. Keller, Thomas
  Book Title/Journal: Proceedings of the 20th Annual International Conference on Digital
    Government Research
  DOI: 10.1145/3325112.3325212
  JCS_FACTOR: 0.0
  Keywords: information visualization, 311 data, e-government, big data analytics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL CONFERENCE ON DIGITAL
    GOVERNMENT RESEARCH
  Title: 'Processes, Potential Benefits, and Limitations of Big Data Analytics: A
    Case Analysis of 311 Data from City of Miami'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "Mobile crowdsensing has found a variety of applications e.g., spectrum\
    \ sensing, environmental monitoring by leveraging the \xE2\u20AC\u0153wisdom\xE2\
    \u20AC\x9D of a potentially large crowd of mobile users. An important metric of\
    \ a crowdsensing task is data accuracy, which relies on the data quality of the\
    \ participating users\xE2\u20AC\u2122 data e.g., users\xE2\u20AC\u2122 received\
    \ SNRs for measuring a transmitter\xE2\u20AC\u2122s transmit signal strength.\
    \ However, the quality of a user can be its private information which, e.g., may\
    \ depend on the user\xE2\u20AC\u2122s location that it can manipulate to its own\
    \ advantage, which can mislead the crowdsensing requester about the knowledge\
    \ of the data\xE2\u20AC\u2122s accuracy. This issue is exacerbated by the fact\
    \ that the user can also manipulate its effort made in the crowdsensing task,\
    \ which is a hidden action that could result in the requester having incorrect\
    \ knowledge of the data\xE2\u20AC\u2122s accuracy. In this paper, we devise truthful\
    \ crowdsensing mechanisms for Quality and Effort Elicitation QEE, which incentivize\
    \ strategic users to truthfully reveal their private quality and truthfully make\
    \ efforts as desired by the requester. The QEE mechanisms achieve the truthful\
    \ design by overcoming the intricate dependency of a user\xE2\u20AC\u2122s data\
    \ on its private quality and hidden effort. Under the QEE mechanisms, we show\
    \ that the crowdsensing requester\xE2\u20AC\u2122s optimal RO effort assignment\
    \ assigns effort only to the best user that has the smallest \xE2\u20AC\u0153\
    virtual valuation\xE2\u20AC\x9D, which depends on the user\xE2\u20AC\u2122s quality\
    \ and the quality\xE2\u20AC\u2122s distribution. We also show that, as the number\
    \ of users increases, the performance gap between the RO effort assignment and\
    \ the socially optimal effort assignment decreases, and converges to 0 asymptotically.\
    \ We further discuss some extensions of the QEE mechanisms. Simulation results\
    \ demonstrate the truthfulness of the QEE mechanisms and the system efficiency\
    \ of the RO effort assignment."
  Author: Gong, Xiaowen and Shroff, Ness B.
  Book Title/Journal: IEEE/ACM Trans. Netw.
  DOI: 10.1109/TNET.2019.2934026
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: IEEE/ACM TRANS. NETW.
  Title: Truthful Mobile Crowdsensing for Strategic Users With Private Data Quality
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Social media provide continuous data streams that contain information
    with different level of sensitivity, validity and accuracy. Therefore, this type
    of information has to be properly filtered, extracted and processed to avoid noisy
    and inaccurate results. The main goal of this work is to propose architecture
    and workflow able to process Twitter social network data in near real-time. The
    primary design of the introduced modern architecture covers all processing aspects
    from data ingestion and storing to data processing and analysing. This paper presents
    Apache Spark and Hadoop implementation. The secondary objective is to analyse
    tweets with the defined topic --- floods. The word frequency method (Word Clouds)
    is shown as a major tool to analyse the content of the input dataset. The experimental
    architecture confirmed the usefulness of many well-known functions of Spark and
    Hadoop in the social data domain. The platforms which were used provided effective
    tools for optimal data ingesting, storing as well as processing and analysing.
    Based on the analytical part, it was observed that the word frequency method (n-grams)
    can effectively reveal the tweets content. According to the results of this study,
    the tweets proved their high informative potential regarding data quality and
    quantity.
  Author: Podhoranyi, Michal and Vojacek, Lukas
  Book Title/Journal: Proceedings of the 2019 4th International Conference on Cloud
    Computing and Internet of Things
  DOI: 10.1145/3361821.3361825
  JCS_FACTOR: 0.0
  Keywords: Apache Spark, social network data, data processing architecture, Twitter
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON CLOUD COMPUTING
    AND INTERNET OF THINGS
  Title: 'Social Media Data Processing Infrastructure by Using Apache Spark Big Data
    Platform: Twitter Data Analysis'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: We overview CiteSeerX, the pioneer digital library search engine, that
    has been serving academic communities for more than 20 years (first released in
    1998), from three perspectives. The system perspective summarizes its architecture
    evolution in three phases over the past 20 years. The data perspective describes
    how CiteSeerX has created searchable scholarly big datasets and made them freely
    available for multiple purposes. In order to be scalable and effective, AI technologies
    are employed in all essential modules. To effectively train these models, a sufficient
    amount of data has been labeled, which can then be reused for training future
    models. Finally, we discuss the future of CiteSeerX. Our ongoing work is to make
    CiteSeerX more sustainable. To this end, we are working to ingest all open access
    scholarly papers, estimated to be 30-40 million. Part of the plan is to discover
    dataset mentions and metadata in scholarly articles and make them more accessible
    via search interfaces. Users will have more opportunities to explore and trace
    datasets that can be reused and discover other datasets for new research projects.
    We summarize what was learned to make a similar system more sustainable and useful.
  Author: Wu, Jian and Kim, Kunho and Giles, C. Lee
  Book Title/Journal: Proceedings of the Conference on Artificial Intelligence for
    Data Discovery and Reuse
  DOI: 10.1145/3359115.3359119
  JCS_FACTOR: 0.0
  Keywords: CiteSeerX, search engines, scholarly big data, digital libraries
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE CONFERENCE ON ARTIFICIAL INTELLIGENCE FOR DATA DISCOVERY
    AND REUSE
  Title: 'CiteSeerX: 20 Years of Service to Scholarly Big Data'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Simulation of Supply Chains comprises huge amounts of data, resulting
    in numerous entities flowing in the model. These networks are highly dynamic systems,
    where entities' relationships and other elements evolve with time, paving the
    way for real-time Supply Chain decision-support tools capable of using real data.
    In light of this, a solution comprising of a Big Data Warehouse to store relevant
    data and a simulation model of an automotive plant, are being developed. The purpose
    of this paper is to address the modelling approach, which allowed the simulation
    model to automatically adapt to the data stored in a Big Data Warehouse and thus
    adapt to new scenarios without manual intervention. The main characteristics of
    the conceived solution were demonstrated, with emphasis to the real-time and the
    ability to allow the model to load the state of the system from the Big Data Warehouse.
  Author: Vieira, Ant\'{o}nio A. C. and Dias, Lu\'{\i}s M. S. and Santos, Maribel
    Y. and Pereira, Guilherme A. B. and Oliveira, Jos\'{e} A.
  Book Title/Journal: Proceedings of the Winter Simulation Conference
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE WINTER SIMULATION CONFERENCE
  Title: 'Real-Time Supply Chain Simulation: A Big Data-Driven Approach'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Big data analytics helps us to find potentially valuable knowledge, but
    as the size of the dataset increases, the computing cost also grows exponentially.
    In our previous work, BotCluster, we had designed a pre-processing filtering pipeline,
    including whitelist filter and flow loss-response rate (FLR) filter, for data
    reduction, which intended to wipe out irrelative noises and reduce the computing
    overhead. However, we still face a data redundancy phenomenon in which some of
    the same feature vectors repeatedly emerged. In this paper, we propose a data
    compacting approach aimed to reduce the input volume and keep enough representative
    feature vectors to fit DBSCAN's (Density-based spatial clustering of applications
    with noise) criteria. It purges the redundant vectors according to a purging threshold
    and keeps the primary representatives. Experimental results have shown that the
    average data reduction ratio is about 81.34%, while the precision has only slightly
    decreased by 1.6% on average, and the results still have 99.88% of IPs overlapped
    with the previous system.
  Author: Wang, Chun-Yu and Fuh, Shih-Hao and Lo, Ta-Chun and Cheng, Qi-Jun and Chen,
    Yu-Cheng and Cho, Feng-Min and Chang, Jyh-Biau and Shieh, Ce-Kuen
  Book Title/Journal: Proceedings of the 6th IEEE/ACM International Conference on
    Big Data Computing, Applications and Technologies
  DOI: 10.1145/3365109.3368778
  JCS_FACTOR: 0.0
  Keywords: botnet, data reduction, data compacting, big data, mapreduce framework,
    netflow, data compression, p2p botnet
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 6TH IEEE/ACM INTERNATIONAL CONFERENCE ON BIG DATA
    COMPUTING, APPLICATIONS AND TECHNOLOGIES
  Title: A Data Compacting Technique to Reduce the NetFlow Size in Botnet Detection
    with BotCluster
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The currency of data can ensure that data is not obsolete and outdated.
    As one of the important bases for evaluating data quality, it plays an important
    role in the availability of data. Data currency rules can effectively discriminate
    the temporal relationship between data sets. The decision tree can availably classify
    and predict the data, and can test the attribute values very well. In this paper,
    the currency rules are combined with the C4.5 algorithm in the decision tree,
    and the improved algorithm is applied to the college elective data in recent years.
    Through experiments, the algorithm used in this paper can extract the statute
    rules from the student elective database. According to the currency rules, the
    college teaching plan can be planned in advance and the curriculum resources can
    be allocated reasonably.
  Author: Liang, Yu and Duan, Xuliang and Ding, Yuanjun and Kou, Xifeng and Huang,
    Jingcheng
  Book Title/Journal: Proceedings of the 2019 4th International Conference on Big
    Data and Computing
  DOI: 10.1145/3335484.3335541
  JCS_FACTOR: 0.0
  Keywords: currency rules, decision tree, course selection information, data mining
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON BIG DATA AND
    COMPUTING
  Title: Data Mining of Students' Course Selection Based on Currency Rules and Decision
    Tree
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'The information from Higher Education Institutions (HEIs) is primarily
    relevant for decision maker and educators. This study tackles e-learners behaviour
    using machine learning, particularly association rules and classifiers. Learners
    are characterized by a set of behaviours and attitudes that determine their learning
    abilities and skills. Learning from data generated by online learners may have
    significant impacts, however, few studies cover this resource from machine learning
    perspectives. We examine different data mining techniques including Random Forests,
    Logistic Regressions and Bayesian Networks as classifiers used for predicting
    e-learners'' classes (High, Medium and Low). The novelty of this study is that
    it explores and compares classifiers performance on the behaviour of online learners
    on four variables: raise hands, visiting IT resources, view announcement and discussion
    impact on e-learners. The results of this study indicate an 80% accuracy level
    obtained by Bayesian Networks; in contrast, the Random Forests have only 63% accuracy
    level and Logistic Regressions for 58%.'
  Author: Al Fanah, Muna and Ansari, Muhammad Ayub
  Book Title/Journal: Proceedings of the 2019 International Conference on Big Data
    and Education
  DOI: 10.1145/3322134.3322145
  JCS_FACTOR: 0.0
  Keywords: Accuracy, Association Rules, Bayesian Networks, Radom Forests, Precision
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON BIG DATA AND EDUCATION
  Title: Understanding E-Learners' Behaviour Using Data Mining Techniques
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Sentiment Analysis (SA), commonly known as opinion mining, during last
    couple of years, it becomes the fastest growing research areas in computer science.
    Conventionally, it helps to automatically detect if a text express is a positive,
    negative or neutral opinion. It enables us to identify and extract subjective
    information in a piece of writing, and this leads to gain an overview of wider
    public opinions or attitudes toward topics, products or services. Many researches
    have been done in this area, but most of them have focused on English and other
    Indo-European languages. Insufficient studies have actually accosted Sentiment
    Analysis in morphologically rich language such as Arabic. Regardless, given the
    increasing number of Arabic users and the exponential growth of online content,
    SA in this language has gained the attention of many researches last years, since
    Arabic raises many challenges because of its derivational, inflectional and agglutinative
    morphology. The objective of this paper is to promote the performance of Arabic
    Sentiment Analysis (ASA) by using Deep learning techniques. For that we implement
    Multi-Layer perceptron model in order to process and classify a dataset (Tweets).
    In fact, the experimental results prove that MLP as a deep learning model has
    a better performance for ASA than classical approaches.
  Author: Chahidi, Hamza and Omara, Hicham and Lazaar, Mohamed and Al Achhab, Mohammed
  Book Title/Journal: Proceedings of the 4th International Conference on Big Data
    and Internet of Things
  DOI: 10.1145/3372938.3372950
  JCS_FACTOR: 0.0
  Keywords: Machine Learning, Sentiment Analysis, Multi-layer Perceptron, Arabic,
    Deep Learning
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON BIG DATA AND INTERNET
    OF THINGS
  Title: Impact of Neural Network Architectures on Arabic Sentiment Analysis
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In this paper, we propose a novel architecture that utilizes features
    of Blockchain, fog computing, and cloud computing to manage IoT data. Blockchain
    allows to have a distributed peer-to-peer network in which non-trusting participants
    can interact with each other without a trusted intermediary or third party. We
    evaluate how this mechanism works to face the challenges of IoT with respect to
    multiple accessibility to IoT devises. We consider a Blockchain architecture in
    presence of edge computing layer. With fog or fog computing, the sensitive data
    can be analyzed locally instead of sending it to the cloud for analysis. Edge
    nodes can also keep track and control of the IoT devices that collect, analyze
    and store data. We show that this control can be better executed when Software
    Defined Network (SDN) and Network Functions Virtualization (NFV) are integrated
    into our process for optimal resource management. In this paper, we present our
    system architecture with a detailed description of the different interactions.
    We remark that the integration of Blockchain, IoT, and edge computing when coupled
    with SDN and NFV-enabled cloud infrastructure can bring to more superior and efficient
    platform for accessing, managing, and processing the huge influx of IoT data.
  Author: El Kafhali, Said and Chahir, Chorouk and Hanini, Mohamed and Salah, Khaled
  Book Title/Journal: Proceedings of the 4th International Conference on Big Data
    and Internet of Things
  DOI: 10.1145/3372938.3372970
  JCS_FACTOR: 0.0
  Keywords: Internet of Things, SDN, Smart Contracts, Fog computing, Edge Computing,
    NFV, Blockchain
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON BIG DATA AND INTERNET
    OF THINGS
  Title: Architecture to Manage Internet of Things Data Using Blockchain and Fog Computing
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "High-quality data is critical for effective data science. As the use\
    \ of data science has grown, so too have concerns that individuals\xE2\u20AC\u2122\
    \ rights to privacy will be violated. This has led to the development of data\
    \ protection regulations around the globe and the use of sophisticated anonymization\
    \ techniques to protect privacy. Such measures make it more challenging for the\
    \ data scientist to understand the data, exacerbating issues of data quality.\
    \ Responsible data science aims to develop useful insights from the data while\
    \ fully embracing these considerations.We pose the high-level problem in this\
    \ article, \xE2\u20AC\u0153How can a data scientist develop the needed trust that\
    \ private data has high quality?\xE2\u20AC\x9D We then identify a series of challenges\
    \ for various data-centric communities and outline research questions for data\
    \ quality and privacy researchers, which would need to be addressed to effectively\
    \ answer the problem posed in this article."
  Author: Srivastava, Divesh and Scannapieco, Monica and Redman, Thomas C.
  Book Title/Journal: J. Data and Information Quality
  DOI: 10.1145/3287168
  JCS_FACTOR: 0.0
  Keywords: private data, data trust, quality of private data, Responsible data science
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. DATA AND INFORMATION QUALITY
  Title: 'Ensuring High-Quality Private Data for Responsible Data Science: Vision
    and Challenges'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Understanding users' behavior of internet usage is essential for the quality
    of service (QoS) analysis on the internet. If the internet providers can better
    understand their users, they may be able to provide better service, and also enhance
    the quality of the service. In general, the information about users' behavior
    is stored as the internet access log files, called event logs, on the server.
    To have the patterns of users' behavior from the event logs, this work aims to
    extract an interesting pattern of inappropriate user behaviors through the method
    of internet usage patterns mining. The primary mechanism of the proposed method
    is the Generalized Sequential Pattern (GSP) algorithm, which is an algorithm of
    sequential pattern mining. This study uses real event logs from an organization
    in Thailand. The results have identified exciting findings that have made possible
    to propose some improvements and increasing the QoS of the internet service.
  Author: Polpinij, Jantima and Namee, Khanista
  Book Title/Journal: Proceedings of the 2019 International Conference on Big Data
    and Education
  DOI: 10.1145/3322134.3322155
  JCS_FACTOR: 0.0
  Keywords: Inappropriate user pattern, Internet usage, Data mining, Generalized Sequential
    Pattern, Sequential pattern mining, Event logs
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON BIG DATA AND EDUCATION
  Title: Internet Usage Patterns Mining from Firewall Event Logs
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Is there a stable cognitive structure of scientific data retrieval process?
    Based on the theory and method of user relevance research, this study explores
    the cognitive characteristics of user scientific data query and retrieval. The
    semi-structured interview method used to collect relevant data, and the content
    analysis method used to encode and analyze the cognitive process of users' scientific
    data query and retrieval. The results show that (1) users scientific data relevance
    judgment not only depend on topicality, but also use accessibility, quality, authority
    and usefulness. (2) There are 7 combination patterns for the use of user's scientific
    data relevance criteria, and (3) different patterns correspond to different user
    relevance types and different user information need states. These 7 criteria usage
    patterns reveal the cognitive enhancement of user scientific data relevance judgment.
    The research results have a great inspiration for the development of interactive
    scientific data retrieval system based on user cognitive enhancement characteristics.
  Author: Liu, Jianping and Wang, Jian and Zhou, Guomin and Zhang, Guilan and Cui,
    Yunpeng and Liu, Juan
  Book Title/Journal: Proceedings of the 3rd International Conference on Computer
    Science and Application Engineering
  DOI: 10.1145/3331453.3360954
  JCS_FACTOR: 0.0
  Keywords: User relevance, Scientific data retrieval, Relevance criteria
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE
    AND APPLICATION ENGINEERING
  Title: The Cognitive Enhancement Process of Scientific Data Retrieval
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'Personalized search aims to adapt document ranking to user''s personal
    interests. Traditionally, this is done by extracting click and topical features
    from historical data in order to construct a user profile. In recent years, deep
    learning has been successfully used in personalized search due to its ability
    of automatic feature learning. However, the small amount of noisy personal data
    poses challenges to deep learning models to learn the personalized classification
    boundary between relevant and irrelevant results. In this paper, we propose PSGAN,
    a Generative Adversarial Network (GAN) framework for personalized search. By means
    of adversarial training, we enforce the model to pay more attention to training
    data that are difficult to distinguish. We use the discriminator to evaluate personalized
    relevance of documents and use the generator to learn the distribution of relevant
    documents. Two alternative ways to construct the generator in the framework are
    tested: based on the current query or based on a set of generated queries. Experiments
    on data from a commercial search engine show that our models can yield significant
    improvements over state-of-the-art models.'
  Author: Lu, Shuqi and Dou, Zhicheng and Jun, Xu and Nie, Jian-Yun and Wen, Ji-Rong
  Book Title/Journal: Proceedings of the 42nd International ACM SIGIR Conference on
    Research and Development in Information Retrieval
  DOI: 10.1145/3331184.3331218
  JCS_FACTOR: 0.0
  Keywords: generative adversarial network, personalized web search
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH
    AND DEVELOPMENT IN INFORMATION RETRIEVAL
  Title: 'PSGAN: A Minimax Game for Personalized Search with Limited and Noisy Click
    Data'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The Web contains millions of relational HTML tables, which cover a multitude
    of different, often very specific topics. This rich pool of data has motivated
    a growing body of research on methods that use web table data to extend local
    tables with additional attributes or add missing facts to knowledge bases. Nearly
    all existing approaches for these tasks build upon the assumption that web table
    data consists of binary relations, meaning that an attribute value depends on
    a single key attribute, and that the key attribute value is contained in the HTML
    table. Inspecting randomly chosen tables on the Web, however, quickly reveals
    that both assumptions are wrong for a large fraction of the tables. In order to
    better understand the potential of non-binary web table data for downstream applications,
    this papers analyses a corpus of 5 million web tables originating from 80 thousand
    different web sites with respect to how many web table attributes are non-binary,
    what composite keys are required to correctly interpret the semantics of the non-binary
    attributes, and whether the values of these keys are found in the table itself
    or need to be extracted from the page surrounding the table. The profiling of
    the corpus shows that at least 38% of the relations are non-binary. Recognizing
    these relations requires information from the title or the URL of the web page
    in 50% of the cases. We find that different websites use keys of varying length
    for the same dependent attribute, e.g. one cluster of websites presents employment
    numbers depending on time, another cluster presents them depending on time and
    profession. By identifying these clusters, we lay the foundation for selecting
    Web data sources according to the specificity of the keys that are used to determine
    specific attributes.
  Author: Lehmberg, Oliver and Bizer, Christian
  Book Title/Journal: Proceedings of the International Workshop on Semantic Big Data
  DOI: 10.1145/3323878.3325806
  JCS_FACTOR: 0.0
  Keywords: key detection, data profiling, web tables, n-ary relations
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE INTERNATIONAL WORKSHOP ON SEMANTIC BIG DATA
  Title: Profiling the Semantics of N-Ary Web Table Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "Deep learning has been widely used for extracting values from big data.\
    \ As many other machine learning algorithms, deep learning requires significant\
    \ training data. Experiments have shown both the volume and the quality of training\
    \ data can significantly impact the effectiveness of the value extraction. In\
    \ some cases, the volume of training data is not sufficiently large for effectively\
    \ training a deep learning model. In other cases, the quality of training data\
    \ is not high enough to achieve the optimal performance. Many approaches have\
    \ been proposed for augmenting training data to mitigate the deficiency. However,\
    \ whether the augmented data are \xE2\u20AC\u0153fit for purpose\xE2\u20AC\x9D\
    \ of deep learning is still a question. A framework for comprehensively evaluating\
    \ the effectiveness of the augmented data for deep learning is still not available.\
    \ In this article, we first discuss a data augmentation approach for deep learning.\
    \ The approach includes two components: the first one is to remove noisy data\
    \ in a dataset using a machine learning based classification to improve its quality,\
    \ and the second one is to increase the volume of the dataset for effectively\
    \ training a deep learning model. To evaluate the quality of the augmented data\
    \ in fidelity, variety, and veracity, a data quality evaluation framework is proposed.\
    \ We demonstrated the effectiveness of the data augmentation approach and the\
    \ data quality evaluation framework through studying an automated classification\
    \ of biology cell images using deep learning. The experimental results clearly\
    \ demonstrated the impact of the volume and quality of training data to the performance\
    \ of deep learning and the importance of the data quality evaluation. The data\
    \ augmentation approach and the data quality evaluation framework can be straightforwardly\
    \ adapted for deep learning study in other domains."
  Author: Ding, Junhua and Li, Xinchuan and Kang, Xiaojun and Gudivada, Venkat N.
  Book Title/Journal: J. Data and Information Quality
  DOI: 10.1145/3317573
  JCS_FACTOR: 0.0
  Keywords: support vector machine, machine learning, convolutional neural network,
    deep learning, Data quality, diffraction image
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. DATA AND INFORMATION QUALITY
  Title: A Case Study of the Augmentation and Evaluation of Training Data for Deep
    Learning
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: With the development of power enterprise informationization after more
    than ten years of development, Achieve full coverage of the company's business,
    effectively supporting the full-business operation of the power grid, and the
    accumulated business data has exploded. However, there are still problems such
    as low data quality, insufficient integration of multi-source business and data
    fusion, which makes it difficult to monitor and analyze the full-business of the
    power grid. This paper will combine the big data technology to study how to conduct
    monitoring and analysis of power grid full-business operation based on multi-source
    business and data fusion, and realize the three-layer architecture of business
    and data combination layer, business and data integration layer and business and
    data aggregation layer. Different levels of analysis and application, such as
    indicator monitoring analysis, subject monitoring analysis, and special monitoring
    analysis, effectively support enterprise management analysis and analytical decision.
  Author: Min, Han Xue and Feng, Zheng Gao and Xi, Liu Peng and Dong, Wang Xu and
    Ping, Xu Zhong
  Book Title/Journal: Proceedings of the 2019 2nd International Conference on Information
    Management and Management Sciences
  DOI: 10.1145/3357292.3357306
  JCS_FACTOR: 0.0
  Keywords: Big data technology, business and data fusion, mining analysis
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 2ND INTERNATIONAL CONFERENCE ON INFORMATION
    MANAGEMENT AND MANAGEMENT SCIENCES
  Title: Application Research of Power Grid Full-Business Monitoring and Analysis
    Based on Multi-Source Business and Data Fusion
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In the digital Economy 'Data is the new oil. In the last decade technology
    has disrupted every filed imaginable. One such booming technology is Blockchain.
    A blockchain is essentially a distributed database of records or public ledger
    of all transactions or digital events that have been executed and shared among
    participating parties. And once entered, the information is immutable. Ongoing
    projects and prior work in the fields of big data, data mining and data science
    has revealed how relevant data can be used to enhance products and services. There
    are uncountable applications and advantages of relevant data. The most valuable
    companies of today treat data as a commodity, which they trade and earn revenues.But
    use of relevant data has also drawn attention by the other non-conventional organizations
    and domains. To facilitate such trading, data marketplaces have emerged. In this
    paper we present a global data marketplace for users to easily buy and share data.
    The main focus of this research is to have a central data sharing platform for
    the recycling industry. This paper is a part of the research project "Recycling
    4.0" which is focusing on sustainably improving the recycling process through
    exchange of information. We identify providing secure platform, data integrity
    and data quality as some major challenges for a data marketplace. In this paper
    we also explore how global data marketplace could be implemented using blockchain
    and similar technologies.
  Author: Lawrenz, Sebastian and Sharma, Priyanka and Rausch, Andreas
  Book Title/Journal: Proceedings of the 2019 International Conference on Blockchain
    Technology
  DOI: 10.1145/3320154.3320165
  JCS_FACTOR: 0.0
  Keywords: Data marketplaces, Smart Contracts, Blockchain, Security, Data quality
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON BLOCKCHAIN TECHNOLOGY
  Title: Blockchain Technology as an Approach for Data Marketplaces
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: As big data has become increasingly necessary in modern farming techniques,
    the dependence on high quality and quantity of ground truthing data has risen.
    Collecting ground truthing data is one of the most labor-intensive aspects of
    the research process. A crowdsourcing platform application to aid laypeople in
    completing ground truthing data can improve the quality and quantity of data for
    growers and agricultural researchers. Focus groups were conducted to gauge opinions
    on crowdsourcing initiatives in agriculture to inform the design of the platform.
    Preliminary results demonstrate that the greatest motivation for the participants
    was having opportunities to develop their skills and access to educational resources.
    They also discussed having a finite timeframe for collecting the data, feeling
    appreciated by the researchers, and being informed on the context and next steps
    of the research. The results of these focus groups will be used to develop design
    prototypes for the crowdsourcing platform.
  Author: Posadas, Brianna B. and Hanumappa, Mamatha and Gilbert, Juan E.
  Book Title/Journal: Proceedings of the IX Latin American Conference on Human Computer
    Interaction
  DOI: 10.1145/3358961.3358970
  JCS_FACTOR: 0.0
  Keywords: urban agriculture, precision agriculture, big data, focus groups
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE IX LATIN AMERICAN CONFERENCE ON HUMAN COMPUTER INTERACTION
  Title: Opinions Concerning Crowdsourcing Applications in Agriculture in D.C.
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Job markets are experiencing an exponential growth in data alongside the
    recent explosion of big data in various domains including health, security and
    finance. Staying current with job market trends entails collecting, processing
    and analyzing huge amounts of data. A typical challenge with analyzing job listings
    is that they vary drastically with regards to verbiage, for instance a given job
    title or skill can be referred to using different words or industry jargons. As
    a result, it becomes incumbent to go beyond words present in job listings and
    carry out analysis aimed at discovering latent structures and trends in job listings.
    In this paper, we present a systematic approach of uncovering latent trends in
    job markets using big data technologies (Apache Spark and Scala) and distributed
    semantic techniques such as latent semantic analysis (LSA). We show how LSA can
    uncover patterns/relationships/trends that will otherwise remain hidden if using
    traditional text mining techniques that rely only on word frequencies in documents.
  Author: Mbah, Raymond Blanch K. and Rege, Manjeet and Misra, Bhabani
  Book Title/Journal: Proceedings of the 2019 3rd International Conference on Compute
    and Data Analysis
  DOI: 10.1145/3314545.3314566
  JCS_FACTOR: 0.0
  Keywords: Natural Language Processing(NLP), Latent Semantic Analysis(LSA), Spark,
    Singular Value Decomposition (SVD), Scala, Big Data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 3RD INTERNATIONAL CONFERENCE ON COMPUTE AND
    DATA ANALYSIS
  Title: Using Spark and Scala for Discovering Latent Trends in Job Markets
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'The massive adoption of high-throughput genomics, deep sequencing technologies
    and big data technologies have made possible the era of precision medicine. However,
    the volume of data and its complexity remain important challenges for precision
    medicine research, hindering development in this field. The literature on precision
    medicine research describes a few platforms to support specific types of studies,
    but none of these offer researchers the level of customization required to meet
    their specific needs [1]. Methods: We propose to design and develop a platform
    able to import and integrate a very large volume of genetics, clinical, demographical
    and environmental data in a cloud computing infrastructure. In our previous publication,
    we presented an approach that can customize existing data models to fit any precision
    medicine research data requirement [1] and the requirement for future large-scale
    precision medicine platforms, in terms of data extensibility and the scalability
    of processing on demand. We also proposed a framework to meet the specific requirement
    of any precision medicine research [2]. In this paper, we describe how this new
    framework was implemented and trialed by the precision medicine researchers at
    the Centre Hospitalier Universitaire de l''Universit\''{e} de Montr\''{e}al (CHUM).
    Results: The data analysis simulations showed that the random forest algorithm
    presents better accuracy results. We obtained an F1-Score of 72% for random forest,
    69% using linear regression and 62% using the neural network algorithm. Conclusion:
    The results suggest that the proposed precision medicine data analysis platform
    allows researchers to configure, prepare the analysis environment and customize
    the platform data model to their specific research in very optimal delays, at
    very low cost and with minimal technical skills.'
  Author: Belghait, Fodil and April, Alain and Hamet, Pavel and Tremblay, Johanne
    and Desrosiers, Christian
  Book Title/Journal: Proceedings of the 9th International Conference on Digital Public
    Health
  DOI: 10.1145/3357729.3357742
  JCS_FACTOR: 0.0
  Keywords: bioinformatics, clinical databases, precision medicine, genomics, big
    data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON DIGITAL PUBLIC HEALTH
  Title: A Large-Scale and Extensible Platform for Precision Medicine Research
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Performance is a key factor for big data applications, and much research
    has been devoted to optimizing these applications. While prior work can diagnose
    and correct data skew, the problem of computation skew---abnormally high computation
    costs for a small subset of input data---has been largely overlooked. Computation
    skew commonly occurs in real-world applications and yet no tool is available for
    developers to pinpoint underlying causes.To enable a user to debug applications
    that exhibit computation skew, we develop a post-mortem performance debugging
    tool. PerfDebug automatically finds input records responsible for such abnormalities
    in a big data application by reasoning about deviations in performance metrics
    such as job execution time, garbage collection time, and serialization time. The
    key to PerfDebug's success is a data provenance-based technique that computes
    and propagates record-level computation latency to keep track of abnormally expensive
    records throughout the pipeline. Finally, the input records that have the largest
    latency contributions are presented to the user for bug fixing. We evaluate PerfDebug
    via in-depth case studies and observe that remediation such as removing the single
    most expensive record or simple code rewrite can achieve up to 16X performance
    improvement.
  Author: Teoh, Jason and Gulzar, Muhammad Ali and Xu, Guoqing Harry and Kim, Miryung
  Book Title/Journal: Proceedings of the ACM Symposium on Cloud Computing
  DOI: 10.1145/3357223.3362727
  JCS_FACTOR: 0.0
  Keywords: Performance debugging, data provenance, data intensive scalable computing,
    fault localization, big data systems
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE ACM SYMPOSIUM ON CLOUD COMPUTING
  Title: 'PerfDebug: Performance Debugging of Computation Skew in Dataflow Systems'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "Providing a 360\xC2\xB0 view of a given data item especially for sensitive\
    \ data is essential toward not only protecting the data and associated privacy\
    \ but also assuring trust, compliance, and ethics of the systems that use or manage\
    \ such data. With the advent of General Data Protection Regulation, California\
    \ Data Privacy Law, and other such regulatory requirements, it is essential to\
    \ support data transparency in all such dimensions. Moreover, data transparency\
    \ should not violate privacy and security requirements. In this article, we put\
    \ forward a vision for how data transparency would be achieved in a de-centralized\
    \ fashion using blockchain technology."
  Author: Bertino, Elisa and Kundu, Ahish and Sura, Zehra
  Book Title/Journal: J. Data and Information Quality
  DOI: 10.1145/3312750
  JCS_FACTOR: 0.0
  Keywords: data provenance, Big data, accountability, privacy
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. DATA AND INFORMATION QUALITY
  Title: Data Transparency with Blockchain and AI Ethics
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Proliferation of sensors embedded in smartphones and smartwatches helps
    capture rich dataset for machine learning algorithms to extract meaningful digital
    bio-markers on consumer devices for monitoring disease progression and treatment
    response. However, development and validation of machine learning algorithms depend
    on gathering high fidelity sensor data and reliable ground-truth. We conduct a
    study, called mLungStudy, with 131 subjects with varying pulmonary conditions
    to collect mobile sensor data including audio, accelerometer, gyroscope using
    a smartphone and a smartwatch, in order to extract pulmonary biomarkers such as
    breathing, coughs, spirometry, and breathlessness. Our study shows that commonly
    used breathing ground-truth data from chestband may not always be reliable as
    a gold-standard. Our analysis shows that breathlessness biomarkers such as pause
    time and pause frequency from 2.15 minutes of audio can be as reliable as those
    extracted from 5 minutes' worth of speech data. This finding can be useful for
    future studies to trade-off between the reliability of breathlessness data and
    patient comfort in generating continuous speech data. Furthermore, we use crowdsourcing
    techniques to annotate pulmonary sound events for developing signal processing
    and machine learning algorithms. In this paper, we highlight several practical
    challenges to collect and annotate physiological data and acoustic symptoms from
    chronic pulmonary patients and ways to improve data quality. We show that the
    waveform visualization of the audio signal improves annotation quality which leads
    to a 6.59% increase in cough classification accuracy and a 6% increase in spirometry
    event classification accuracy. Findings from this study inform future studies
    focusing on developing explainable machine learning models to extract pulmonary
    digital bio-markers using mobile sensors.
  Author: Rahman, Md Mahbubur and Nathan, Viswam and Nemati, Ebrahim and Vatanparvar,
    Korosh and Ahmed, Mohsin and Kuang, Jilong
  Book Title/Journal: Proceedings of the 13th EAI International Conference on Pervasive
    Computing Technologies for Healthcare
  DOI: 10.1145/3329189.3329204
  JCS_FACTOR: 0.0
  Keywords: Data Quality, Cough, Digital Biomarkers, Breathing, Crowdsourced Annotation,
    mHealth, Breathlessness
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 13TH EAI INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING
    TECHNOLOGIES FOR HEALTHCARE
  Title: Towards Reliable Data Collection and Annotation to Extract Pulmonary Digital
    Biomarkers Using Mobile Sensors
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Scalability, standardization, and management are important issues when
    working with very large Volunteered Geographic Information (VGI). VGI is a rich
    and valuable source of Points of Interest (POI) information, but its inherent
    heterogeneity in content, structure, and scale across sources present major challenges
    for interlinking data sources for analysis. To be useful at scale, the raw information
    needs to be transformed into a standardized schema that can be easily and reliably
    used by data analysts. In this work, we tackle the problem of unifying POI categories
    (e.g. restaurants, temple, and hotel) across multiple data sources to aid in improving
    land use maps and population distribution estimation as well as support data analysts
    wishing to fuse multiple data sources with the OpenStreetMap (OSM) mapping platform
    or working with projects that are already configured in the OSM schema and wish
    to add additional sources of information. Graph theory and its implementation
    through the SONET graph database, provides a programmatic way to organize, store,
    and retrieve standardized POI categories at multiple levels of abstraction. Additionally,
    it addresses category heterogeneity across data sources by standardizing and managing
    categories in a way that makes cross-domain analysis possible.
  Author: Palumbo, Rachel and Thompson, Laura and Thakur, Gautam
  Book Title/Journal: Proceedings of the 3rd ACM SIGSPATIAL International Workshop
    on Geospatial Humanities
  DOI: 10.1145/3356991.3365474
  JCS_FACTOR: 0.0
  Keywords: points of interest, openstreetmap, big data, graph database, ontology
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 3RD ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON GEOSPATIAL
    HUMANITIES
  Title: 'SONET: A Semantic Ontological Network Graph for Managing Points of Interest
    Data Heterogeneity'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: A large number of published datasets (or sources) that follow Linked Data
    principles is currently available and this number grows rapidly. However, the
    major target of Linked Data, i.e., linking and integration, is not easy to achieve.
    In general, information integration is difficult, because (a) datasets are produced,
    kept, or managed by different organizations using different models, schemas, or
    formats, (b) the same real-world entities or relationships are referred with different
    URIs or names and in different natural languages,&lt;?brk?&gt;(c) datasets usually
    contain complementary information, (d) datasets can contain data that are erroneous,
    out-of-date, or conflicting, (e) datasets even about the same domain may follow
    different conceptualizations of the domain, (f) everything can change (e.g., schemas,
    data) as time passes. This article surveys the work that has been done in the
    area of Linked Data integration, it identifies the main actors and use cases,
    it analyzes and factorizes the integration process according to various dimensions,
    and it discusses the methods that are used in each step. Emphasis is given on
    methods that can be used for integrating several datasets. Based on this analysis,
    the article concludes with directions that are worth further research.
  Author: Mountantonakis, Michalis and Tzitzikas, Yannis
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3345551
  JCS_FACTOR: 0.0
  Keywords: big data, RDF, Data integration, semantic web, data discovery
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: 'Large-Scale Semantic Integration of Linked Data: A Survey'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: We present a smartphone application for at-home participation in large-scale
    neuroscientific studies. Our goal is to establish user-experience design as a
    paradigm in basic neuroscientific research to overcome the limits of current studies,
    especially in rare neurological disorders.The presented application guides users
    through the fitting procedure of the EEG headset and automatically encrypts and
    uploads recorded data to a remote server. User-feedback and neurophysiological
    data from a pilot study with eighteen subjects indicate that the application can
    be used outside of a laboratory, without the need for external guidance. We hope
    to inspire future work on the intersection between basic neuroscience and human-computer
    interaction as a promising paradigm to accelerate research on rare neurological
    diseases and assistive neurotechnology.
  Author: Hohmann, Matthias R. and Hackl, Michelle and Wirth, Brian and Zaman, Talha
    and Enficiaud, Raffi and Grosse-Wentrup, Moritz and Sch\"{o}lkopf, Bernhard
  Book Title/Journal: Extended Abstracts of the 2019 CHI Conference on Human Factors
    in Computing Systems
  DOI: 10.1145/3290607.3313002
  JCS_FACTOR: 0.0
  Keywords: medical studies, smartphone application, electrophysiology, neuroscience,
    big data, wearable sensors, user-centered design
  SCI_FACTOR: 0.0
  TITLE_UPPER: EXTENDED ABSTRACTS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING
    SYSTEMS
  Title: 'MYND: A Platform for Large-Scale Neuroscientific Studies'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'Logs are widely used by large and complex software-intensive systems
    for troubleshooting. There have been a lot of studies on log-based anomaly detection.
    To detect the anomalies, the existing methods mainly construct a detection model
    using log event data extracted from historical logs. However, we find that the
    existing methods do not work well in practice. These methods have the close-world
    assumption, which assumes that the log data is stable over time and the set of
    distinct log events is known. However, our empirical study shows that in practice,
    log data often contains previously unseen log events or log sequences. The instability
    of log data comes from two sources: 1) the evolution of logging statements, and
    2) the processing noise in log data. In this paper, we propose a new log-based
    anomaly detection approach, called LogRobust. LogRobust extracts semantic information
    of log events and represents them as semantic vectors. It then detects anomalies
    by utilizing an attention-based Bi-LSTM model, which has the ability to capture
    the contextual information in the log sequences and automatically learn the importance
    of different log events. In this way, LogRobust is able to identify and handle
    unstable log events and sequences. We have evaluated LogRobust using logs collected
    from the Hadoop system and an actual online service system of Microsoft. The experimental
    results show that the proposed approach can well address the problem of log instability
    and achieve accurate and robust results on real-world, ever-changing log data.'
  Author: Zhang, Xu and Xu, Yong and Lin, Qingwei and Qiao, Bo and Zhang, Hongyu and
    Dang, Yingnong and Xie, Chunyu and Yang, Xinsheng and Cheng, Qian and Li, Ze and
    Chen, Junjie and He, Xiaoting and Yao, Randolph and Lou, Jian-Guang and Chintalapati,
    Murali and Shen, Furao and Zhang, Dongmei
  Book Title/Journal: Proceedings of the 2019 27th ACM Joint Meeting on European Software
    Engineering Conference and Symposium on the Foundations of Software Engineering
  DOI: 10.1145/3338906.3338931
  JCS_FACTOR: 0.0
  Keywords: Deep Learning, Anomaly Detection, Log Analysis, Log Instability, Data
    Quality
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 27TH ACM JOINT MEETING ON EUROPEAN SOFTWARE
    ENGINEERING CONFERENCE AND SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING
  Title: Robust Log-Based Anomaly Detection on Unstable Log Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'Financial markets are event- and data-driven to an extremely high degree.
    For making decisions and triggering actions stakeholders require notifications
    about significant events and reliable background information that meet their individual
    requirements in terms of timeliness, accuracy, and completeness. As one of Europe''s
    leading providers of financial data and regulatory solutions vwd: processes an
    average of 18 billion event notifications from 500+ data sources for 30 million
    symbols per day. Our large-scale distributed event-based systems handle daily
    peak rates of 1+ million event notifications per second and additional load generated
    by singular pivotal events with global impact.In this poster we give practical
    insights into our IT systems. We outline the infrastructure we operate and the
    event-driven architecture we apply at vwd. In particular we showcase the (geo)distributed
    publish/subscribe broker network we operate across locations and countries to
    provide market data to our customers with varying quality of information (QoI)
    properties.'
  Author: Frischbier, Sebastian and Paic, Mario and Echler, Alexander and Roth, Christian
  Book Title/Journal: Proceedings of the 13th ACM International Conference on Distributed
    and Event-Based Systems
  DOI: 10.1145/3328905.3332513
  JCS_FACTOR: 0.0
  Keywords: stream-processing, financial data, Event-processing, infrastructure, publish/subscribe,
    big data, quality of information, broker network
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 13TH ACM INTERNATIONAL CONFERENCE ON DISTRIBUTED
    AND EVENT-BASED SYSTEMS
  Title: A Real-World Distributed Infrastructure for Processing Financial Data at
    Scale
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The article describes the issues of Open Government Data (OGD) and problems
    with the use of such data. Good quality and proper publishing of OGD enable (apart
    from the control function) their business use. This affects the economic benefits.
    The author has identified the main problems of data publication based on Central
    Repositories for Public Information (CRPI) in Poland, the USA, the UK and Germany.
    The article focuses on the maturity of data formats, automated processing with
    Application Programming Interface (API), using the concept of Linked Open Data
    (LOD). The aim of the article is to identify barriers to the implementation of
    OGD-based solutions and to indicate recommendations to overcome these barriers.
    The research shows that the methods of sharing OGD differ significantly between
    countries despite common guidelines. The main problem is the use of unstructured
    data, unsuitable for the use of LOD.
  Author: Wieczorkowski, Jundefineddrzej
  Book Title/Journal: Proceedings of the 2019 3rd International Conference on E-Commerce,
    E-Business and E-Government
  DOI: 10.1145/3340017.3340022
  JCS_FACTOR: 0.0
  Keywords: Open Data, Central Repository for Public Information, OGD, LOD, Linked
    Open Data, Open Government Data, CRPI, E-government, Big Data, Linked Data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 3RD INTERNATIONAL CONFERENCE ON E-COMMERCE,
    E-BUSINESS AND E-GOVERNMENT
  Title: Barriers to Using Open Government Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Urban computing is an emerging area of investigation in which researchers
    study cities using digital data. Location-Based Social Networks (LBSNs) generate
    one specific type of digital data that offers unprecedented geographic and temporal
    resolutions. We discuss fundamental concepts of urban computing leveraging LBSN
    data and present a survey of recent urban computing studies that make use of LBSN
    data. We also point out the opportunities and challenges that those studies open.
  Author: Silva, Thiago H. and Viana, Aline Carneiro and Benevenuto, Fabr\'{\i}cio
    and Villas, Leandro and Salles, Juliana and Loureiro, Antonio and Quercia, Daniele
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3301284
  JCS_FACTOR: 0.0
  Keywords: urban informatics, location-based social networks, big data, city dynamics,
    urban societies, urban sensing, Urban computing
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: 'Urban Computing Leveraging Location-Based Social Network Data: A Survey'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: In March 2017, PIs and co-PIs funded through the NSF BIGDATA program were
    brought together along with selected industry and government invitees to discuss
    current research, identify current challenges, discuss promising future directions,
    foster new collaborations, and share accomplishments, at BDPI-2017. Given that
    two recent NITRD [2] and NSF [1] meeting reports contained a set of recommendations,
    grand challenges, and high impact priorities for Big Data, the organizers of this
    meeting shifted the focus of the breakout sessions to discuss problems and available
    data sets that exist in five application domains - policy, health, education,
    economy &amp; finance, and environment &amp; energy. These domains were selected
    based on a survey of the PIs/co-PIs and should not be interpreted as being more
    important than others. Slides that were presented by the different breakout group
    leaders are available at https://www.bi.vt.edu/ nsf-big-data/. We hope this report
    will serve as a blueprint for promising big data research in five application
    domains.
  Author: Singh, Lisa and Deshpande, Amol and Zhou, Wenchao and Banerjee, Arindam
    and Bowers, Alex and Friedler, Sorelle and Jagadish, H.V. and Karypis, George
    and Obradovic, Zoran and Vullikanti, Anil and Zuo, Wangda
  Book Title/Journal: SIGMOD Rec.
  DOI: 10.1145/3316416.3316425
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: SIGMOD REC.
  Title: NSF BIGDATA PI Meeting - Domain-Specific Research Directions and Data Sets
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: From the perspective of full cost, this paper uses Coase's transaction
    cost theory to analyze the causes of crowdsourcing, and on this basis to analyze
    the applicability of crowdsourcing platform products. At the same time, based
    on the crowdsourcing platform--zbj.com, we use the big data technology to grasp
    and analyze the related data of the crowdsourcing platform's successful cases
    in the past five months, and use the relevant statistical analysis method to categorize
    and analyze the industry attributes of the top five orders of the success cases
    of the zbj.com, in order to verify the theory mentioned in the article.
  Author: Li, Ruixue and Peng, Can and Sun, Huiliang
  Book Title/Journal: Proceedings of the 2019 International Conference on Management
    Science and Industrial Engineering
  DOI: 10.1145/3335550.3335577
  JCS_FACTOR: 0.0
  Keywords: Selection Strategy Analysis, Crowdsourcing platform, Appropriate products,
    Full cost
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE
    AND INDUSTRIAL ENGINEERING
  Title: Product Selection Strategy Analysis of Crowdsourcing Platform from the Full
    Cost Perspective
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In conventional metamorphic testing, metamorphic relations (MRs) are identified
    as necessary properties of a computer program's intended functionality, whereby
    violations of MRs reveal faults in the program---under the assumption that the
    source and follow-up inputs (test cases used in metamorphic testing) are valid.
    In the present study, the authors argue that MRs can also be used to validate
    and assess the quality of the program's input data---under the assumption that
    the source or follow-up inputs can be inappropriately generated. Using this new
    perspective, a case study in the natural language processing domain is used to
    explore the different types of text messages that are difficult to interpret by
    (Chinese-English) machine translation. A total of 46,180 short user comments on
    Personal Tailor (a 2013 Chinese film), collected from Douban (a popular Chinese
    social media platform), has been used as the primary dataset of this study, and
    the analysis of results demonstrates that the proposed MR-based data validation
    method is useful for the automatic identification of poorly translated text messages.
  Author: Yan, Boyang and Yecies, Brian and Zhou, Zhi Quan
  Book Title/Journal: Proceedings of the 4th International Workshop on Metamorphic
    Testing
  DOI: 10.1109/MET.2019.00018
  JCS_FACTOR: 0.0
  Keywords: natural language processing, machine translation, data validation, social
    media, Douban, data quality assessment, sentiment analysis, metamorphic relation,
    Oracle problem, metamorphic testing
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL WORKSHOP ON METAMORPHIC TESTING
  Title: 'Metamorphic Relations for Data Validation: A Case Study of Translated Text
    Messages'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Technical and organizational innovations such as Open Data, Internet of
    Things and Big Data have fueled renewed interest in policy analytics in the public
    sector. This revamped version of policy analysis continues the long-standing tradition
    of applying statistical modeling to better understand policy effects and decision
    making, but also incorporates other computational approaches such as artificial
    intelligence (AI) and computer simulation. Although much attention has been given
    to the development of capabilities for data analysis, there is much less attention
    to understanding the role of data management in a context of AI in government.
    In this paper, we argue that data management capabilities are foundational to
    data analysis of any kind, but even more important in the present AI context.
    This is so because without proper data management, simply acquiring data or systems
    will not produce desired outcomes. We also argue that realizing the potential
    of AI for social good relies on investments specifically focused on this social
    outcome, investments in the processes of building trust in government data, and
    ensuring the data are ready and suitable for use, for both immediate and future
    uses.
  Author: Harrison, Teresa and F. Luna-Reyes, Luis and Pardo, Theresa and De Paula,
    Nic and Najafabadi, Mahdi and Palmer, Jillian
  Book Title/Journal: Proceedings of the 20th Annual International Conference on Digital
    Government Research
  DOI: 10.1145/3325112.3325245
  JCS_FACTOR: 0.0
  Keywords: Data Management, Artificial Intelligence, Policy Analysis, Data Analytics,
    DMBOK
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL CONFERENCE ON DIGITAL
    GOVERNMENT RESEARCH
  Title: 'The Data Firehose and AI in Government: Why Data Management is a Key to
    Value and Ethics'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'With the advent of the era of big data, power supply security management
    systems will get a lot of picture data. In the face of massive image data, this
    paper studies the image management technology based on convolutional neural network.
    Aiming at the high repetition rate of self built image database samples and the
    problem that many sample classes contain uncorrelated images, two algorithms are
    proposed to improve the quality of the database: de duplication and de uncorrelation.
    By using the depth convolution neural network, the Embedding represented by the
    corresponding image is taken, and the distance between Embedding is calculated
    in the Euclidean space to achieve the purpose of de duplication and de uncorrelation.
    In this paper, "time" and "accuracy" are used to evaluate the performance of de
    duplication and de uncorrelation algorithms. The comparison examples of some sample
    classes before and after removing repetition and before and after removing uncorrelation
    are shown. The Recall-value of the database after removing duplicate and uncorrelated
    is tested based on the GoogLe Netplus-model respectively, which proves the effectiveness
    of the two filtering algorithms and overcomes the complexity of the traditional
    filtering process.'
  Author: Yuwei, Sun and Jianbao, Zhu and Qingshan, Ma and Xinchun, Yu and Ye, Shi
    and Yu, Chen
  Book Title/Journal: Proceedings of the 2019 5th International Conference on Systems,
    Control and Communications
  DOI: 10.1145/3377458.3377464
  JCS_FACTOR: 0.0
  Keywords: deep learning, Picture management, convolutional neural network
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 5TH INTERNATIONAL CONFERENCE ON SYSTEMS, CONTROL
    AND COMMUNICATIONS
  Title: Picture Management of Power Supply Safety Management System Based on Deep
    Learning Technology
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In working to rescue victims of human trafficking, law enforcement officers
    face a host of challenges. Working in complex, layered organizational structures,
    they face challenges of collaboration and communication. Online information is
    central to every phase of a human-trafficking investigation. With terabytes of
    available data such as sex work ads, policing is increasingly a big-data research
    problem. In this study, we interview sixteen law enforcement officers working
    to rescue victims of human trafficking to try to understand their computational
    needs. We highlight three major areas where future work in human-computer interaction
    can help. First, combating human trafficking requires advances in information
    visualization of large, complex, geospatial data, as victims are frequently forcibly
    moved across jurisdictions. Second, the need for unified information databases
    raises critical research issues of usable security and privacy. Finally, the archaic
    nature of information systems available to law enforcement raises policy issues
    regarding resource allocation for software development.
  Author: Deeb-Swihart, Julia and Endert, Alex and Bruckman, Amy
  Book Title/Journal: Proceedings of the 2019 CHI Conference on Human Factors in Computing
    Systems
  DOI: 10.1145/3290605.3300561
  JCS_FACTOR: 0.0
  Keywords: law enforcement, human trafficking, qualitative, needs analysis
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING
    SYSTEMS
  Title: Understanding Law Enforcement Strategies and Needs for Combating Human Trafficking
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Map matching is important in many trajectory based applications like route
    optimization and traffic schedule, etc. As the widely used methods, Hidden Markov
    Model and its variants are well studied to provide accurate and efficient map
    matching service. However, HMM based methods fail to utilize the value of enormous
    trajectory big data, which are useful for the map matching task. Furthermore,
    with many following-up works, they are still easily influenced by the noisy records,
    which are very common in the real system. To solve these problems, we revisit
    the map matching task from the data perspective, and propose to utilize the great
    power of data to help solve these problems. We build a deep learning based model
    to utilize all the trajectory data for joint training and knowledge sharing. With
    the help of embedding techniques and sequence learning model with attention enhancement,
    our system does the map matching in the latent space, which is tolerant to the
    noise in the physical space. Extensive experiments demonstrate that our model
    outperforms the widely used HMM based methods more than 10% (absolute accuracy)
    and works robustly in the noisy settings in the meantime.
  Author: Zhao, Kai and Feng, Jie and Xu, Zhao and Xia, Tong and Chen, Lin and Sun,
    Funing and Guo, Diansheng and Jin, Depeng and Li, Yong
  Book Title/Journal: Proceedings of the 27th ACM SIGSPATIAL International Conference
    on Advances in Geographic Information Systems
  DOI: 10.1145/3347146.3359090
  JCS_FACTOR: 0.0
  Keywords: deep learning, data driven system, map matching
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 27TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON
    ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS
  Title: 'DeepMM: Deep Learning Based Map Matching with Data Augmentation'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Knowing who is an expert on social media is a challenging yet important
    task, especially in a world where misleading information is commonplace and where
    social media is an important information source for knowledge seekers. In this
    paper we investigate expertise heuristics by comparing features of experts versus
    non-experts in big data settings. We employ a large set of features to classify
    experts and non-experts using data collected on two social media platform (Twitter
    and reddit). Our results show a good ability to predict who is an expert, especially
    using language-based features, validating that heuristics can be developed to
    differentiate experts from novices organically, based on social media use. Our
    results contribute to the development of expertise location and identification
    systems as well as our understanding on how experts present themselves on social
    media.
  Author: Horne, Benjamin D. and Nevo, Dorit and Adal\i{}, Sibel
  Book Title/Journal: SIGMIS Database
  DOI: 10.1145/3353401.3353406
  JCS_FACTOR: 0.0
  Keywords: data analytics., social media, expertise location
  SCI_FACTOR: 0.0
  TITLE_UPPER: SIGMIS DATABASE
  Title: 'Recognizing Experts on Social Media: A Heuristics-Based Approach'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: In the last decade the (big) data-driven science paradigm became a wide-spread
    reality. However, this approach has some limitations such as a performance dependency
    on the quality of the data and the lack of reproducibility of the results. In
    order to enable this reproducibility, many tools such as Workflow Management Systems
    were developed to formalize process pipelines and capture execution traces. However,
    interoperating data generated by these solutions became a problem, since most
    systems adopted proprietary data models. To support interoperability across heterogeneous
    provenance data, we propose a Service Oriented Architecture with a polystore storage
    design in which provenance is conceptually represented utilizing the ProvONE model.
    A wrapper layer is responsible for transforming data described by heterogeneous
    formats into ProvONE-compliant. Moreover, we propose a query layer that provides
    location and access transparency to users. Furthermore, we conduct two feasibility
    studies, showcasing real usecase scenarios. Firstly, we illustrate how two research
    groups can compare their processes and results. Secondly, we show how our architecture
    can be used as a queriable provenance repository. We show Polyflow's viability
    for both scenarios using the Goal-Question-Metric methodology. Finally, we show
    our solution usability and extensibility appeal by comparing it to similar approaches.
  Author: Mendes, Yan and Braga, Regina and Str\"{o}ele, Victor and de Oliveira, Daniel
  Book Title/Journal: Proceedings of the XV Brazilian Symposium on Information Systems
  DOI: 10.1145/3330204.3330259
  JCS_FACTOR: 0.0
  Keywords: polystore, heterogeneous provenance data integration, Workflows interoperability
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE XV BRAZILIAN SYMPOSIUM ON INFORMATION SYSTEMS
  Title: 'Polyflow: A SOA for Analyzing Workflow Heterogeneous Provenance Data in
    Distributed Environments'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Stable and high-quality electric energy is the main driving force for
    the development of social science, technology, and the national economic leap.
    The assessment and monitoring of electrical safety rely on the generation, collection
    and statistics of large amounts of data by the power system. For the possible
    problems and impurities in these data, this paper uses the 'local Chebyshev theorem'
    and the 'near data averaging method' for the attribute values. The error is cleaned,
    and the 'sorting neighbor algorithm' is used to clean the duplicate data, thereby
    improving the data quality and realizing the accuracy of the safety monitoring
    of the power grid of the smart grid.
  Author: Lv, Zhining and Hu, Ziheng and Ning, Baifeng and Li, Wei and Yan, Gangfeng
    and Ding, Lifu and Shi, Xiasheng and Guo, Ningxuan
  Book Title/Journal: Proceedings of the 2019 the International Conference on Pattern
    Recognition and Artificial Intelligence
  DOI: 10.1145/3357777.3357781
  JCS_FACTOR: 0.0
  Keywords: Power monitoring, Proximity data averaging, Chebyshev theory, Data cleaning
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 THE INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION
    AND ARTIFICIAL INTELLIGENCE
  Title: Safety Monitoring of Power Industrial Control Terminals Based on Data Cleaning
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Referring to the supervised learning and unsupervised learning in machine
    learning, we divide the data cleaning processes into supervised and unsupervised
    two forms too, and then, we reclassify the data quality problems into canonicalization
    error, redundancy error, strong logic error and weak logic error according to
    the characteristics of unsupervised cleaning. For the weak logic errors, we propose
    a repair framework AC-Framework and an algorithm AC-Repair based on the attribute
    correlation. When repairing, we first establish a priority queue(PQ) for elements
    to be repaired according to the minimum cost idea and take the corresponding conflict-free
    data set(Icf) as a training set to learn the correlation among attributes. Then,
    we select the first element in PQ list as the candidate element to repair, and
    recompute the PQ list after one repair round to improve the efficiency. Finally,
    in order to prevent the algorithm from endless loops, we set a label flag to mark
    the repaired elements, in this way, every error element will be repaired at most
    once. In the experimental part, we compare the AC-Repair algorithm with the interpolation-based
    repair algorithm to verify its validity.
  Author: Li, Pei and Dai, Chaofan and Wang, Wenqian
  Book Title/Journal: Proceedings of the 2019 the 5th International Conference on
    E-Society, e-Learning and e-Technologies
  DOI: 10.1145/3312714.3312717
  JCS_FACTOR: 0.0
  Keywords: weak logic errors, Unsupervised data cleaning, attribute correlation,
    machine learning, minimum repair cost
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 THE 5TH INTERNATIONAL CONFERENCE ON E-SOCIETY,
    E-LEARNING AND E-TECHNOLOGIES
  Title: Application of Attribute Correlation in Unsupervised Data Cleaning
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "This paper proposes to deduce certain fixes to graphs G based on data\
    \ quality rules \xCE\xA3 and ground truth \xCE\u201C (i.e., validated attribute\
    \ values and entity matches). We fix errors detected by \xCE\xA3 in G such that\
    \ the fixes are assured correct as long as \xCE\xA3and \xCE\u201C are correct.\
    \ We deduce certain fixes in two paradigms. (a) We interact with users and \"\
    incrementally\" fix errors online. Whenever users pick a small set V0 of nodes\
    \ in G, we fix all errors pertaining to V0 and accumulate ground truth in the\
    \ process. (b) Based on accumulated \xCE\u201C, we repair the entire graph G offline;\
    \ while this may not correct all errors in G, all fixes are guaranteed certain.We\
    \ develop techniques for deducing certain fixes. (1) We define data quality rules\
    \ to support conditional functional dependencies, recursively defined keys and\
    \ negative rules on graphs, such that we can deduce fixes by combining data repairing\
    \ and object identification. (2) We show that deducing certain fixes is Church-Rosser,\
    \ i.e., the deduction converges at the same fixes regardless of the order of rules\
    \ applied. (3) We establish the complexity of three fundamental problems associated\
    \ with certain fixes. (4) We provide (parallel) algorithms for deducing certain\
    \ fixes online and offline, and guarantee to reduce running time when given more\
    \ processors. Using real-life and synthetic data, we experimentally verify the\
    \ effectiveness and scalability of our methods."
  Author: Fan, Wenfei and Lu, Ping and Tian, Chao and Zhou, Jingren
  Book Title/Journal: Proc. VLDB Endow.
  DOI: 10.14778/3317315.3317318
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROC. VLDB ENDOW.
  Title: Deducing Certain Fixes to Graphs
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Almost all organizations use some type of Entity Resolution (ER) methods
    to uniquely identify their customers and vendors across different channels of
    contact. In the case of persons, this requires the use of personally identifying
    information (PII) such as name, address, phone number, and email address. Because
    of the growing concerns over data privacy and identity theft, organizations are
    reluctant to release personally-identifiable customer information even for education
    and research purposes. An alternative is to generate synthetic data to use in
    student exercises and for research related to entity resolution methods and techniques.
    One advantage of synthetically generated data for ER is it can be fully annotated
    with the correct linking making it very easy to calculate the precision and recall
    of linking operations. This paper discusses a simple method to generate synthetic
    data as input for ER processes. The method allows the user to randomly assign
    certain types and levels of data quality errors along with other types of non-error
    variations to the data, such as nicknames, different date formats, and changes
    in address. For ER research in particular, the method can create introduce data
    redundancy by copying records referencing the same person into the same file or
    into different files with different record layouts.
  Author: Ye, Yumeng and Talburt, John R.
  Book Title/Journal: J. Comput. Sci. Coll.
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. COMPUT. SCI. COLL.
  Title: Generating Synthetic Data to Support Entity Resolution Education and Research
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: 'A/B Testing is the gold standard to estimate the causal relationship
    between a change in a product and its impact on key outcome measures. It is widely
    used in the industry to test changes ranging from simple copy change or UI change
    to more complex changes like using machine learning models to personalize user
    experience. The key aspect of A/B testing is evaluation of experiment results.
    Designing the right set of metrics - correct outcome measures, data quality indicators,
    guardrails that prevent harm to business, and a comprehensive set of supporting
    metrics to understand the "why" behind the key movements is the #1 challenge practitioners
    face when trying to scale their experimentation program [18, 22]. On the technical
    side, improving sensitivity of experiment metrics is a hard problem and an active
    research area, with large practical implications as more and more small and medium
    size businesses are trying to adopt A/B testing and suffer from insufficient power.
    In this tutorial we will discuss challenges, best practices, and pitfalls in evaluating
    experiment results, focusing on both lessons learned and practical guidelines
    as well as open research questions.'
  Author: Shi, Xiaolin and Dmitriev, Pavel and Gupta, Somit and Fu, Xin
  Book Title/Journal: Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery &amp; Data Mining
  DOI: 10.1145/3292500.3332297
  JCS_FACTOR: 0.0
  Keywords: online metrics, a/b testing, user experience evaluation, controlled experiments
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE
    DISCOVERY &AMP; DATA MINING
  Title: Challenges, Best Practices and Pitfalls in Evaluating Results of Online Controlled
    Experiments
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: Ilyas, Ihab F. and Chu, Xu
  Book Title/Journal: empty
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: EMPTY
  Title: Data Cleaning
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: book
  Year: 2019
- Abstract: 'Participatory sensing has become a new data collection paradigm that
    leverages the wisdom of the crowd for big data applications without spending cost
    to buy dedicated sensors. It collects data from human sensors by using their own
    devices such as cell phone accelerometers, cameras, and GPS devices. This benefit
    comes with a drawback: human sensors are arbitrary and inherently uncertain due
    to the lack of quality guarantee. Moreover, participatory sensing data are time
    series that exhibit not only highly irregular dependencies on time but also high
    variance between sensors. To overcome these limitations, we formulate the problem
    of validating uncertain time series collected by participatory sensors. In this
    article, we approach the problem by an iterative validation process on top of
    a probabilistic time series model. First, we generate a series of probability
    distributions from raw data by tailoring a state-of-the-art dynamical model, namely
    &lt;u&gt;G&lt;/u&gt;eneralised &lt;u&gt;A&lt;/u&gt;uto &lt;u&gt;R&lt;/u&gt;egressive
    &lt;u&gt;C&lt;/u&gt;onditional &lt;u&gt;H&lt;/u&gt;eteroskedasticity (GARCH),
    for our joint time series setting. Second, we design a feedback process that consists
    of an adaptive aggregation model to unify the joint probabilistic time series
    and an efficient user guidance model to validate aggregated data with minimal
    effort. Through extensive experimentation, we demonstrate the efficiency and effectiveness
    of our approach on both real data and synthetic data. Highlights from our experiences
    include the fast running time of a probabilistic model, the robustness of an aggregation
    model to outliers, and the significant effort saving of a guidance model.'
  Author: Cong, Phan Thanh and Tam, Nguyen Thanh and Yin, Hongzhi and Zheng, Bolong
    and Stantic, Bela and Hung, Nguyen Quoc Viet
  Book Title/Journal: ACM Trans. Intell. Syst. Technol.
  DOI: 10.1145/3326164
  JCS_FACTOR: 0.0
  Keywords: trust management, probabilistic database, Participatory sensing
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM TRANS. INTELL. SYST. TECHNOL.
  Title: Efficient User Guidance for Validating Participatory Sensing Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: RNA-seq and Ribo-seq are popular techniques for quantifying cellular transcription
    and translation. These experiments use next-generation sequencing to produce genome-wide
    high-resolution snapshots of the total populations of mRNAs and translating ribosomes
    within the investigated samples. When performed in concert, these experiments
    yield valuable information about protein synthesis rates and translational efficiency.
    Due to their intricate experimental protocols and demanding data processing requirements,
    quality control and analysis of such experiments are often challenging. Therefore,
    methods for accurately assessing data quality, and for identifying contaminated
    samples, are greatly needed. In the following we use a novel negative selection
    inspired algorithm called Boundary Detection Using Nearest Neighbors (BDUNN),
    for the identification of corrupted samples. Our algorithm constructs a detector
    set and reduced training set that defines the boundaries between normal data points
    and potential anomalies. Subsequently, a nearest neighbor algorithm is used to
    classify unseen observations. We compare the performance of BDUNN with other popular
    negative selection and one-class classification algorithms, and show that BDUNN
    is capable of accurately and efficiently detecting anomalies in standard anomaly
    detection datasets and simulated RNA-seq and Ribo-seq data sets. Furthermore,
    we have implemented our method within an existing R Shiny platform for analyzing
    RNA-seq an Ribo-seq datasets, which permits downstream analysis of anomalous samples.
  Author: Perkins, Patrick and Heber, Steffen
  Book Title/Journal: Proceedings of the 10th ACM International Conference on Bioinformatics,
    Computational Biology and Health Informatics
  DOI: 10.1145/3307339.3342169
  JCS_FACTOR: 0.0
  Keywords: negative selection algorithm, ribosome profiling, machine learning, anomaly
    detection, rna-seq, sample quality
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 10TH ACM INTERNATIONAL CONFERENCE ON BIOINFORMATICS,
    COMPUTATIONAL BIOLOGY AND HEALTH INFORMATICS
  Title: Using a Novel Negative Selection Inspired Anomaly Detection Algorithm to
    Identify Corrupted Ribo-Seq and RNA-Seq Samples
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'As severe dropout in single-cell RNA sequencing (scRNA-seq) degrades
    data quality, current methods for network inference face increased uncertainty
    from such data. To examine how dropout influences directional dependency inference
    from scRNA-seq data, we thus studied four methods based on discrete data that
    are model-free without parametric model assumptions. They include two established
    methods: conditional entropy and Kruskal-Wallis test, and two recent methods:
    causal inference by stochastic complexity and function index. We also included
    three non-directional methods for a contrast. On simulated data, function index
    performed most favorably at varying dropout rates, sample sizes, and discrete
    levels. On an scRNA-seq dataset from developing mouse cerebella, function index
    and Kruskal-Wallis test performed favorably over other methods in detecting expression
    of developmental genes as a function of time. Overall among the four methods,
    function index is most resistant to dropout for both directional and dependency
    inference. The next best choice, Kruskal-Wallis test, carries a directional bias
    towards a uniformly distributed variable. We conclude that a method robust to
    marginal distributions with a sufficiently large sample size can reap benefits
    of single-cell over bulk RNA sequencing in understanding molecular mechanisms
    at the cellular resolution.'
  Author: Dvo\v{r}\'{a}kov\'{a}, Eli\v{s}ka and Kumar, Sajal and Kl\'{e}ma, Ji\v{r}\'{\i}
    and \v{Z}elezn\'{y}, Filip and Drbal, Karel and Song, Mingzhou
  Book Title/Journal: Proceedings of the 2019 6th International Conference on Bioinformatics
    Research and Applications
  DOI: 10.1145/3383783.3383793
  JCS_FACTOR: 0.0
  Keywords: model-free, directional dependency, single-cell sequencing
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 6TH INTERNATIONAL CONFERENCE ON BIOINFORMATICS
    RESEARCH AND APPLICATIONS
  Title: Evaluating Model-Free Directional Dependency Methods on Single-Cell RNA Sequencing
    Data with Severe Dropout
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Government, through Open Government Data "OGD, becomes one of the important
    producers of open data. OGD is an opportunity to create valuable services and
    innovative products useful for citizens as a primarily targeted consumer. However,
    the expected benefits of OGD are not yet met. That is to say, several research
    communities' studies insist on the necessity of creating valuable data in order
    to generate valuable services. These studies are still insufficient for a shared
    understanding of how OGD contribute to the creation of value. For this purpose,
    this paper presents a review of a set of data lifecycle models compared against
    their contribution to the creation of value in the context of OGD.
  Author: Elmekki, Hanae and Chiadmi, Dalila and Lamharhar, Hind
  Book Title/Journal: Proceedings of the ArabWIC 6th Annual International Conference
    Research Track
  DOI: 10.1145/3333165.3333180
  JCS_FACTOR: 0.0
  Keywords: data lifecycle, Open Government Data, data value creation
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE ARABWIC 6TH ANNUAL INTERNATIONAL CONFERENCE RESEARCH
    TRACK
  Title: 'Open Government Data: Towards a Comparison of Data Lifecycle Models'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Lately, the world attention is directed to transforming daily life to
    a smarter one, we cannot deny the smart city concept that became pervading. This
    concept will give every device the chance to communicate with other devices, it
    will simply create the smarter version of everything. However, data heterogeneity
    and quality changes are one of the best priorities and challenges that should
    be handled in this promising concept.In this paper we present a review about data
    categories circulating in a smart city depending on its required services. We
    also study the quality of information as one of both, major challenges and treasures
    in a smart city.
  Author: Rhazal, Oumaima El and Tomader, Mazri
  Book Title/Journal: Proceedings of the 4th International Conference on Smart City
    Applications
  DOI: 10.1145/3368756.3368965
  JCS_FACTOR: 0.0
  Keywords: smart city, internet of things (IoT), quality of information (QoI)
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON SMART CITY APPLICATIONS
  Title: 'Study of Smart City Data: Categories and Quality Challenges'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: With the development and popularization of Internet technology, our country
    is increasingly aware of the importance of e-government, and continuously expands
    the channels and means of e-government development in policy, such as the application
    of e-government to the management of non-profit organizations. However, in practice,
    "e-government + NPO (non-profit organization) management" still has problems such
    as digital divide, information sharing and insufficient disclosure, and information
    security. Therefore, this paper proposes a more complete non-profit organization
    management model based on e-government. From the perspectives of optimization
    services, information sharing, network supervision and information security, it
    is explained how to effectively realize the efficient management of non-profit
    organizations based on e-government.
  Author: Lin, Yuting
  Book Title/Journal: Proceedings of the 2019 7th International Conference on Computer
    and Communications Management
  DOI: 10.1145/3348445.3348464
  JCS_FACTOR: 0.0
  Keywords: non-profit organization, management model, E-government
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND
    COMMUNICATIONS MANAGEMENT
  Title: Government Management Model of Non-Profit Organizations Based on E-Government
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'Data driven Knowledge Graph is rapidly adapted by different societies.
    Many open domain and specific domain knowledge graphs have been constructed, and
    many industries have benefited from knowledge graph. Currently, enterprise related
    knowledge graph is classified as specific domain, but the applications span from
    solving a narrow specific problem to Enterprise Knowledge Management system. With
    the digital transform of traditional industry, Enterprise knowledge becomes more
    and more complicated, it involves knowledge from common domain, multiple specific
    domains, and corporate-specific in general. This tutorial provides an overview
    of current Enterprise Knowledge Graph(EKG). It distinguishes the EKG from specific
    domain according to the knowledge it covers, and provides the examples to illustrate
    the difference between EKG and specific domain KG. The tutorial further summarizes
    EKG into three types: Specific Business Task Enterprise KG, Specific Business
    Unit Enterprise KG and Cross Business Unit Enterprise KG, and illustrates the
    characteristics, steps, challenges, and future research in constructing and consuming
    of each of these three types of EKG .'
  Author: Duan, Rong and Xiao, Yanghua
  Book Title/Journal: Proceedings of the 28th ACM International Conference on Information
    and Knowledge Management
  DOI: 10.1145/3357384.3360314
  JCS_FACTOR: 0.0
  Keywords: relation extraction, ontology, entity recognition, knowledge graph, enterprise
    knowledge management
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION
    AND KNOWLEDGE MANAGEMENT
  Title: Enterprise Knowledge Graph From Specific Business Task to Enterprise Knowledge
    Management
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: This article first introduce a new government initiative emerging after
    the US presidential election in 2008. Comparing to the more descriptive definitions
    of e-government, supporters of these new government initiatives emphasize the
    transformative and normative aspect of the newest generation of Information and
    Communication Technology (ICTs). They argue that the new initiative redefines
    how government should operate and transform state-citizen relationships. To understand
    the core of this initiative and whether it offers new opportunities to solve public
    problems, we collected and analyzed research papers published in the e-governance
    area between 2008 and 2017. Our analysis demonstrates that the use of new generation
    of ICTs has promoted the government information infrastructure. In other words,
    the application of new ICTs enables the government to accumulate and use a large
    amount of data, so that the government makes better decisions. The advancement
    of open data, the wide use of social media, and the potential of data analytics
    have also generated pressure to address challenging questions and issues in e-democracy.
    However, the analysis leads us to deliberate on whether the use of new generation
    of ICTs worldwide have actually achieved their goal. In the conclusion, we present
    challenges to be addressed before new innovative ICTs realize their potential
    towards better public governance.
  Author: Liu, Shuhua Monica and Pan, Liting and Lei, Yupei
  Book Title/Journal: Proceedings of the 12th International Conference on Theory and
    Practice of Electronic Governance
  DOI: 10.1145/3326365.3326374
  JCS_FACTOR: 0.0
  Keywords: E-governance, Transformative governance, Information and communication
    technology (ICT)
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON THEORY AND PRACTICE
    OF ELECTRONIC GOVERNANCE
  Title: What is the Role of New Generation of ICTs in Transforming Government Operation
    and Redefining State-Citizen Relationship in the Last Decade?
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: This panel brings the workings and results of the ACM Education Council
    Task Force on Data Science Education. The task force has gathered information
    on existing programs and has reviewed documents such as the result of the National
    Academies deliberations on data science. The task force is charged with exploring
    the role of computer science in data science education, understanding that data
    science is an inherently interdisciplinary field and not exclusively a computer
    science field. The panel will present a summary of the task force findings by
    two members of the task force and perspectives from leaders in data-intensive
    applications from China. The goal of the panel is to present the findings, but
    also to obtain perspectives from the attendees in order to enrich the task force's
    work.
  Author: Cassel, Lillian and Hongzhi, Wang
  Book Title/Journal: Proceedings of the ACM Conference on Global Computing Education
  DOI: 10.1145/3300115.3312508
  JCS_FACTOR: 0.0
  Keywords: data science, computing for data science, computing curriculum
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE ACM CONFERENCE ON GLOBAL COMPUTING EDUCATION
  Title: 'Panel: The Computing in Data Science'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'The paper presents a systematic literature review of the literature on
    the competencies that are essential to develop a globally competitive workforce
    in the field of data science. The systematic review covers a wide range of literature
    but focuses primarily, but not exclusively, on the computing, information systems,
    management, and organisation science literature. The paper uses a broad research
    search strategy covering four separate electronic databases. The search strategy
    led the researchers to scan 139 titles, abstracts and keywords. Sixty potentially
    relevant articles were identified, of which 42 met the quality criteria and contributed
    to the analysis. A critical appraisal checklist assessed the validity of each
    empirical study. The researchers grouped the findings under six broad competency
    themes: organisational, technical, analytical, ethical and regulatory, cognitive
    and social. Thematic analysis was used to develop a unified model of data science
    competency based on the evidence of the findings. This model will be applied to
    case studies and survey research in future studies. A unified data science competency
    model, supported by empirical evidence, is crucial in closing the skills gap,
    thereby improving the quality and competitiveness of the South Africa''s data
    science workforce. Researchers are encouraged to contribute to the further conceptual
    development of data science competency.'
  Author: Hattingh, Mari\'{e} and Marshall, Linda and Holmner, Marlene and Naidoo,
    Rennie
  Book Title/Journal: Proceedings of the South African Institute of Computer Scientists
    and Information Technologists 2019
  DOI: 10.1145/3351108.3351110
  JCS_FACTOR: 0.0
  Keywords: Data Science, Skills, Competency, Systematic Literature Review
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE SOUTH AFRICAN INSTITUTE OF COMPUTER SCIENTISTS AND
    INFORMATION TECHNOLOGISTS 2019
  Title: 'Data Science Competency in Organisations: A Systematic Review and Unified
    Model'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Computing environment in IoT (Internet of Things) is surrounded with huge
    amounts of heterogeneous data fulfilling many services in everyone's daily life.
    Since, communication process in IoT takes place using different devices such as
    smart phones, sensors, mobile devices, household devices, embedded equipment etc.
    With the use of these variety of devices, the exchange of data in open internet
    environment is prone to vulnerabilities. The main cause for these vulnerabilities
    is the weaknesses in the design of software components and hardware components.
    Bridging communications gaps in the IoT is a complex process as the data is from
    heterogeneous sources. An effort is made in this paper to discuss various challenges
    that are being faced in security and privacy of data. This will be very much helpful
    for researchers who want to pursue research.
  Author: Aljawarneh, Shadi and Radhakrishna, Vangipuram and Kumar, Gunupudi Rajesh
  Book Title/Journal: Proceedings of the 5th International Conference on Engineering
    and MIS
  DOI: 10.1145/3330431.3330457
  JCS_FACTOR: 0.0
  Keywords: challenges in IoT, research issues, vulnerability, IoT classification,
    S/W weakness, security and privacy, IoT architecture, IoT services
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON ENGINEERING AND
    MIS
  Title: A Recent Survey on Challenges in Security and Privacy in Internet of Things
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The ubiquity of data lakes has created fascinating new challenges for
    data management research. In this tutorial, we review the state-of-the-art in
    data management for data lakes. We consider how data lakes are introducing new
    problems including dataset discovery and how they are changing the requirements
    for classic problems including data extraction, data cleaning, data integration,
    data versioning, and metadata management.
  Author: Nargesian, Fatemeh and Zhu, Erkang and Miller, Ren\'{e}e J. and Pu, Ken
    Q. and Arocena, Patricia C.
  Book Title/Journal: Proc. VLDB Endow.
  DOI: 10.14778/3352063.3352116
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROC. VLDB ENDOW.
  Title: 'Data Lake Management: Challenges and Opportunities'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: 'Supply Chain Management (SCM) was born and developed first in an industrial
    context. In the field of services, little research has addressed the issue of
    the company''s SCM. According to [1] "service logistics is an approach that stabilizes
    and guarantees the continuity of flows: it is then oriented more towards the service
    provided than towards reducing traffic costs". The SCM of services is of increasing
    interest to companies facing strong competition, market globalization and rapid
    changes in information and communication technologies. This evolution has led
    to a rapid integration of new digital practices in this field.So, how is the digitalization
    of the SCM of service companies looking today and what will be the future trends?
    On the one hand, with the help of the literature review, we seek to identify the
    concept of the SCM in services and its specificities, then that of digitization
    of the SCM and its organizational dimension. On the other hand, we are attempting
    a prospective approach to the current practices and digitalization prospects of
    the service company''s SCM.'
  Author: Bentalha, Badr and Hmioui, Aziz and Alla, Lhoussaine
  Book Title/Journal: Proceedings of the 4th International Conference on Smart City
    Applications
  DOI: 10.1145/3368756.3369005
  JCS_FACTOR: 0.0
  Keywords: SCM, prospective approach, supply chain, digital, service company
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON SMART CITY APPLICATIONS
  Title: 'The Digitalization of the Supply Chain Management of Service Companies:
    A Prospective Approach'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: MET is a relatively new workshop on metamorphic testing for academic researchers
    and industry practitioners. The first international workshop on MET (MET 2016)
    was co-located with the 38th International Conference on Software Engineering
    (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become
    an annual event. This paper reports on the fourth International Workshop on Metamorphic
    Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the
    41st International Conference on Software Engineering (ICSE 2019). We first outline
    the aims of the workshop, followed by a discussion of its keynote speech and technical
    program.
  Author: Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.
  Book Title/Journal: SIGSOFT Softw. Eng. Notes
  DOI: 10.1145/3356773.3356810
  JCS_FACTOR: 0.0
  Keywords: software verification and validation, software testing, metamorphic testing,
    software engineering
  SCI_FACTOR: 0.0
  TITLE_UPPER: SIGSOFT SOFTW. ENG. NOTES
  Title: 'Workshop Summary: 2019 IEEE / ACM Fourth International Workshop on Metamorphic
    Testing (MET 2019)'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Since the introduction of Bitcoin---the first wide-spread application
    driven by blockchains---the interest of the public and private sector in blockchains
    has skyrocketed. At the core of this interest are the ways in which blockchains
    can be used to improve data management, e.g., by enabling federated data management
    via decentralization, resilience against failure and malicious actors via replication
    and consensus, and strong data provenance via a secured immutable ledger.In practice,
    high-performance blockchains for data management are usually built in permissioned
    environments in which the participants are vetted and can be identified. In this
    setting, blockchains are typically powered by Byzantine fault-tolerant consensus
    protocols. These consensus protocols are used to provide full replication among
    all honest blockchain participants by enforcing an unique order of processing
    incoming requests among the participants.In this tutorial, we take an in-depth
    look at Byzantine fault-tolerant consensus. First, we take a look at the theory
    behind replicated computing and consensus. Then, we delve into how common consensus
    protocols operate. Finally, we take a look at current developments and briefly
    look at our vision moving forward.
  Author: Gupta, Suyash and Hellings, Jelle and Rahnama, Sajjad and Sadoghi, Mohammad
  Book Title/Journal: Proceedings of the 20th International Middleware Conference
    Tutorials
  DOI: 10.1145/3366625.3369437
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 20TH INTERNATIONAL MIDDLEWARE CONFERENCE TUTORIALS
  Title: 'An In-Depth Look of BFT Consensus in Blockchain: Challenges and Opportunities'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: This paper describes how machine learning works with "coring matrix",
    which is designed for measuring the similarity between heterogeneously structured
    references, to get a better performance in Entity Resolution (ER). In the scoring
    matrix, each entity reference is tokenized and all pairs of tokens between the
    references are scored by a similarity scoring function such as the Levenshtein
    edit distance. In so doing, a similarity score vector can measure the similarity
    between references. With the similarity score vector, machine learning is used
    to make the linking decision. Our experiments show that machine learning based
    on score vector outperforms TF-IDF and FuzzyWuzzy benchmarks. One possible explanation
    is that a similarity score vector conveys much more information than a single
    similarity score. Random forest and neural network even get better performance
    with raw score vector input than with the statistic characteristic input.
  Author: Li, Xinming and Talburt, John R. and Li, Ting and Liu, Xiangwen
  Book Title/Journal: J. Comput. Sci. Coll.
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. COMPUT. SCI. COLL.
  Title: Scoring Matrix Combined with Machine Learning for Heterogeneously Structured
    Entity Resolution
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Evaluating the arguments for and against using digital data derived from
    security breaches.
  Author: Douglas, David M.
  Book Title/Journal: Commun. ACM
  DOI: 10.1145/3368091
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMMUN. ACM
  Title: Should Researchers Use Data from Security Breaches?
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Introduction
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: AIOps is about empowering software and service engineers (e.g., developers,
    program managers, support engineers, site reliability engineers) to efficiently
    and effectively build and operate online services and applications at scale with
    artificial intelligence (AI) and machine learning (ML) techniques. AIOps can help
    improve service quality and customer satisfaction, boost engineering productivity,
    and reduce operational cost. In this technical briefing, we first summarize the
    real-world challenges in building AIOps solutions based on our practice and experience
    in Microsoft. We then propose a roadmap of AIOps related research directions,
    and share a few successful AIOps solutions we have built for Microsoft service
    products.
  Author: Dang, Yingnong and Lin, Qingwei and Huang, Peng
  Book Title/Journal: 'Proceedings of the 41st International Conference on Software
    Engineering: Companion Proceedings'
  DOI: 10.1109/ICSE-Companion.2019.00023
  JCS_FACTOR: 0.0
  Keywords: software analytics, AIOps, DevOps
  SCI_FACTOR: 0.0
  TITLE_UPPER: 'PROCEEDINGS OF THE 41ST INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING:
    COMPANION PROCEEDINGS'
  Title: 'AIOps: Real-World Challenges and Research Innovations'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: When properly secured, anonymized, and optimized for research, administrative
    data can be put to work to help government programs better serve those in need.
  Author: Hastings, Justine S. and Howison, Mark and Lawless, Ted and Ucles, John
    and White, Preston
  Book Title/Journal: Commun. ACM
  DOI: 10.1145/3335150
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMMUN. ACM
  Title: Unlocking Data to Improve Public Policy
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Welcome to ACM SIGMOD Record series of interviews with distinguished members
    of the database community. I'm Marianne Winslett, and today we're at the 2017
    SIGMOD and PODS conference in Chicago. I have here with me Mike Franklin, who
    is the chair of the Computer Science department at the University of Chicago.
    Before that, for many years, Mike was a professor at Berkeley where he also served
    as a chair of the Computer Science division. Mike was a co-founder and director
    of the Algorithms, Machines, and People Lab, better known as the AMPLab. He is
    an ACM fellow, a two-time winner of the SIGMOD Ten Year Test of Time Award, and
    a founder of the successful startup, Truviso. Mike's Ph.D. is from the University
    of Wisconsin Madison. So, Mike, welcome!
  Author: Winslett, Marianne and Braganholo, Vanessa
  Book Title/Journal: SIGMOD Rec.
  DOI: 10.1145/3377391.3377398
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: SIGMOD REC.
  Title: Michael Franklin Speaks Out on Data Science
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: 'The General Society Survey(GSS) is a kind of government-funded survey
    which aims at examining the Socio-economic status, quality of life, and structure
    of contemporary society. GSS dataset is regarded as one of the authoritative source
    for the government and organization practitioners to make data-driven policies.
    The previous analytic approaches for GSS dataset are designed by combining expert
    knowledges and simple statistics. In this paper, we proposed a comprehensive data
    management and data mining approach for GSS datasets. The approach is designed
    to be operated in a two-phase manner: a data management phase which can improve
    the quality of GSS data by performing attribute preprocessing and filter-based
    attribute selection; a data mining phase which can extract hidden knowledges from
    the dataset by performing data mining analysis including prediction analysis,
    classification analysis, association analysis and clustering analysis. By leveraging
    the power of data mining techniques, our proposed approach can explore knowledges
    in a fine-grained manner with minimum human interference. Experiments on Chinese
    General Social Survey dataset are conducted at the end to evaluate the performance
    of our approach.'
  Author: Pan, Zhiwen and Zhao, Shuangye and Pacheco, Jesus and Zhang, Yuxin and Song,
    Xiaofan and Chen, Yiqiang and Dai, Lianjun and Zhang, Jun
  Book Title/Journal: Proceedings of the 4th International Conference on Crowd Science
    and Engineering
  DOI: 10.1145/3371238.3371269
  JCS_FACTOR: 0.0
  Keywords: Society survey, Data management, Data mining, Decision support systems,
    Knowledge discovery
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON CROWD SCIENCE AND
    ENGINEERING
  Title: Comprehensive Data Management and Analytics for General Society Survey Dataset
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'As data volume and variety have increased, so have the ties between machine
    learning and data integration become stronger. For machine learning to be effective,
    one must utilize data from the greatest possible variety of sources; and this
    is why data integration plays a key role. At the same time machine learning is
    driving automation in data integration, resulting in overall reduction of integration
    costs and improved accuracy. This tutorial focuses on three aspects of the synergistic
    relationship between data integration and machine learning: (1) we survey how
    state-of-the-art data integration solutions rely on machine learning-based approaches
    for accurate results and effective human-in-the-loop pipelines, (2) we review
    how end-to-end machine learning applications rely on data integration to identify
    accurate, clean, and relevant data for their analytics exercises, and (3) we discuss
    open research challenges and opportunities that span across data integration and
    machine learning.'
  Author: Dong, Xin Luna and Rekatsinas, Theodoros
  Book Title/Journal: Proceedings of the 25th ACM SIGKDD International Conference
    on Knowledge Discovery &amp; Data Mining
  DOI: 10.1145/3292500.3332296
  JCS_FACTOR: 0.0
  Keywords: schema mapping, data integration, entity linkage, data cleaning, data
    fusion
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE
    DISCOVERY &AMP; DATA MINING
  Title: 'Data Integration and Machine Learning: A Natural Synergy'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The recent advances in DNA sequencing technology, from first-generation
    sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed
    the genome research landscape. Its data throughput is unprecedented and severalfold
    as compared with past technologies. DNA sequencing technologies generate sequencing
    data that are big, sparse, and heterogeneous. This results in the rapid development
    of various data protocols and bioinformatics tools for handling sequencing data.In
    this review, a historical snapshot of DNA sequencing is taken with an emphasis
    on data manipulation and tools. The technological history of DNA sequencing is
    described and reviewed in thorough detail. To manipulate the sequencing data generated,
    different data protocols are introduced and reviewed. In particular, data compression
    methods are highlighted and discussed to provide readers a practical perspective
    in the real-world setting. A large variety of bioinformatics tools are also reviewed
    to help readers extract the most from their sequencing data in different aspects,
    such as sequencing quality control, genomic visualization, single-nucleotide variant
    calling, INDEL calling, structural variation calling, and integrative analysis.
    Toward the end of the article, we critically discuss the existing DNA sequencing
    technologies for their pitfalls and potential solutions.
  Author: Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin,
    Qiuzhen and Kwong, Sam and Liang, Cheng
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3340286
  JCS_FACTOR: 0.0
  Keywords: third-generation sequencing (TGS), history, tools, software, DNA sequencing,
    bioinformatics, computational biology, data protocols, technology
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: 'DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics
    Tools'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The recent advances in DNA sequencing technology, from first-generation
    sequencing (FGS) to third-generation sequencing (TGS), have constantly transformed
    the genome research landscape. Its data throughput is unprecedented and severalfold
    as compared with past technologies. DNA sequencing technologies generate sequencing
    data that are big, sparse, and heterogeneous. This results in the rapid development
    of various data protocols and bioinformatics tools for handling sequencing data.In
    this review, a historical snapshot of DNA sequencing is taken with an emphasis
    on data manipulation and tools. The technological history of DNA sequencing is
    described and reviewed in thorough detail. To manipulate the sequencing data generated,
    different data protocols are introduced and reviewed. In particular, data compression
    methods are highlighted and discussed to provide readers a practical perspective
    in the real-world setting. A large variety of bioinformatics tools are also reviewed
    to help readers extract the most from their sequencing data in different aspects,
    such as sequencing quality control, genomic visualization, single-nucleotide variant
    calling, INDEL calling, structural variation calling, and integrative analysis.
    Toward the end of the article, we critically discuss the existing DNA sequencing
    technologies for their pitfalls and potential solutions.
  Author: Wong, Ka-Chun and Zhang, Jiao and Yan, Shankai and Li, Xiangtao and Lin,
    Qiuzhen and Kwong, Sam and Liang, Cheng
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3340286
  JCS_FACTOR: 0.0
  Keywords: third-generation sequencing (TGS), tools, technology, software, history,
    bioinformatics, data protocols, DNA sequencing, computational biology
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: 'DNA Sequencing Technologies: Sequencing Data Protocols and Bioinformatics
    Tools'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Name authority is a common issue in digital library. This paper mainly
    summarizes the practice of constructing name authority database based on multi-source
    data in NSTL. Firstly, we load, integrate different source data and convert them
    into unified structure. Then, we extract scientific entities and relationships
    from these data, according to metadata model. For different entities, we use different
    disambiguation rules and algorithms. As a result, we have constructed author name
    authority database, institution authority database, journal authority database,
    and fund authority database. Compared with Incites, taking six institutions name
    authority data as a sample, the result shows that the average accuracy can reach
    86.8%.1
  Author: Yu, Qianqian and Zhang, Jianyong and Qian, Li and Dong, Zhipeng and Huang,
    Yongwen and Jianhua, Liu
  Book Title/Journal: Proceedings of the 18th Joint Conference on Digital Libraries
  DOI: 10.1109/JCDL.2019.00088
  JCS_FACTOR: 0.0
  Keywords: name disambiguation, NSTL, name authority, multi-source
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 18TH JOINT CONFERENCE ON DIGITAL LIBRARIES
  Title: Practice of Constructing Name Authority Database Based on Multi-Source Data
    Integration
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Advancements in the field of healthcare information management have led
    to the development of a plethora of software, medical devices and standards. As
    a consequence, the rapid growth in quantity and quality of medical data has compounded
    the problem of heterogeneity; thereby decreasing the effectiveness and increasing
    the cost of diagnostics, treatment and follow-up. However, this problem can be
    resolved by using a semi-structured data storage and processing engine, which
    can extract semantic value from a large volume of patient data, produced by a
    variety of data sources, at variable rates and conforming to different abstraction
    levels. Going beyond the traditional relational model and by re-purposing state-of-the-art
    tools and technologies, we present, the Ubiquitous Health Profile (UHPr), which
    enables a semantic solution to the data interoperability problem, in the domain
    of healthcare1.
  Author: Satti, Fahad Ahmed and Khan, Wajahat Ali and Lee, Ganghun and Khattak, Asad
    Masood and Lee, Sungyoung
  Book Title/Journal: Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing
  DOI: 10.1145/3297280.3297354
  JCS_FACTOR: 0.0
  Keywords: ACM proceedings, text tagging
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 34TH ACM/SIGAPP SYMPOSIUM ON APPLIED COMPUTING
  Title: Resolving Data Interoperability in Ubiquitous Health Profile Using Semi-Structured
    Storage and Processing
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Process mining has emerged as a research field that focuses on the analysis
    of processes using event data. Process mining is a current topic that reveals
    several challenges, the most important of which have defined in the Process Mining
    Manifesto [1]. However, none of the published works have mentioned the progress
    of process challenges from data-intensive system to knowledge-intensive system
    related to process mining field. Therefore, the objective of this paper is to
    provide researchers with the recent challenges emerged during the passage from
    data-intensive system to knowledge-intensive system.
  Author: Lamghari, Zineb and Radgui, Maryam and Saidi, Rajaa and Rahmani, Moulay
    Driss
  Book Title/Journal: Proceedings of the ArabWIC 6th Annual International Conference
    Research Track
  DOI: 10.1145/3333165.3333168
  JCS_FACTOR: 0.0
  Keywords: Data-intensive, Process Mining, Knowledge-intensive, Process mining challenges,
    Business Process Management, Adaptive Case Management
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE ARABWIC 6TH ANNUAL INTERNATIONAL CONFERENCE RESEARCH
    TRACK
  Title: Passage Challenges from Data-Intensive System to Knowledge-Intensive System
    Related to Process Mining Field
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Private and public institutions are using open and public data to provide
    better services, which increases the impact of open data on daily life. With the
    advancement of technology, it becomes also important to equip our younger generation
    with the essential skills for future challenges. In order to bring up a generation
    equipped with 21st century skills, open data could facilitate educational processes
    at school level as an educational resource. Open data could acts as a key resource
    to enhance the understanding of data through critical thinking and ethical vision
    among the youth and school pupils. To bring open data into schools, it is important
    to know the teacher's perspective on open data literacy and its possible impact
    on pupils. As a research contribution, we answered these questions through a Danish
    public school teacher's survey where we interviewed 10 Danish public school teachers
    of grade 5-7th and analyzed their views about the impact of open data on pupils'
    learning development. After analyzing Copenhagen city's open data, we identified
    four open data educational themes that could facilitate different subjects, e.g.
    geography, mathematics, basic science and social science. The survey includes
    interviews, open discussions, questionnaires and an experiment with the grade
    7th pupils, where we test the pupils' understanding with open data. The survey
    concluded that open data cannot only empower pupils to understand real facts about
    their local areas, improve civics awareness and develop digital and data skills,
    but also enable them to come up with the ideas to improve their communities.
  Author: Saddiqa, Mubashrah and Rasmussen, Lise and Magnussen, Rikke and Larsen,
    Birger and Pedersen, Jens Myrup
  Book Title/Journal: Proceedings of the 15th International Symposium on Open Collaboration
  DOI: 10.1145/3306446.3340821
  JCS_FACTOR: 0.0
  Keywords: educational themes, impact, educational resource, school pupils, open
    data
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 15TH INTERNATIONAL SYMPOSIUM ON OPEN COLLABORATION
  Title: Bringing Open Data into Danish Schools and Its Potential Impact on School
    Pupils
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'Data science is an interdisciplinary scientific approach that provides
    methods to understand and solve problems in an evidence-based manner, using data
    and experience. Despite the clear benefits from adoption, many firms face challenges,
    be that legal, organisational, or business practices, when seeking to implement
    and embed data science within an existing framework. In this workshop, panel and
    audience members drew on their experiences to elaborate on the challenges encountered
    when attempting to deploying data science within existing frameworks. Panel and
    audience members were drawn from business, academia, and think-tanks. For discussion
    purposes the challenges were grouped within three themes: regulatory; investment;
    and workforce.'
  Author: Alexander, Rohan and Lyons, Kelly and Alexopoulos, Michelle and Austin,
    Lisa
  Book Title/Journal: Proceedings of the 29th Annual International Conference on Computer
    Science and Software Engineering
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: data science adoption, legal, challenges, business practices, organisational
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 29TH ANNUAL INTERNATIONAL CONFERENCE ON COMPUTER
    SCIENCE AND SOFTWARE ENGINEERING
  Title: 'Workshop on Barriers to Data Science Adoption: Why Existing Frameworks Aren''t
    Working'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Individuals increasingly use mobile, wearable, and ubiquitous devices
    capable of unobtrusive collection of vast amounts of scientifically rich personal
    data over long periods (months to years), and in the context of their daily life.
    However, numerous human and technological factors challenge longitudinal data
    collection, often limiting research studies to very short data collection periods
    (days to weeks), spawning recruitment biases, and affecting participant retention
    over time. This workshop is designed to bring together researchers involved in
    longitudinal data collection studies to foster an insightful exchange of ideas,
    experiences, and discoveries to improve the studies' reliability, validity, and
    perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection
    for the participants.
  Author: Manea, Vlad and Berrocal, Allan and De Masi, Alexandre and M\o{}ller, Naja
    Holten and Wac, Katarzyna and Bayer, Hannah and Lehmann, Sune and Ashley, Euan
  Book Title/Journal: Adjunct Proceedings of the 2019 ACM International Joint Conference
    on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International
    Symposium on Wearable Computers
  DOI: 10.1145/3341162.3347758
  JCS_FACTOR: 0.0
  Keywords: mobile devices, panel technique, in situ, human sensing, longitudinal
    studies, human subject studies
  SCI_FACTOR: 0.0
  TITLE_UPPER: ADJUNCT PROCEEDINGS OF THE 2019 ACM INTERNATIONAL JOINT CONFERENCE
    ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2019 ACM INTERNATIONAL
    SYMPOSIUM ON WEARABLE COMPUTERS
  Title: 'LDC ''19: International Workshop on Longitudinal Data Collection in Human
    Subject Studies'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Preface
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: With the rapid development of the data mining industry, the value hidden
    in the massive data has been discovered, but at the same time it has also raised
    concerns about privacy leakage, leakage of sensitive data and other issues. These
    problems have also become numerous studies. Among the methods for solving these
    problems, data desensitization technology has been widely adopted for its outstanding
    performance. However, with the increasing scale of data and the increasing dimension
    of data, the traditional desensitization method for static data can no longer
    meet the requirements of various industries in today's environment to protect
    sensitive data. In the face of ever-changing data sets of scale and dimension,
    static desensitization technology relies on artificially designated desensitization
    rules to grasp the massive data, and it is difficult to control the loss of data
    connotation. In response to these problems, this paper proposes a real-time dynamic
    desensitization method based on data flow, and combines the data anonymization
    mechanism to optimize the data desensitization strategy. Experiments show that
    this method can efficiently and stably perform real-time desensitization of stream
    data, and can save more information to support data mining in the next steps.
  Author: Tian, Bing and Lv, Shuqing and Yin, Qilin and Li, Ning and Zhang, Yue and
    Liu, Ziyan
  Book Title/Journal: Proceedings of the International Conference on Advanced Information
    Science and System
  DOI: 10.1145/3373477.3373499
  JCS_FACTOR: 0.0
  Keywords: data desensitization, stream data, dynamic desensitization
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION
    SCIENCE AND SYSTEM
  Title: Real-Time Dynamic Data Desensitization Method Based on Data Stream
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Threat intelligence sharing has become a cornerstone of cooperative and
    collaborative cybersecurity. Sources providing such data have become more widespread
    in recent years, ranging from public entities (driven by legislatorial changes)
    to commercial companies and open communities that provide threat intelligence
    in order to help organisations and individuals to better understand and assess
    the cyber threat landscape putting their systems at risk. Tool support to automatically
    process this information is emerging concurrently. It has been observed that the
    quality of information received by the sources varies significantly and that in
    order to assess the quality of a threat intelligence source it is not sufficient
    to only consider qualitative indications of the source itself, but it is necessary
    to monitor the data provided by the source continuously to be able to draw conclusions
    about the quality of information provided by a source. In this paper, we propose
    a methodology for evaluating cyber threat information sources based on quantitative
    parameters. The methodology aims to facilitate trust establishment to threat intelligence
    sources, based on a weighted evaluation method that allows each entity to adapt
    it to its own needs and priorities. The approach facilitates automated tools utilising
    threat intelligence, since information to be considered can be prioritised based
    on which source is trusted the most at the time the intelligence arrives.
  Author: Schaberreiter, Thomas and Kupfersberger, Veronika and Rantos, Konstantinos
    and Spyros, Arnolnt and Papanikolaou, Alexandros and Ilioudis, Christos and Quirchmayr,
    Gerald
  Book Title/Journal: Proceedings of the 14th International Conference on Availability,
    Reliability and Security
  DOI: 10.1145/3339252.3342112
  JCS_FACTOR: 0.0
  Keywords: trust indicators, Cooperative and collaborative cybersecurity, cyber threat
    intelligence source evaluation, quality parameters, cyber threat information sharing
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY
    AND SECURITY
  Title: A Quantitative Evaluation of Trust in the Quality of Cyber Threat Intelligence
    Sources
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: With the increasing popularity of the Internet-of-Things (IoT), organizations
    are revisiting their practices as well as adopting new ones so they can deal with
    an ever-growing amount of sensed and actuated data that IoT-compliant things generate.
    Some of these practices are about the use of cloud and/or fog computing. The former
    promotes "anything-as-a-service" and the latter promotes "process data next to
    where it is located". Generally presented as competing models, this paper discusses
    how cloud and fog could work hand-in-hand through a seamless coordination of their
    respective "duties". This coordination stresses out the importance of defining
    where the data of things should be sent (either cloud, fog, or cloud&amp;fog concurrently)
    and in what order (either cloud then fog, fog then cloud, or fog&amp;cloud concurrently).
    Applications' concerns with data such as latency, sensitivity, and freshness dictate
    both the appropriate recipients and the appropriate orders. For validation purposes,
    a healthcare-driven IoT application along with an in-house testbed, that features
    real sensors and fog and cloud platforms, have permitted to carry out different
    experiments that demonstrate the technical feasibility of the coordination model.
  Author: Maamar, Zakaria and Baker, Thar and Faci, Noura and Ugljanin, Emir and Khafajiy,
    Mohammed Al and Bur\'{e}gio, Vanilson
  Book Title/Journal: Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing
  DOI: 10.1145/3297280.3297477
  JCS_FACTOR: 0.0
  Keywords: internet-of-things, coordination, cloud, healthcare, fog
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 34TH ACM/SIGAPP SYMPOSIUM ON APPLIED COMPUTING
  Title: 'Towards a Seamless Coordination of Cloud and Fog: Illustration through the
    Internet-of-Things'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: References
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: 'While sensor networks have been pervasively deployed in the real world,
    more and more mobile crowdsensing (MCS) applications have come into realization
    to collaboratively detect events and collect data. This paper aims to design a
    novel incentive mechanism to achieve good services for mobile crowdsensing applications.
    Responding to insufficient participants, we propose a novel Experience-Based incentive
    mechanism using Reverse Auction (EBRA). Additionally, it can also guarantee fair
    competition while maximizing the total profit of the service platform. Through
    strictly proving, our proposed EBRA incentive mechanism satisfies four properties:
    computational efficiency, individual rationality, profitability, and truthfulness.
    The extensive simulations show that the proposed EBRA method has a better performance
    over 20% than other benchmark mechanisms.'
  Author: Tan, Wenan and Jiang, Zihui
  Book Title/Journal: Proceedings of the International Conference on Artificial Intelligence,
    Information Processing and Cloud Computing
  DOI: 10.1145/3371425.3371459
  JCS_FACTOR: 0.0
  Keywords: incentive mechanism, sensor network, fairness competition, mobile crowdsensing
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE,
    INFORMATION PROCESSING AND CLOUD COMPUTING
  Title: A Novel Experience-Based Incentive Mechanism for Mobile Crowdsensing System
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'The development of quantitative techniques for determining the amount
    of heat lost through the building envelope is essential for targeted retrofits.
    This type of evaluation is traditionally a resource intensive process that involves
    onsite appraisal and in-situ measurements. In order to build more efficient and
    scalable methods for retrofit analysis, new sources of data could be used. Smart
    thermostat data, for example, provide a valuable resource, however they often
    lack detailed information about the building characteristics and energy loads.
    This paper presents and compares three methods for assessing heating characteristics
    of households using a dataset that does not contain heating power. The three methods
    are based on: (1) balance point plots, (2) the extraction of indoor temperature
    decay curves, and (3) the classic differential equation for indoor temperature.
    These methods all take a gray box approach in which physics-based and machine
    learning models are combined. The dataset used for this study consists of over
    4,000 houses in Ontario and New York. The three methods are applied to each building
    and the resulting data is analyzed to determine whether the results are statistically
    sound. It is found that there is a positive linear correlation between characteristics
    derived for each method, although there is uncertainty about absolute values.
    This result indicates that the methods can be used to ascertain relative values
    for the thermal characteristics of a building. The methods suggested in this paper
    may therefore be used to filter heating profiles to target potential retrofit
    measures or other stock-level decisions.'
  Author: Baasch, Gaby and Wicikowski, Adam and Faure, Ga\"{e}lle and Evins, Ralph
  Book Title/Journal: Proceedings of the 6th ACM International Conference on Systems
    for Energy-Efficient Buildings, Cities, and Transportation
  DOI: 10.1145/3360322.3360836
  JCS_FACTOR: 0.0
  Keywords: Buildings, thermal characteristics, smart thermostats, gray box models
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 6TH ACM INTERNATIONAL CONFERENCE ON SYSTEMS FOR
    ENERGY-EFFICIENT BUILDINGS, CITIES, AND TRANSPORTATION
  Title: Comparing Gray Box Methods to Derive Building Properties from Smart Thermostat
    Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Conclusion and Future Thoughts
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: The great amount of time series generated by machines has enormous value
    in intelligent industry. Knowledge can be discovered from high-quality time series,
    and used for production optimization and anomaly detection in industry. However,
    the original sensors data always contain many errors. This requires a sophisticated
    cleaning strategy and a well-designed system for industrial data cleaning. Motivated
    by this, we introduce Cleanits, a system for industrial time series cleaning.
    It implements an integrated cleaning strategy for detecting and repairing three
    kinds of errors in industrial time series. We develop reliable data cleaning algorithms,
    considering features of both industrial time series and domain knowledge. We demonstrate
    Cleanits with two real datasets from power plants. The system detects and repairs
    multiple dirty data precisely, and improves the quality of industrial time series
    effectively. Cleanits has a friendly interface for users, and result visualization
    along with logs are available during each cleaning process.
  Author: Ding, Xiaoou and Wang, Hongzhi and Su, Jiaxuan and Li, Zijue and Li, Jianzhong
    and Gao, Hong
  Book Title/Journal: Proc. VLDB Endow.
  DOI: 10.14778/3352063.3352066
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROC. VLDB ENDOW.
  Title: 'Cleanits: A Data Cleaning System for Industrial Time Series'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The research field of moving objects has been quite active in the past
    20 years. The recording of position data becomes easy and huge amounts of mobile
    data are collected. Moving objects databases represent time-dependent objects
    and support queries with spatial and temporal constraints. In this paper we provide
    the vision of a multi-model and intelligent moving objects database. The goal
    is to enhance the data management of moving objects by providing extensive data
    models for different applications and fusing artificial intelligence techniques.
    Toward this goal, we propose how to develop corresponding modules and integrate
    them into the system to achieve the next-generation moving objects database.
  Author: Xu, Jianqiu and Lu, Hua and G\"{u}ting, Ralf Hartmut
  Book Title/Journal: Proceedings of the 16th International Symposium on Spatial and
    Temporal Databases
  DOI: 10.1145/3340964.3340975
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 16TH INTERNATIONAL SYMPOSIUM ON SPATIAL AND TEMPORAL
    DATABASES
  Title: 'Understanding Human Mobility: A Multi-Modal and Intelligent Moving Objects
    Database'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data acquisition of mobile tracking devices often suffers from invalid
    and non-continuous input data streams. This issue especially occurs with current
    wearables tracking the user's activity and vital data. Typical reasons include
    the short battery life and the fact that the body-worn tracking device may be
    doffed. Other reasons, such as technical issues, can corrupt the data and render
    it unusable. In this paper, we introduce a data imputation concept which complements
    and thus fixes incomplete datasets by using a new merging approach that is particularly
    suitable for assessing activities and vital data. Our technique enables the dataset
    to become coherent and comprehensive so that it is ready for further analysis.
    In contrast to previous approaches, our technique enables the controlled creation
    of continuous data sets that also contain information on the level of uncertainty
    for possible reconversions, approximations, or later analysis.
  Author: Haescher, Marian and Matthies, Denys J. C. and Krause, Silvio and Bieber,
    Gerald
  Book Title/Journal: Proceedings of the 12th ACM International Conference on PErvasive
    Technologies Related to Assistive Environments
  DOI: 10.1145/3316782.3322785
  JCS_FACTOR: 0.0
  Keywords: accelerometer, data imputation, data fusion, smartwatch, sensor fusion,
    mobile device, controlled data creation, coherent database
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES
    RELATED TO ASSISTIVE ENVIRONMENTS
  Title: Presenting a Data Imputation Concept to Support the Continuous Assessment
    of Human Vital Data and Activities
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Learning Analytics (LA) research has demonstrated the potential of LA
    in detecting and monitoring cognitive-affective parameters and improving student
    success. But most of it has been applied to online and computerized learning environments
    whereas physical classrooms have largely remained outside the scope of such research.
    This paper attempts to bridge that gap by proposing a student feedback model in
    which they report on the difficult/easy and engaging/boring aspects of their lecture.
    We outline the pedagogical affordances of an aggregated time-series of such data
    and discuss it within the context of LA research.
  Author: Mitra, Ritayan and Chavan, Pankaj
  Book Title/Journal: Proceedings of the 9th International Conference on Learning
    Analytics &amp; Knowledge
  DOI: 10.1145/3303772.3303821
  JCS_FACTOR: 0.0
  Keywords: mobile application, Large lectures, live feedback, quantified self, learning
    analytics
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS
    &AMP; KNOWLEDGE
  Title: DEBE Feedback for Large Lecture Classroom Analytics
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: empty
  Author: Verma, Neeta and Dawar, Savita
  Book Title/Journal: Commun. ACM
  DOI: 10.1145/3349629
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMMUN. ACM
  Title: Digital Transformation in the Indian Government
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Open source software (OSS) is essential for modern society and, while
    substantial research has been done on individual (typically central) projects,
    only a limited understanding of the periphery of the entire OSS ecosystem exists.
    For example, how are tens of millions of projects in the periphery interconnected
    through technical dependencies, code sharing, or knowledge flows? To answer such
    questions we a) create a very large and frequently updated collection of version
    control data for FLOSS projects named World of Code (WoC) and b) provide basic
    tools for conducting research that depends on measuring interdependencies among
    all FLOSS projects. Our current WoC implementation is capable of being updated
    on a monthly basis and contains over 12B git objects. To evaluate its research
    potential and to create vignettes for its usage, we employ WoC in conducting several
    research tasks. In particular, we find that it is capable of supporting trend
    evaluation, ecosystem measurement, and the determination of package usage. We
    expect WoC to spur investigation into global properties of OSS development leading
    to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates
    the discovery of key technical dependencies, code flow, and social networks that
    provide the basis to determine the structure and evolution of the relationships
    that drive FLOSS activities and innovation.
  Author: Ma, Yuxing and Bogart, Chris and Amreen, Sadika and Zaretzki, Russell and
    Mockus, Audris
  Book Title/Journal: Proceedings of the 16th International Conference on Mining Software
    Repositories
  DOI: 10.1109/MSR.2019.00031
  JCS_FACTOR: 0.0
  Keywords: software mining, software supply chain, software ecosystem
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON MINING SOFTWARE
    REPOSITORIES
  Title: 'World of Code: An Infrastructure for Mining the Universe of Open Source
    VCS Data'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data Cleaning refers to the process of detecting and fixing errors in
    the data. Human involvement is instrumental at several stages of this process
    such as providing rules or validating computed repairs. There is a plethora of
    data cleaning algorithms addressing a wide range of data errors (e.g., detecting
    duplicates, violations of integrity constraints, and missing values). Many of
    these algorithms involve a human in the loop, however, this latter is usually
    coupled to the underlying cleaning algorithms. In a real data cleaning pipeline,
    several data cleaning operations are performed using different tools. A high-level
    reasoning on these tools, when combined to repair the data, has the potential
    to unlock useful use cases to involve humans in the cleaning process. Additionally,
    we believe there is an opportunity to benefit from recent advances in active learning
    methods to minimize the effort humans have to spend to verify data items produced
    by tools or humans. There is currently no end-to-end data cleaning framework that
    systematically involves humans in the cleaning pipeline regardless of the underlying
    cleaning algorithms. In this paper, we present opportunities that this framework
    could offer, and highlight key challenges that need to be addressed to realize
    this vision. We present a design vision and discuss scenarios that motivate the
    need for this framework to judiciously assist humans in the cleaning process.
  Author: Rezig, El Kindi and Ouzzani, Mourad and Elmagarmid, Ahmed K. and Aref, Walid
    G. and Stonebraker, Michael
  Book Title/Journal: Proceedings of the Workshop on Human-In-the-Loop Data Analytics
  DOI: 10.1145/3328519.3329133
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE WORKSHOP ON HUMAN-IN-THE-LOOP DATA ANALYTICS
  Title: Towards an End-to-End Human-Centric Data Cleaning Framework
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Ubiquitous mobile devices with rich sensors and advanced communication
    capabilities have given rise to mobile crowdsensing systems. The diverse reliabilities
    of mobile users and the openness of sensing paradigms raise concerns for data
    trustworthiness, user privacy, and incentive provision. Instead of considering
    these issues as isolated modules in most existing researches, we comprehensively
    capture both conflict and inner-relationship among them. In this paper, we propose
    a holistic solution for trustworthy and privacy-aware mobile crowdsensing with
    no need of a trusted third party. Specifically, leveraging cryptographic technologies,
    we devise a series of protocols to enable benign users to request tasks, contribute
    their data, and earn rewards anonymously without any data linkability. Meanwhile,
    an anonymous trust/reputation model is seamlessly integrated into our scheme,
    which acts as reference for our fair incentive design, and provides evidence to
    detect malicious users who degrade the data trustworthiness. Particularly, we
    first propose the idea of limiting the number of issued pseudonyms which serves
    to efficiently tackle the anonymity abuse issue. Security analysis demonstrates
    that our proposed scheme achieves stronger security with resilience against possible
    collusion attacks. Extensive simulations are presented which demonstrate the efficiency
    and practicality of our scheme.
  Author: Wu, Haiqin and Wang, Liangmin and Xue, Guoliang and Tang, Jian and Yang,
    Dejun
  Book Title/Journal: IEEE/ACM Trans. Netw.
  DOI: 10.1109/TNET.2019.2944984
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: IEEE/ACM TRANS. NETW.
  Title: Enabling Data Trustworthiness and User Privacy in Mobile Crowdsensing
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Social media provides the government with novel methods to improve regulation.\
    \ One leading case has been the use of Yelp reviews to target food safety inspections.\
    \ While previous research on data from Seattle finds that Yelp reviews can predict\
    \ unhygienic establishments, we provide a more cautionary perspective. First,\
    \ we show that prior results are sensitive to what we call \xE2\u20AC\u0153Extreme\
    \ Imbalanced Sampling\xE2\u20AC\x9D: extreme because the dataset was restricted\
    \ from roughly 13k inspections to a sample of only 612 inspections with only extremely\
    \ high or low inspection scores, and imbalanced by not accounting for class imbalance\
    \ in the population. We show that extreme imbalanced sampling is responsible for\
    \ claims about the power of Yelp information in the original classification setup.\
    \ Second, a re-analysis that utilizes the full dataset of 13k inspections and\
    \ models the full inspection score (regression instead of classification) shows\
    \ that (a) Yelp information has lower predictive power than prior inspection history\
    \ and (b) Yelp reviews do not significantly improve predictions, given existing\
    \ information about restaurants and inspection history. Contrary to prior claims,\
    \ Yelp reviews do not appear to aid regulatory targeting. Third, this case study\
    \ highlights critical issues when using social media for predictive models in\
    \ governance and corroborates recent calls for greater transparency and reproducibility\
    \ in machine learning."
  Author: Altenburger, Kristen M. and Ho, Daniel E.
  Book Title/Journal: The World Wide Web Conference
  DOI: 10.1145/3308558.3313683
  JCS_FACTOR: 0.0
  Keywords: food safety, consumer reviews, replication, Yelp, regulation
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE WORLD WIDE WEB CONFERENCE
  Title: Is Yelp Actually Cleaning Up the Restaurant Industry? A Re-Analysis on the
    Relative Usefulness of Consumer Reviews
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: In the business environment, knowledge of company data is essential for
    a variety of tasks. The European funded project euBusinessGraph enables the establishment
    of a company data platform where data providers and consumers can publish and
    access company data. The core of the platform is the semantic data model that
    is the conceptual representation of company data in a common way so that it is
    easier to share and interlink company data. In this paper we show how the unified
    model and Grafterizer, a tool for manipulating and transforming raw data into
    Linked Data, support the linking challenge proposed in FEIII 2019. Results show
    that geographical enrichment of RDF data supports the interlinking process between
    company entities in different datasets.
  Author: Maurino, Andrea and Rula, Anisa and von, Bj\o{}rn Marius and Gomez, Mauricio
    Soto and Elves\ae{}ter, Brian and Roman, Dumitru
  Book Title/Journal: Proceedings of the 5th Workshop on Data Science for Macro-Modeling
    with Financial and Economic Datasets
  DOI: 10.1145/3336499.3338012
  JCS_FACTOR: 0.0
  Keywords: Company data, Entity Matching, RDF, Record Linkage
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 5TH WORKSHOP ON DATA SCIENCE FOR MACRO-MODELING
    WITH FINANCIAL AND ECONOMIC DATASETS
  Title: Modelling and Linking Company Data in the EuBusinessGraph Platform
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: This paper develops a real-time and reliable data collection system for
    big scale emotional recognition systems. Based on the data sample set collected
    in the initialization stage and by considering the dynamic migration of emotional
    recognition data, we design an adaptive Kth average device clustering algorithm
    for migration perception. We define a sub-modulus weight function, which minimizes
    the sum of the weights of the subsets covered by a cover to achieve high-precision
    device positioning. Combining the energy of the data collection devices and the
    energy of the wireless emotional device, we balance the data collection efficiency
    and energy consumption, and define a minimum access number problem based on energy
    and storage space constraints. By designing an approximate algorithm to solve
    the approximate minimum Steiner point problem, the continuous collection of emotional
    recognition data and the connectivity of data acquisition devices are guaranteed
    under the energy constraint of wireless devices. We validate the proposed algorithms
    through simulation experiments using different emotional recognition systems and
    different data scale. Furthermore, we analyze the proposed algorithms in terms
    of topology for devices classification, location accuracy, and data collection
    efficiency by comparing with the Bayesian classifier-based expectation maximization
    algorithm, the background difference-based moving target detection arithmetic
    averaging algorithm, and the Hungarian algorithm for solving the assignment problem.
  Author: Jin, Yong and Qian, Zhenjiang and Chen, Shunjiang
  Book Title/Journal: Personal Ubiquitous Comput.
  DOI: 10.1007/s00779-019-01217-0
  JCS_FACTOR: 0.0
  Keywords: Data acquisition, Location, Data collection, Edge devices, Emotional recognition,
    Collection cost
  SCI_FACTOR: 0.0
  TITLE_UPPER: PERSONAL UBIQUITOUS COMPUT.
  Title: Data Collection Scheme with Minimum Cost and Location of Emotional Recognition
    Edge Devices
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The adoption of artificial intelligence (AI) is becoming increasingly
    popular in the public sector, but there is a severe lack of relevant theoretical
    research. The government of China also has high expectations for AI innovation.
    This paper proposes a four-stage model for AI development in public sectors to
    help public administrators think about the impact of AI on their organizations.
    We empirically investigate a case of AI adoption for delivering public services
    in local government in China. The findings improve our understanding of not only
    the status of AI innovation but also the factors motivating and challenging public
    sectors that are intending to adopt AI. Given that AI application in public sectors
    is still in its infancy, this study provides us with an opportunity to conduct
    longitudinal tracking of AI innovation in local government in China. This could
    help public administrators to think more comprehensively about the changes and
    transformations that AI may bring to the public sector.
  Author: Chen, Tao and Ran, Longya and Gao, Xian
  Book Title/Journal: Proceedings of the 20th Annual International Conference on Digital
    Government Research
  DOI: 10.1145/3325112.3325243
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL CONFERENCE ON DIGITAL
    GOVERNMENT RESEARCH
  Title: 'AI Innovation for Advancing Public Service: The Case of China''s First Administrative
    Approval Bureau'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'The data gluttony of AI is well known: Data fuels the artificial intelligence.
    Technologies that help to gather the needed data are then essential, among which
    the IoT. However, the deployment of IoT solutions raises significant challenges,
    especially regarding the resource and financial costs at stake. It is our view
    that mobile crowdsensing, aka phone sensing, has a major role to play because
    it potentially contributes massive data at a relatively low cost. Still, crowdsensing
    is useless, and even harmful, if the contributed data are not properly analyzed.
    This paper surveys our work on the development of systems facing this challenge,
    which also illustrates the virtuous circles of AI. We specifically focus on how
    intelligent crowdsensing middleware leverages on-device machine learning to enhance
    the reported physical observations. Keywords: Crowdsensing, Middleware, Online
    learning.'
  Author: Du, Yifan and Issarny, Val\'{e}rie and Sailhan, Fran\c{c}oise
  Book Title/Journal: SIGOPS Oper. Syst. Rev.
  DOI: 10.1145/3352020.3352033
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: SIGOPS OPER. SYST. REV.
  Title: 'When the Power of the Crowd Meets the Intelligence of the Middleware: The
    Mobile Phone Sensing Case'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: What are graph dependencies? What do we need them for? What new challenges
    do they introduce? This article tackles these questions. It aims to incite curiosity
    and interest in this emerging area of research.
  Author: Fan, Wenfei
  Book Title/Journal: J. Data and Information Quality
  DOI: 10.1145/3310230
  JCS_FACTOR: 0.0
  Keywords: Dependencies, graphs, validation, dependency discovery, satisfiability,
    error detection, certain fixes, implication
  SCI_FACTOR: 0.0
  TITLE_UPPER: J. DATA AND INFORMATION QUALITY
  Title: 'Dependencies for Graphs: Challenges and Opportunities'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Outlier Detection
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: 'This paper will showcase the work that the City of Portland has done
    around developing Privacy and Information Protection Principles considering the
    current state of technology, the social digital age, and advance inference algorithms
    like machine learning or other Artificial Intelligence tools. By creating more
    responsible data stewardship in the public sector, municipalities are set to build
    trusted information networks involving communities and complex social issues.
    Particularly, the promotion of data privacy can lead to the emergence of anti-poverty
    and economic development strategies.The City of Portland has developed seven Privacy
    and Information Protection Principles: Transparency and accountability, full lifecycle
    stewardship, equitable data management, ethical and non-discriminatory use of
    data, data openness, automated decision systems, and data utility. These principles
    have implications in social equity and the future of technology management in
    smart cities projects. Principle implementation involves the collaboration of
    different agencies, particularly focused on ethics and human rights supporting
    sustainable development.This work is part of emergent strategies for a new generation
    of city services based on data and information, which aim to improve civic engagement,
    social benefits to communities in city neighborhoods and better collaboration
    with partners and other government agencies.'
  Author: Dominguez, Hector and Mowry, Judith and Perez, Elisabeth and Kendrick, Christine
    and Martin, Kevin
  Book Title/Journal: Proceedings of the 2nd ACM/EIGSCC Symposium on Smart Cities
    and Communities
  DOI: 10.1145/3357492.3358628
  JCS_FACTOR: 0.0
  Keywords: Digital equity, government services, Digital Inclusion, Privacy, Automatic
    decision systems
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2ND ACM/EIGSCC SYMPOSIUM ON SMART CITIES AND COMMUNITIES
  Title: Privacy and Information Protection for a New Generation of City Services
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'To enhance the transparency, accountability and efficiency of the Dutch
    Ministry of Justice and Security, the ministry has set up an open data program
    to proactively stimulate sharing its (publicly funded) data sets with the public.
    Disclosure of personal data is considered as one of the main threats for data
    opening. In this contribution we argue that, according to Dutch laws, the criminal
    data within the Dutch justice domain are sensitive data in GDPR terms and that
    the criminal data can only be opened if these sensitive data are transformed to
    have no personal information. Subsequently, having no personal information in
    data sets is related to two GDPR concepts: the data being anonymous in its GDPR
    sense or the data being pseudonymized in its GDPR sense. These two GDPR concepts,
    i.e., being anonymous data or pseudonymized data in a GDPR sense, can be distinguished
    in our setting based on whether the data controller cannot or can revert the data
    protection process, respectively. (Note that the terms anonymous and pseudonymized
    are interpreted differently in the technical domain.) We examine realizing these
    GDPR concepts with the Statistical Disclosure Control (SDC) technology and subsequently
    argue that pseudonymized data in a GDPR sense delivers a better data utility than
    the other. At the end, we present a number of the consequences of adopting either
    of these concepts, which can inform legislators and policymakers to define their
    strategy for opening privacy sensitive microdata sets, like those pertaining to
    the Dutch criminal justice domain.'
  Author: S. Bargh, Mortaza and Meijer, Ronald and Vink, Marco and van Den Braak,
    Susan and Schirm, Walter and Choenni, Sunil
  Book Title/Journal: Proceedings of the 20th Annual International Conference on Digital
    Government Research
  DOI: 10.1145/3325112.3325222
  JCS_FACTOR: 0.0
  Keywords: Criminal justice data, GDPR, Data protection, Justice domain data, Microdata,
    Open data, Privacy
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL CONFERENCE ON DIGITAL
    GOVERNMENT RESEARCH
  Title: Opening Privacy Sensitive Microdata Sets in Light of GDPR
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Electrical energy consumption has been an ongoing research area since
    the coming of smart homes and Internet of Things. Consumption characteristics
    and usages profiles are directly influenced by building occupants and their interaction
    with electrical appliances. Data analysis together with machine learning models
    can be utilized to extract valuable information for the benefit of occupants themselves
    (conserve energy and increase comfort levels), power plants (maintenance), and
    grid operators (stability). Public energy datasets provide a scientific foundation
    to develop and benchmark these algorithms and techniques. With datasets exceeding
    tens of terabytes, we present a novel study of five whole-building energy datasets
    with high sampling rates, their signal entropy, and how a well-calibrated measurement
    can have a significant effect on the overall storage requirements. We show that
    some datasets do not fully utilize the available measurement precision, therefore
    leaving potential accuracy and space savings untapped. We benchmark a comprehensive
    list of 365 file formats, transparent data transformations, and lossless compression
    algorithms. The primary goal is to reduce the overall dataset size while maintaining
    an easy-to-use file format and access API. We show that with careful selection
    of file format and encoding scheme, we can reduce the size of some datasets by
    up to 73%.
  Author: Kriechbaumer, Thomas and Jorde, Daniel and Jacobsen, Hans-Arno
  Book Title/Journal: Proceedings of the Tenth ACM International Conference on Future
    Energy Systems
  DOI: 10.1145/3307772.3328285
  JCS_FACTOR: 0.0
  Keywords: Energy dataset, non-intrusive load monitoring, waveform compression, high
    sampling rate, file format, electricity aggregate
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON FUTURE ENERGY
    SYSTEMS
  Title: Waveform Signal Entropy and Compression Study of Whole-Building Energy Datasets
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: As one of the national key infrastructures, the ubiquitous power Internet
    of Things (IoT) provides a convenient method for large-scale power information
    collection. The widespread transmission of massive power information using data
    mining techniques for large amounts of data can yield valuable information. Therefore,
    hacker attacks are endless, posing a threat to the security of the state, society,
    collectives and individuals. In this paper, we propose a secure transmission scheme
    of power information, named "SSD" (Split &amp; Signature &amp; Disturbing). In
    the scheme, after the data is split, it will be anonymized and selected for different
    paths to be transferred to the destination node. After recombination, the data
    will be restored. The SSD ensures the indistinguishability and security of the
    sensitive data by data splitting and disturbing method, and protects the anonymity
    of individual identities by group signature. The experimental results show that
    the individual prediction/actual data similarity approaches 0%, and the similarity
    ratio of the category data (three types in the experiment) is 37.32%, which can
    be judged to be basically non-correlated.
  Author: Luo, Jingtang and Yao, Shiying and Gou, Jijun and Shuai, Lisha and Cao,
    Yu
  Book Title/Journal: Proceedings of the 2019 3rd International Conference on Computer
    Science and Artificial Intelligence
  DOI: 10.1145/3374587.3374631
  JCS_FACTOR: 0.0
  Keywords: Ubiquitous Power IoT, Data Splitting, Information Security
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 2019 3RD INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE
    AND ARTIFICIAL INTELLIGENCE
  Title: A Secure Transmission Scheme of Sensitive Power Information in Ubiquitous
    Power IoT
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Data Transformation
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: We establish a robust schema design framework for data with missing values.
    The framework is based on the new notion of an embedded functional dependency,
    which is independent of the interpretation of missing values, able to express
    completeness and integrity requirements on application data, and capable of capturing
    many redundant data value occurrences. We establish axiomatic and algorithmic
    foundations for reasoning about embedded functional dependencies. These foundations
    allow us to establish generalizations of Boyce-Codd and Third normal forms that
    do not permit any redundancy in any future application data, or minimize their
    redundancy across dependency-preserving decompositions, respectively. We show
    how to transform any given schema into application schemata that meet given completeness
    and integrity requirements and the conditions of the generalized normal forms.
    Data over those application schemata are therefore fit for purpose by design.
    Extensive experiments with benchmark schemata and data illustrate our framework,
    and the effectiveness and efficiency of our algorithms, but also provide quantified
    insight into database schema design trade-offs.
  Author: Wei, Ziheng and Link, Sebastian
  Book Title/Journal: Proc. VLDB Endow.
  DOI: 10.14778/3342263.3342626
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROC. VLDB ENDOW.
  Title: Embedded Functional Dependencies and Data-Completeness Tailored Database
    Design
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: In recent years, significant research efforts have been spent towards
    building intelligent and user-friendly IoT systems to enable a new generation
    of applications capable of performing complex sensing and recognition tasks. In
    many of such applications, there are usually multiple different sensors monitoring
    the same object. Each of these sensors can be regarded as an information source
    and provides us a unique "view" of the observed object. Intuitively, if we can
    combine the complementary information carried by multiple sensors, we will be
    able to improve the sensing performance. Towards this end, we propose DeepFusion,
    a unified multi-sensor deep learning framework, to learn informative representations
    of heterogeneous sensory data. DeepFusion can combine different sensors' information
    weighted by the quality of their data and incorporate cross-sensor correlations,
    and thus can benefit a wide spectrum of IoT applications. To evaluate the proposed
    DeepFusion model, we set up two real-world human activity recognition testbeds
    using commercialized wearable and wireless sensing devices. Experiment results
    show that DeepFusion can outperform the state-of-the-art human activity recognition
    methods.
  Author: Xue, Hongfei and Jiang, Wenjun and Miao, Chenglin and Yuan, Ye and Ma, Fenglong
    and Ma, Xin and Wang, Yijiang and Yao, Shuochao and Xu, Wenyao and Zhang, Aidong
    and Su, Lu
  Book Title/Journal: Proceedings of the Twentieth ACM International Symposium on
    Mobile Ad Hoc Networking and Computing
  DOI: 10.1145/3323679.3326513
  JCS_FACTOR: 0.0
  Keywords: Deep Learning, Internet of Things, Sensor Fusion
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE TWENTIETH ACM INTERNATIONAL SYMPOSIUM ON MOBILE
    AD HOC NETWORKING AND COMPUTING
  Title: 'DeepFusion: A Deep Learning Framework for the Fusion of Heterogeneous Sensory
    Data'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The use and reuse of scientific data is ultimately dependent on the ability
    to understand what those data represent, how they were captured, and how they
    can be used. In many ways, data are only as useful as the metadata available to
    describe them. Unfortunately, due to growing data volumes, large and distributed
    collaborations, and a desire to store data for long periods of time, scientific
    "data lakes" quickly become disorganized and lack the metadata necessary to be
    useful to researchers. New automated approaches are needed to derive metadata
    from scientific files and to use these metadata for organization and discovery.
    Here we describe one such system, Xtract, a service capable of processing vast
    collections of scientific files and automatically extracting metadata from diverse
    file types. Xtract relies on function as a service models to enable scalable metadata
    extraction by orchestrating the execution of many, short-running extractor functions.
    To reduce data transfer costs, Xtract can be configured to deploy extractors centrally
    or near to the data (i.e., at the edge). We present a prototype implementation
    of Xtract and demonstrate that it can derive metadata from a 7 TB scientific data
    repository.
  Author: Skluzacek, Tyler J. and Chard, Ryan and Wong, Ryan and Li, Zhuozhao and
    Babuji, Yadu N. and Ward, Logan and Blaiszik, Ben and Chard, Kyle and Foster,
    Ian
  Book Title/Journal: Proceedings of the 5th International Workshop on Serverless
    Computing
  DOI: 10.1145/3366623.3368140
  JCS_FACTOR: 0.0
  Keywords: materials science, metadata extraction, serverless, data lakes, file systems
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE 5TH INTERNATIONAL WORKSHOP ON SERVERLESS COMPUTING
  Title: Serverless Workflows for Indexing Large Scientific Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Deep learning has been very successful in the past decades, especially
    in Computer Vision and Speech Recognition fields. It has been also used successfully
    in the Natural Language Processing field because of the availability of an enormous
    amount of online text data, such as social networks and reviews websites, which
    have gained a lot of popularity and success in the past years. Sentiment Analysis
    is one of the hottest applications of Natural Language Processing (NLP). Many
    researchers have done excellent work on Sentiment Analysis for English language.
    However, the amount of work on Sentiment Analysis for Arabic language is, in comparison,
    very limited due to the complexity of the Arabic language's morphology and orthography.
    Unlike the English language, Arabic has many different dialects which makes Sentiment
    Analysis for Arabic more difficult and challenging, especially when working on
    data collected from social networks, which is known to be unstructured and noisy.
    Most of the work that has been done on Sentiment Analysis of Arabic language,
    focused on using lexicons and basic machine learning models. In addition, most
    of the work has been done on small datasets because of the limited number of the
    available annotated datasets for Arabic language. This paper proposes state-of-the-art
    research for Sentiment Analysis of Arabic microblogging using new techniques,
    and a sophisticated Arabic text data preprocessing.
  Author: Soufan, Ayah
  Book Title/Journal: Proceedings of the ArabWIC 6th Annual International Conference
    Research Track
  DOI: 10.1145/3333165.3333185
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: PROCEEDINGS OF THE ARABWIC 6TH ANNUAL INTERNATIONAL CONFERENCE RESEARCH
    TRACK
  Title: Deep Learning for Sentiment Analysis of Arabic Text
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Integration of embedded multimedia devices with powerful computing platforms,
    e.g., machine learning platforms, helps to build smart cities and transforms the
    concept of Internet of Things into Internet of Multimedia Things (IoMT). To provide
    different services to the residents of smart cities, the IoMT technology generates
    big multimedia data. The management of big multimedia data is a challenging task
    for IoMT technology. Without proper management, it is hard to maintain consistency,
    reusability, and reconcilability of generated big multimedia data in smart cities.
    Various machine learning techniques can be used for automatic classification of
    raw multimedia data and to allow machines to learn features and perform specific
    tasks. In this survey, we focus on various machine learning platforms that can
    be used to process and manage big multimedia data generated by different applications
    in smart cities. We also highlight various limitations and research challenges
    that need to be considered when processing big multimedia data in real-time.
  Author: Usman, Muhammad and Jan, Mian Ahmad and He, Xiangjian and Chen, Jinjun
  Book Title/Journal: ACM Comput. Surv.
  DOI: 10.1145/3323334
  JCS_FACTOR: 0.0
  Keywords: machine learning, management, IoMT, multimedia, smart cities
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM COMPUT. SURV.
  Title: A Survey on Big Multimedia Data Processing and Management in Smart Cities
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: 'A mobile crowdsensing (MCS) platform motivates employing participants
    from the crowd to complete sensing tasks. A crucial problem is to maximize the
    profit of the platform, i.e., the charge of a sensing task minus the payments
    to participants that execute the task. In this article, we improve the profit
    via the data reconstruction method, which brings new challenges, because it is
    hard to predict the reconstruction quality due to the dynamic features and mobility
    of participants. In particular, two Profit-driven Online Participant Selection
    (POPS) problems under different situations are studied in our work: (1) for S-POPS,
    the sensing cost of the different parts within the target area is the Same. Two
    mechanisms are designed to tackle this problem, including the ProSC and ProSC+.
    An exponential-based quality estimation method and a repetitive cross-validation
    algorithm are combined in the former mechanism, and the spatial distribution of
    selected participants are further discussed in the latter mechanism; (2) for V-POPS,
    the sensing cost of different parts within the target area is Various, which makes
    it the NP-hard problem. A heuristic mechanism called ProSCx is proposed to solve
    this problem, where the searching space is narrowed and both the participant quantity
    and distribution are optimized in each slot. Finally, we conduct comprehensive
    evaluations based on the real-world datasets. The experimental results demonstrate
    that our proposed mechanisms are more effective and efficient than baselines,
    selecting the participants with a larger profit for the platform.'
  Author: Chen, Yueyue and Guo, Deke and Bhuiyan, MD Zakirul Alam and Xu, Ming and
    Wang, Guojun and Lv, Pin
  Book Title/Journal: ACM Trans. Sen. Netw.
  DOI: 10.1145/3342515
  JCS_FACTOR: 0.0
  Keywords: online participant selection, data reconstruction, Compressive mobile
    crowdsensing
  SCI_FACTOR: 0.0
  TITLE_UPPER: ACM TRANS. SEN. NETW.
  Title: Towards Profit Optimization During Online Participant Selection in Compressive
    Mobile Crowdsensing
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Rule-Based Data Cleaning
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: 3D maps of urban environments are useful in various fields ranging from
    cellular network planning to urban planning and climatology. These models are
    typically constructed using expensive techniques such as manual annotation with
    3D modeling tools, extrapolated from satellite or aerial photography, or using
    specialized hardware with depth sensing devices. In this work, we show that 3D
    urban maps can be extracted from standard GNSS data, by analyzing the received
    satellite signals that are attenuated by obstacles, such as buildings. Furthermore,
    we show that these models can be extracted from low-accuracy GNSS data, crowdsourced
    opportunistically from standard smartphones during their user's uncontrolled daily
    commute trips, unleashing the potential of applying the principle to wide areas.
    Our proposal incorporates position inaccuracies in the calculations, and accommodates
    different sources of variability of the satellite signals' SNR. The diversity
    of collection conditions of crowdsourced GNSS positions is used to mitigate bias
    and noise from the data. A binary classification model is trained and evaluated
    on multiple urban scenarios using data crowdsourced from over 900 users. Our results
    show that the generalization accuracy for a Random Forest classifier in typical
    urban environments lies between 79% and 91% on 4 m wide voxels, demonstrating
    the potential of the proposed method for building 3D maps for wide urban areas.
  Author: Rodrigues, Jo\~{a}o G. P. and Aguiar, Ana
  Book Title/Journal: The 25th Annual International Conference on Mobile Computing
    and Networking
  DOI: 10.1145/3300061.3345456
  JCS_FACTOR: 0.0
  Keywords: gnss snr measurements, 3d mapping, crowdsensing
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE 25TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING
  Title: Extracting 3D Maps from Crowdsourced GNSS Skyview Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality is one of the most important problems in data management,
    since dirty data often leads to inaccurate data analytics results and incorrect
    business decisions. Poor data across businesses and the U.S. government are reported
    to cost trillions of dollars a year. Multiple surveys show that dirty data is
    the most common barrier faced by data scientists. Not surprisingly, developing
    effective and efficient data cleaning solutions is challenging and is rife with
    deep theoretical and engineering problems.This book is about data cleaning, which
    is used to refer to all kinds of tasks and activities to detect and repair errors
    in the data. Rather than focus on a particular data cleaning task, we give an
    overview of the endto- end data cleaning process, describing various error detection
    and repair methods, and attempt to anchor these proposals with multiple taxonomies
    and views. Specifically, we cover four of the most common and important data cleaning
    tasks, namely, outlier detection, data transformation, error repair (including
    imputing missing values), and data deduplication. Furthermore, due to the increasing
    popularity and applicability of machine learning techniques, we include a chapter
    that specifically explores how machine learning techniques are used for data cleaning,
    and how data cleaning is used to improve machine learning models.This book is
    intended to serve as a useful reference for researchers and practitioners who
    are interested in the area of data quality and data cleaning. It can also be used
    as a textbook for a graduate course. Although we aim at covering state-of-the-art
    algorithms and techniques, we recognize that data cleaning is still an active
    field of research and therefore provide future directions of research whenever
    appropriate.
  Author: empty
  Book Title/Journal: Data Cleaning
  DOI: empty
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: DATA CLEANING
  Title: Machine Learning and Probabilistic Data Cleaning
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inbook
  Year: 2019
- Abstract: Big Data, is a growing technique these days. There are many uses of Big
    Data; Artificial Intelligence, Health Care, Business, and many more. For that
    reason, it becomes necessary to deal with this massive volume of data with caution
    and care in a term to make sure that the data used and produced is in high quality.
    Therefore, the Big Data quality is must, and its rules have to be satisfied. In
    this paper, the main Big Data Quality Factors, which need to be measured, is presented
    in the perspective of the data itself, the data management, data processing, and
    data users. This research highlights the quality factors that may be used later
    to create different Big Data quality models.
  Author: Abdallah, Mohammad
  Book Title/Journal: 2019 International Conference on Big Data and Computational
    Intelligence (ICBDCI)
  DOI: 10.1109/ICBDCI.2019.8686099
  JCS_FACTOR: 0.0
  Keywords: Big Data;Quality Measurement;Quality Model;Quality Assurance
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 INTERNATIONAL CONFERENCE ON BIG DATA AND COMPUTATIONAL INTELLIGENCE
    (ICBDCI)
  Title: Big Data Quality Challenges
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Big Data has become an imminent part of all industries and business sectors
    today. All organizations in any sector like energy, banking, retail, hardware,
    networking, etc all generate huge quantum of heterogenous data which if mined,
    processed and analyzed accurately can reveal immensely useful patterns for business
    heads to apply to generate and grow their businesses. Big Data helps in acquiring,
    processing and analyzing large amounts of heterogeneous data to derive valuable
    results. Quality of information is affected by size, speed and format in which
    data is generated. Hence, Quality of Big Data is of great relevance and importance.
    We propose addressing various aspects of the raw data to improve its quality in
    the pre-processing stage, as the raw data may not usable as-is. We are exploring
    process like Cleansing to fix as much data as feasible, Noise filters to remove
    bad data, as well sub-processes for Integration and Filtering along with Data
    Transformation/Normalization. We evaluate and profile the Big Data during acquisition
    stage, which is adapted to expectations to avoid cost overheads later while also
    improving and leading to accurate data analysis. Hence, it is imperative to improve
    Data quality even it is absorbed and utilized in an industry's Big Data system.
    In this paper, we propose a Pre-Processing Framework to address quality of data
    in a weather monitoring and forecasting application that also takes into account
    global warming parameters and raises alerts/notifications to warn users and scientists
    in advance.
  Author: Juneja, Ashish and Das, Nripendra Narayan
  Book Title/Journal: 2019 International Conference on Machine Learning, Big Data,
    Cloud and Parallel Computing (COMITCon)
  DOI: 10.1109/COMITCon.2019.8862267
  JCS_FACTOR: 0.0
  Keywords: Big Data;Data integrity;Meteorology;Monitoring;Data mining;Organizations;Big
    Data;Big Data Quality;Data Quality;preprocessing;pre-processing
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 INTERNATIONAL CONFERENCE ON MACHINE LEARNING, BIG DATA, CLOUD
    AND PARALLEL COMPUTING (COMITCON)
  Title: 'Big Data Quality Framework: Pre-Processing Data in Weather Monitoring Application'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The development of Big Data applications is not well-explored, to our
    knowledge. Embracing Big Data in system building, questions arise as to how to
    elicit, specify, analyse, model, and document Big Data quality requirements. In
    our ongoing research, we explore a requirements modelling language for Big Data
    software applications. In this paper, we introduce QualiBD, a modelling tool that
    implements the proposed goal-oriented requirements language that facilitates the
    modelling of Big Data quality requirements.
  Author: Arruda, Darlan and Madhavji, Nazim H.
  Book Title/Journal: 2019 IEEE International Conference on Big Data (Big Data)
  DOI: 10.1109/BigData47090.2019.9006294
  JCS_FACTOR: 0.0
  Keywords: Tools;Data models;Containers;Software;Real-time systems;Big Data applications;Big
    Data Applications;Quality Requirements;Big Data Goal-oriented Requirements Language;Requirements
    Modelling Tool
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
  Title: 'QualiBD: A Tool for Modelling Quality Requirements for Big Data Applications'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data quality assessment, management and improvement is an integral part
    of any big data intensive scientific research to ensure accurate, reliable, and
    reproducible scientific discoveries. The task of maintaining the quality of data,
    however, is non-trivial and poses a challenge for a program like the Department
    of Energy's Atmospheric Radiation Measurement (ARM) that collects data from hundreds
    of instruments across the world, and distributes thousands of streaming data products
    that are continuously produced in near-real-time for an archive 1.7 Petabyte in
    size and growing. In this paper, we present a computational data processing workflow
    to address the data quality issues via an easy and intuitive web-based portal
    that allows reporting of any quality issues for any site, facility or instruments
    at a granularity down to individual variables in the data files. This portal allows
    instrument specialists and scientists to provide corrective actions in the form
    of symbolic equations. A parallel processing framework applies the data improvement
    to a large volume of data in an efficient, parallel environment, while optimizing
    data transfer and file I/O operations; corrected files are then systematically
    versioned and archived. A provenance tracking module tracks and records any change
    made to the data during its entire life cycle which are communicated transparently
    to the scientific users. Developed in Python using open source technologies, this
    software architecture enables fast and efficient management and improvement of
    data in an operational data center environment.
  Author: Kumar, Jitendra and Crow, Michael C. and Devarakonda, Ranjeet and Giansiracusa,
    Michael and Guntupally, Kavya and Olatt, Joseph V. and Price, Zach and Shanafield,
    Harold A. and Singh, Alka
  Book Title/Journal: 2019 IEEE International Conference on Big Data (Big Data)
  DOI: 10.1109/BigData47090.2019.9006358
  JCS_FACTOR: 0.0
  Keywords: Data integrity;Instruments;Atmospheric measurements;Dictionaries;Big Data;Portals;scientific
    data workflows;data quality;provenance;atmospheric science
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
  Title: "Provenance\xE2\u20AC\u201Caware workflow for data quality management and\
    \ improvement for large continuous scientific data streams"
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'Currently, as a result of the continuous increase of data, one of the
    key issues is the development of systems and applications to deal with storage,
    management and processing of big numbers of data. These data are found in unstructured
    ways. Data management with traditional approaches is inappropriate because of
    the large and complex data sizes. Hadoop is a suitable solution for the continuous
    increase in data sizes. The important characteristics of the Hadoop are distributed
    processing, high storage space, and easy administration. Hadoop is better known
    for distributed file systems. In this paper, we have proposed techniques and algorithms
    that deal with big data including data collecting, data preprocessing, algorithms
    for data cleaning, A Technique for Converting Unstructured Data to Structured
    Data using metadata, distributed data file system (fragmentation algorithm) and
    Quality assurance algorithms by using the model is the statistical model to evaluate
    the highest educational institutions. We concluded that Metadata accelerates query
    response required and facilitates query execution, metadata will be content for
    reports, fields and descriptions. Total time access for three complex queries
    in distributed processing it is 00: 03: 00 per second while in nondistributed
    processing it is at 00: 15: 77 per second, average is approximately five minutes
    per second. Quality assurance note values (T-test) is 0.239 and values (T-dis)
    is 1.96, as a result of dealing with scientific sets and humanities sets. In the
    comparison law, it can be deduced that if the t-test is smaller than the t-dis;
    so there is no difference between the mean of the scientific and humanities samples,
    the values of C.V for both scientific is (8.585) and humanities sets is (7.427),
    using the law of homogeneity know whether any sets are more homogeneous whenever
    the value of a small C.V was more homogeneous however the humanity set is more
    homogeneity.'
  Author: Khaleel, Majida Yaseen and Hamad, Murtadha M.
  Book Title/Journal: 2019 12th International Conference on Developments in eSystems
    Engineering (DeSE)
  DOI: 10.1109/DeSE.2019.00072
  JCS_FACTOR: 0.0
  Keywords: Distributed databases;Metadata;Big Data;Standards;File systems;Data integrity;Big
    Data;data quality;unstructured Data Distributed data file system;and statistical
    model.
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 12TH INTERNATIONAL CONFERENCE ON DEVELOPMENTS IN ESYSTEMS ENGINEERING
    (DESE)
  Title: Data Quality Management for Big Data Applications
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Fueled with growth in the fields of Internet of Things (IoT) and Big Data,
    data has become one of the most valuable assets in today's world. While we are
    leveraging this data for analyzing complex systems using machine learning and
    deep learning, a considerable amount of time and effort is spent on addressing
    data quality issues. If undetected, data quality issues can cause large deviations
    in the analysis, misleading data scientists. To ease the effort of identifying
    and addressing data quality challenges, we introduce DQA, a scalable, automated
    and interactive data quality advisor. In this paper, we describe the DQA framework,
    provide detailed description of its components and the benefits of integrating
    it in a data science process. We propose a programmatic approach for implementing
    the data quality framework which automatically generates dynamic executable graphs
    for performing data validations fine-tuned for a given dataset. We discuss the
    use of DQA to build a library of validation checks common to many applications.
    We provide insight into how DQA addresses many persistence and usability issues
    which currently make data cleaning a laborious task for data scientists. Finally,
    we provide a case study of how DQA is implemented in a realworld system and describe
    the benefits realized.
  Author: Shrivastava, Shrey and Patel, Dhaval and Bhamidipaty, Anuradha and Gifford,
    Wesley M. and Siegel, Stuart A. and Ganapavarapu, Venkata Sitaramagiridharganesh
    and Kalagnanam, Jayant R.
  Book Title/Journal: 2019 IEEE International Conference on Big Data (Big Data)
  DOI: 10.1109/BigData47090.2019.9006187
  JCS_FACTOR: 0.0
  Keywords: Data integrity;Machine learning;Pipelines;Cleaning;Libraries;Buildings;Data
    quality;machine learning;data cleaning;scalability;automation;data science
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
  Title: 'DQA: Scalable, Automated and Interactive Data Quality Advisor'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The quality of data for decision-making will always be a major factor
    for companies that want to remain competitors. In addition, the era of Big Data
    has brought new challenges for the processing, management, storage of data and
    in particular the challenge represented by the veracity of these data which is
    one of the 5Vs that characterizes Big Data. This characteristic that defines the
    quality or reliability of the data and its sources must be verified in the future
    systems of each company. In this paper, we present an approach that helps to improve
    the quality of Big Data by the distributed execution of algorithms for detecting
    and correcting data errors. The idea is to have a multi-agents model for errors
    detection and correction in big data flow. This model linked to a repository specific
    to each company. This repository contains the most frequent errors, metadata,
    error types, error detection algorithms and error correction algorithms. Each
    agent of this model represents an algorithm and will be deployed in multiple instances
    when needed. The use of these agents will go through two steps. In the first step,
    the detection agents and error correction agents manage each flow entering the
    system in real time. In the second step, all the processed data flows in first
    step will be a dataset to which the error detection and correction agents are
    applied in batch in order to process other types of errors. Among architectures
    who allow this processing type, we have chosen Lambda architecture.
  Author: Snineh, Sidi Mohamed and Bouattane, Omar and Youssfi, Mohamed and Daaif,
    Abdelaziz
  Book Title/Journal: 2019 Third International Conference on Intelligent Computing
    in Data Sciences (ICDS)
  DOI: 10.1109/ICDS47004.2019.8942297
  JCS_FACTOR: 0.0
  Keywords: Big Data;Multi-agent systems;Companies;Task analysis;Error correction;Data
    integrity;Real-time systems;Multi-agents;Big Data;Data quality;detection and correction
    of data errors
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 THIRD INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA
    SCIENCES (ICDS)
  Title: Towards a multi-agents model for errors detection and correction in big data
    flows
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "In the era of big data, the scientific and social demand for quality\
    \ data is aggressive and urgent. This paper sheds light on the expanded role of\
    \ metrology of verifying validated procedures of data production and developing\
    \ adequate uncertainty evaluation methods to ensure the trustworthiness of data\
    \ and information. In this regard, I explore the mechanism of the national standard\
    \ reference data (SRD) program of Korea, which connects various scientific and\
    \ social sectors to metrology by applying useful metrological concepts and methods\
    \ to produce reliable data and convert such data into national standards. In particular,\
    \ the changing interpretation of metrological key concepts, such as \xE2\u20AC\
    \u0153measurement,\xE2\u20AC\x9D \xE2\u20AC\u0153traceability,\xE2\u20AC\x9D and\
    \ \xE2\u20AC\u0153uncertainty,\xE2\u20AC\x9D will be explored and reconsidered\
    \ from the perspective of data quality assurance. As a result, I suggest the concept\
    \ of \xE2\u20AC\u0153data traceability\xE2\u20AC\x9D with \xE2\u20AC\u0153the\
    \ matrix of data quality evaluation\xE2\u20AC\x9D according to the elements of\
    \ a data production system and related evaluation criteria. To conclude, I suggest\
    \ social and policy implications for the new role of metrology and standards for\
    \ producing and disseminating reliable knowledge sources from big data."
  Author: Lee, Doyoung
  Book Title/Journal: IEEE Access
  DOI: 10.1109/ACCESS.2019.2904286
  JCS_FACTOR: 3.367
  Keywords: Standards;Uncertainty;Big Data;Metrology;Reliability;Measurement uncertainty;Biomedical
    measurement;Big data;data quality;data traceability;metrology;standard reference
    data;uncertainty
  SCI_FACTOR: 0.587
  TITLE_UPPER: IEEE ACCESS
  Title: 'Big Data Quality Assurance Through Data Traceability: A Case Study of the
    National Standard Reference Data Program of Korea'
  Title_JCS: IEEE Access
  Title_SCI: IEEE Access
  Type Publication: article
  Year: 2019
- Abstract: Data quality tests validate heterogeneous data to detect violations of
    syntactic and semantic constraints. The specification of these constraints can
    be incomplete because domain experts typically specify them in an ad hoc manner.
    Existing automated test approaches can generate false alarms and do not explain
    the constraint violations while reporting faulty data records. In previous work,
    we proposed ADQuaTe, which is an automated data quality test approach that uses
    an unsupervised deep learning techni que (1) to discover constraints from big
    datasets that may have been missed by experts, and (2) to label as suspicious
    those records that violate the constraints. These records are grouped and explanations
    for constraint violations are presented to domain experts who determine whether
    or not the groups are actually faulty. This paper presents ADQuaTe2, which extends
    ADQuaTe to use an interactive learning technique that incorporates expert feedback
    to retrain the learning model and improve the accuracy of constraint discovery
    and fault detection. We evaluate the effectiveness of the approach on real-world
    datasets from a health data warehouse and a plant diagnosis database. We also
    use datasets with known faults from the UCI repository to evaluate the improvement
    in the accuracy of the approach after incorporating ground truth knowledge.
  Author: Homayouni, Hajar and Ghosh, Sudipto and Ray, Indrakshi and Kahn, Michael
    G
  Book Title/Journal: 2019 IEEE International Conference on Big Data (Big Data)
  DOI: 10.1109/BigData47090.2019.9006446
  JCS_FACTOR: 0.0
  Keywords: Fault detection;Data integrity;Data models;Semantics;Decision trees;Self-organizing
    feature maps;Inspection;Big Data;Data quality tests;Explainable learning;Interactive
    learning;Unsupervised learning
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
  Title: An Interactive Data Quality Test Approach for Constraint Discovery and Fault
    Detection
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Surface-enhanced Raman spectroscopy (SERS) significantly enhances the
    Raman scattering by molecules, enabling detection and identification of small
    quantities of relevant bio-/chemical markers in a wide range of applications.
    In this paper, we present a big data platform with both a local client and cloud
    server built for acquiring, processing, visualizing and storing SERS sensor data.
    The local client controls the hardware (i.e., spectrometer and stage) to collect
    SERS spectra from HP designed sensors, and offers the options to analyze, visualize
    and save the spectra with meta-data records, including relevant experimental conditions.
    The cloud server contains remote databases and web interface for centralized data
    management to users from different locations. Here we describe how this platform
    was built and demonstrate its use for automated sensor quality control based on
    sensor images. Sensor quality control is a common practice, employed in sensor
    production to select high performing sensors. Image-based approach is a natural
    way to perform sensor quality control without destructing the sensors. Automating
    this process using the proposed platform can also reduce the time spent and achieve
    consistent result by avoiding human visual inspection.
  Author: Zuo, Yiming and Vernica, Rares and Lei, Yang and Barcelo, Steven and Rogacs,
    Anita
  Book Title/Journal: 2019 IEEE Conference on Multimedia Information Processing and
    Retrieval (MIPR)
  DOI: 10.1109/MIPR.2019.00093
  JCS_FACTOR: 0.0
  Keywords: Quality control;Servers;Cloud computing;Big Data;Visualization;Inspection;Databases;Big
    data platform, Surface Enhanced Raman Spectroscopy, sensor quality control
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL
    (MIPR)
  Title: A Big Data Platform for Surface Enhanced Raman Spectroscopy Data with an
    Application on Image-Based Sensor Quality Control
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Missing data filling is a key step in power big data preprocessing, which
    helps to improve the quality and the utilization of electric power data. Due to
    the limitations of the traditional methods of filling missing data, an improved
    random forest filling algorithm is proposed. As a result of the horizontal and
    vertical directions of the electric power data are based on the characteristics
    of time series. Therefore, the method of improved random forest filling missing
    data combines the methods of linear interpolation, matrix combination and matrix
    transposition to solve the problem of filling large amount of electric power missing
    data. The filling results show that the improved random forest filling algorithm
    is applicable to filling electric power data in various missing forms. What's
    more, the accuracy of the filling results is high and the stability of the model
    is strong, which is beneficial in improving the quality of electric power data.
  Author: Deng, Wei and Guo, Yixiu and Liu, Jie and Li, Yong and Liu, Dingguo and
    Zhu, Liang
  Book Title/Journal: Chinese Journal of Electrical Engineering
  DOI: 10.23919/CJEE.2019.000025
  JCS_FACTOR: 0.0
  Keywords: Filling;Power systems;Random forests;Interpolation;Data models;Big Data;Data
    mining;Big data cleaning;missing data filling;data preprocessing;random forest;data
    quality
  SCI_FACTOR: 0.0
  TITLE_UPPER: CHINESE JOURNAL OF ELECTRICAL ENGINEERING
  Title: A missing power data filling method based on improved random forest algorithm
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: In the era of big data, cloud computing and the Internet of Things, the
    quality of data has tremendous impact on our everyday life. Moreover, the increasing
    velocity, volume and variety of data requires new approaches for quality assessment.
    In this paper, a new approach for quality assessment is presented that applies
    metamorphic testing to data quality. The exemplary application of the approach
    on a big data application shows promising results for the suitability of the approach.
  Author: Auer, Florian and Felderer, Michael
  Book Title/Journal: 2019 IEEE/ACM 4th International Workshop on Metamorphic Testing
    (MET)
  DOI: 10.1109/MET.2019.00019
  JCS_FACTOR: 0.0
  Keywords: Big Data;Data integrity;Testing;Encyclopedias;Internet;Electronic publishing;metamorphic
    testing, data quality, big data, quality assessment, metamorphic data relations
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE/ACM 4TH INTERNATIONAL WORKSHOP ON METAMORPHIC TESTING (MET)
  Title: Addressing Data Quality Problems with Metamorphic Data Relations
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data analysis is the most important aspect of any business as it is critical
    to decision-making. Data quality assessment is a necessary function to be performed
    before data analysis, as the quality of data has high impact on the outcome of
    the analysis. Data quality is a multi-dimensional factor that affects the analysis
    in numerous ways. Among all the dimensions, accuracy is the most important and
    hardest dimension to assess. With the advent of big data, this problem becomes
    more complicated. There are only few studies that focus on data accuracy with
    minimal domain expert dependency. In this paper, we propose an extensive data
    accuracy assessment tool that uses machine learning to determine the accuracy
    of data. In addition, our model also addresses the intrinsic and contextual categories
    of data accuracy. Our model was developed on Apache Spark which serves as the
    big data environment for handling large datasets.
  Author: Mylavarapu, Goutam and Thomas, Johnson P and Viswanathan, K Ashwin
  Book Title/Journal: 2019 IEEE 4th International Conference on Big Data Analytics
    (ICBDA)
  DOI: 10.1109/ICBDA.2019.8713218
  JCS_FACTOR: 0.0
  Keywords: Data integrity;Big Data;Tools;Data models;Standards;Machine learning;Couplings;Data
    accuracy;contextual accuracy;data quality;word embeddings;record linkage;k-nearest
    neighbors;logistic regression;decision trees
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON BIG DATA ANALYTICS (ICBDA)
  Title: An Automated Big Data Accuracy Assessment Tool
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: The operation of complex AC/DC power grid changes rapidly and dynamically,
    which objectively puts forward higher requirements for on-line analysis, and it
    is urgent to improve the basic data quality of power grid. Because of low quality
    and poor synchronization of the basic data of power grid, it is impossible to
    accurately map the actual operation of the power grid. At the same time, the cross-system
    data matching degree is low and the data correlation is poor, so it can not support
    the multi-scale data analysis for all kinds of applications. In this paper, the
    associated method of multi-source heterogeneous data in the power grid is studied.
    Combined with big data's access characteristics, big data storage, big data retrieval
    and artificial intelligence technology, the high-speed data storage and index
    architecture of power big data are constructed, and a multi-dimensional index
    reflecting the associated relationship of operating data is established from the
    dimensions of time, space, application, device and so on. It is easier to analyze
    multi-source data, to improve the basic data quality of power grid, which provides
    effective support for accurate data analysis and evaluation of power grid.
  Author: Pan, Lingling and Liu, Jun and Li, Feng
  Book Title/Journal: 2019 IEEE 4th Advanced Information Technology, Electronic and
    Automation Control Conference (IAEAC)
  DOI: 10.1109/IAEAC47372.2019.8997699
  JCS_FACTOR: 0.0
  Keywords: Big Data;Power grids;Indexes;Power measurement;Time measurement;Memory;power
    big data;multi-source heterogeneous data;spatio-temporal correlation;data storage;multi-dimensional
    index
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE 4TH ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION
    CONTROL CONFERENCE (IAEAC)
  Title: Multi-dimensional Index Construction of Electric Power Multi-source Measurement
    Data considering Spatio-temporal Correlation
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Increasing agricultural production is top most solution in the face of
    rapid population growth through digitalization of agriculture by using most developed
    technology like big data. There is a long debate on the application of big data
    in agriculture. This study is an attempt to explore the suitability of the big
    data technologies for increasing production and improving quality in agriculture.
    The study uses an extensive review of current research works and studies in agriculture
    for exploring the best and compatible practices which can help farmers at field
    level for increasing production and improving quality. This study reveals a number
    of available big data technologies and practices in agriculture for solving the
    current problems and challenges at field level. A conceptual model is developed
    for proper implementation of available big data technologies at farmer's field
    level. The study highlights data generation procedure, availability of technology,
    availability of hardware, software, data collection techniques, method of analysis
    and suitability of application of big data technologies for smart agriculture.
    The article explores that there are still some challenges exists in this field
    as a new domain in agriculture like privacy of data, data quality, availability,
    initial investment, infrastructure and related expertise. The study suggests that
    government initiatives, public-private partnership, openness of data, financial
    investment and regional basis research work are necessary for implementing the
    big data technologies in agriculture at large scale.
  Author: Islam Sarker, Md Nazirul and Wu, Min and Chanthamith, Bouasone and Yusufzada,
    Shaheen and Li, Dan and Zhang, Jie
  Book Title/Journal: 2019 2nd International Conference on Artificial Intelligence
    and Big Data (ICAIBD)
  DOI: 10.1109/ICAIBD.2019.8836982
  JCS_FACTOR: 0.0
  Keywords: Agriculture;Big Data;Production;Sensors;Systematics;Sociology;Statistics;big
    data;smart agriculture;data driven;precision agriculture;smart farming
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG
    DATA (ICAIBD)
  Title: 'Big Data Driven Smart Agriculture: Pathway for Sustainable Development'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: We attempt to demonstrate the value of big data to enterprises by interweaving
    the perceptions, challenges, and opportunities of big data for businesses. While
    enterprises are aware of the value of big data to their businesses, there are
    challenges of exploiting big data in terms of data quality and usage. Executives
    might lack knowledge on how applications are related to one another in the big
    data ecosystem and the business benefits to reap. We summarize the results of
    a research study that explores emerging business perceptions of big data. We examine
    the current practice in 20 large enterprises, each having an annual revenue of
    more than USD 0.5 billion and present our findings related to executive perceptions.
    We provide insights on how firms can develop their big data expertise along various
    dimensions and identify critical ideas to be further investigated to better understand
    the issues that practitioners and researchers might be equally grappling with.
  Author: Singh, Nitin and Lai, Kee-Hung and Vejvar, Markus and Cheng, T. C. E.
  Book Title/Journal: IEEE Engineering Management Review
  DOI: 10.1109/EMR.2019.2900208
  JCS_FACTOR: 0.0
  Keywords: Big Data;Data integrity;Data analysis;Software;Investment;Companies;Big
    data;business analytics;Hadoop;people;talent gap;implementation
  SCI_FACTOR: 0.3
  TITLE_UPPER: IEEE ENGINEERING MANAGEMENT REVIEW
  Title: 'Big Data Technology: Challenges, Prospects, and Realities'
  Title_JCS: N/A
  Title_SCI: IEEE Engineering Management Review
  Type Publication: article
  Year: 2019
- Abstract: With the growing importance of data in all aspects of the functioning
    of an enterprise, having good quality of data is crucial in support of business
    processes. However, there do not exist good metrics to measure the quality of
    data that is available within an enterprise. While there are several data quality
    standards, their complexity and their required customization makes them difficult
    to use in real-world industrial scenarios. In this paper, we discuss the challenges
    encountered in measuring data quality within asset management systems. We propose
    a policy-based approach for measuring data quality, and show how such an approach
    can be customized and interpreted easily by practitioners in the field.
  Author: "Grueneberg, K. and Calo, S. and Dewan, P. and Verma, D. and O\xE2\u20AC\
    \u2122Gorman, Tristan"
  Book Title/Journal: 2019 IEEE International Conference on Big Data (Big Data)
  DOI: 10.1109/BigData47090.2019.9006422
  JCS_FACTOR: 0.0
  Keywords: Data integrity;Standards;Measurement;Asset management;Data models;Complexity
    theory;Data Quality;Asset Management Systems;Policy based Data Management
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA)
  Title: A Policy-based Approach for Measuring Data Quality
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Smart urban transportation management can be considered as a multifaceted
    big data challenge. It strongly relies on the information collected into multiple,
    widespread, and heterogeneous data sources as well as on the ability to extract
    actionable insights from them. Besides data, full stack (from platform to services
    and applications) Information and Communications Technology (ICT) solutions need
    to be specifically adopted to address smart cities challenges. Smart urban transportation
    management is one of the key use cases addressed in the context of the EUBra-BIGSEA
    (Europe-Brazil Collaboration of Big Data Scientific Research through Cloud-Centric
    Applications) project. This paper specifically focuses on the City Administration
    Dashboard, a public transport analytics application that has been developed on
    top of the EUBra-BIGSEA platform and used by the Municipality stakeholders of
    Curitiba, Brazil, to tackle urban traffic data analysis and planning challenges.
    The solution proposed in this paper joins together a scalable big and fast data
    analytics platform, a flexible and dynamic cloud infrastructure, data quality
    and entity matching algorithms as well as security and privacy techniques. By
    exploiting an interoperable programming framework based on Python Application
    Programming Interface (API), it allows an easy, rapid and transparent development
    of smart cities applications.
  Author: "Fiore, Sandro and Elia, Donatello and Pires, Carlos Eduardo and Mestre,\
    \ Demetrio Gomes and Cappiello, Cinzia and Vitali, Monica and Andrade, Nazareno\
    \ and Braz, Tarciso and Lezzi, Daniele and Moraes, Regina and Basso, Tania and\
    \ Kozievitch, N\xC3\xA1dia P. and Fonseca, Keiko Ver\xC3\xB4nica Ono and Antunes,\
    \ Nuno and Vieira, Marco and Palazzo, Cosimo and Blanquer, Ignacio and Meira,\
    \ Wagner and Aloisio, Giovanni"
  Book Title/Journal: IEEE Access
  DOI: 10.1109/ACCESS.2019.2936941
  JCS_FACTOR: 3.367
  Keywords: Urban areas;Big Data;Data analysis;Transportation;Cloud computing;Data
    mining;Europe;Big data;cloud computing;data analytics;data privacy;data quality;distributed
    environment;public transport management;smart city
  SCI_FACTOR: 0.587
  TITLE_UPPER: IEEE ACCESS
  Title: An Integrated Big and Fast Data Analytics Platform for Smart Urban Transportation
    Management
  Title_JCS: IEEE Access
  Title_SCI: IEEE Access
  Type Publication: article
  Year: 2019
- Abstract: Relay protection big data creates good conditions for the improvement
    of professional applications, and data integrity is an important aspect that reflects
    data quality. The association of relay protection big data is intense. This paper
    applies Apriori algorithm to mine data relevance and generate association rules.
    Based on this, the integrity of relay protection data is checked, and the incomplete
    data is predicted. Taking the relay protection defect data as an example, the
    paper explores the correlation among 251 items of the six dimensions of protection
    relay defect data such as type of protection, the severity of the defect, whether
    the protection is out of operation, the defect location, the cause of the defect,
    and the equipment manufacturer, completing the processing of incomplete data with
    great application results.
  Author: Guo, Peng and Yang, Guosheng and Wang, Wenhuan and Yan, Zhoutian and Zhang,
    Lie and Zhang, Hanfang
  Book Title/Journal: 2019 International Conference on Electronic Engineering and
    Informatics (EEI)
  DOI: 10.1109/EEI48997.2019.00115
  JCS_FACTOR: 0.0
  Keywords: Apriori algorithm;relay protection;defect data;integrity check
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 INTERNATIONAL CONFERENCE ON ELECTRONIC ENGINEERING AND INFORMATICS
    (EEI)
  Title: Relay Protection Data Integrity Check Method Based on Big Data Association
    Algorithm
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: High volumes of satellite data management within an organization is still
    challenging and daunting in the era of big data. The increasing information technology
    costs and limited budgets, growing satellite data needs, data availability across
    multiple teams and projects, strategic goals of organization, and expected project
    outcomes require better satellite data management mechanism and system to facilitate
    research and development activities. An organization level centralized satellite
    data repository is a practical solution to satisfy these requirements. This paper
    describes the best practices and experiences from building such a central satellite
    data repository within our organization, including data management strategy and
    policy, scalable and extensible system infrastructure, comprehensive data management
    system, and technical support and user assistance. These practices can be borrowed
    and applied in other organizations with similar requirements.
  Author: Han, Weiguo and Jochum, Matthew
  Book Title/Journal: IGARSS 2019 - 2019 IEEE International Geoscience and Remote
    Sensing Symposium
  DOI: 10.1109/IGARSS.2019.8900190
  JCS_FACTOR: 0.0
  Keywords: Satellites;Organizations;Research and development;Databases;Monitoring;Buildings;Data
    integrity;Satellite Data Management;Big Data;Data Quality;Central Data Repository
  SCI_FACTOR: 0.0
  TITLE_UPPER: IGARSS 2019 - 2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING
    SYMPOSIUM
  Title: Practices and Experiences in High Volumes of Satellite Data Management
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: 'The multimedia transmission represents a typical big data application
    in the fifth-generation (5G) wireless networks. However, supporting multimedia
    big data transmission over 5G wireless networks imposes many new and open challenges
    because multimedia big data services are both time-sensitive and bandwidth-intensive
    over time-varying wireless channels with constrained wireless resources. To overcome
    these difficulties, in this paper we propose the information-centric virtualization
    architectures for software-defined statistical delay-bounded quality of service
    (QoS) provisioning over 5G multimedia big data wireless networks. In particular,
    our proposed schemes integrate the three 5G-promising candidate techniques to
    guarantee the statistical delay-bounded QoS for multimedia big data transmissions:
    1) information-centric network (ICN), to derive the optimal in-network caching
    locations for multimedia big data; 2) network functions virtualization (NFV),
    to abstract the PHY-layer infrastructures into several virtualized networks to
    derive the optimal multimedia data contents delivery paths; and 3) software-defined
    networks (SDNs), to dynamically reconfigure wireless resources allocation architectures
    through the SDN-control plane. Under our proposed architectures, to jointly optimize
    the implementations of NFV and SDN techniques under ICN architectures, we develop
    the three virtual network selection and transmit-power allocation schemes to:
    1) maximize single user''s effective capacity; 2) jointly optimize the aggregate
    effective capacity and allocation fairness over all users; and 3) coordinate non-cooperative
    gaming among all users, respectively. By simulations and numerical analyses, we
    show that our proposed architectures and schemes significantly outperform the
    other existing schemes in supporting the statistical delay-bounded QoS provisioning
    over the 5G multimedia big data wireless networks.'
  Author: Zhang, Xi and Zhu, Qixuan
  Book Title/Journal: IEEE Journal on Selected Areas in Communications
  DOI: 10.1109/JSAC.2019.2927088
  JCS_FACTOR: 9.144
  Keywords: Big Data;Quality of service;Wireless networks;5G mobile communication;Resource
    management;Wireless sensor networks;5G multimedia big data wireless networks;ICN;NFV;SDN;optimal
    transmit power;statistical delay-bounded QoS;effective capacity;relay selection
  SCI_FACTOR: 2.986
  TITLE_UPPER: IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS
  Title: Information-Centric Virtualization for Software-Defined Statistical QoS Provisioning
    Over 5G Multimedia Big Data Wireless Networks
  Title_JCS: IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS
  Title_SCI: IEEE Journal on Selected Areas in Communications
  Type Publication: article
  Year: 2019
- Abstract: Data in the real world is often dirty. Inconsistency is an important kind
    of dirty data; before repairing inconsistency, we need to detect them first. The
    time complexities of the current inconsistency detection algorithms are super-linear
    to the size of data and not suitable for the big data. For the inconsistency detection
    of big data, we develop an algorithm that detects inconsistency within the one-pass
    scan of the data according to both the functional dependency (FD) and the conditional
    functional dependency (CFD) in our previous work. In this paper, we propose inconsistency
    detection algorithms in terms of FD, CFD, and Denial Constraint (DC). DCs are
    more expressive than FDs and CFDs. Developing the algorithm to detect the violation
    of DCs increases the applicability of our inconsistency detection algorithms.
    We compare the performance of our algorithm with the performance of implementing
    SQL queries in MySQL and BigQuery. The experimental results indicate the high
    efficiency of our algorithms.
  Author: Zhang, Meifan and Wang, Hongzhi and Li, Jianzhong and Gao, Hong
  Book Title/Journal: IEEE Access
  DOI: 10.1109/ACCESS.2019.2898707
  JCS_FACTOR: 3.367
  Keywords: Big Data;Data integrity;Detection algorithms;Databases;Time complexity;Hazards;Business;Inconsistency
    detection;big data;one-pass algorithm;data quality;denial constraint
  SCI_FACTOR: 0.587
  TITLE_UPPER: IEEE ACCESS
  Title: One-Pass Inconsistency Detection Algorithms for Big Data
  Title_JCS: IEEE Access
  Title_SCI: IEEE Access
  Type Publication: article
  Year: 2019
- Abstract: "In an era of super computing, data is increasing exponentially requiring\
    \ more proficiency from the available technologies of data storage, data processing,\
    \ and analysis. Such continuous massive growth of structured and unstructured\
    \ data is referred to as a \xE2\u20AC\u0153Big data\xE2\u20AC\x9D. The processing\
    \ and storage of big data through a conventional technique is not possible. Due\
    \ to improved proficiency of Big Data solution in handling data, such as NoSQL\
    \ caused the developers in the previous decade to start preferring big data databases,\
    \ such as Apache Cassandra, Oracle, and NoSQL. NoSQL is a modern database technology\
    \ that is designed to provide scalability to support voluminous data, leading\
    \ to the rise of NoSQL as the most viable database solution. These modern databases\
    \ aim to overcome the limitations of relational databases such as unlimited scalability,\
    \ high performance, data modeling, data distribution, and continuous availability.\
    \ These days, the larger enterprises need to shift NoSQL databases due to their\
    \ more flexible models. It is a great challenge for business organizations and\
    \ enterprises to transform their existing databases to NoSQL databases considering\
    \ heterogeneity and complexity in relational data. In addition, with the emergence\
    \ of big data, data cleansing has become a great challenge. In this paper, we\
    \ proposed an approach that has two modules: data transformation and data cleansing\
    \ module. The first phase is the transformation of a relational database to Oracle\
    \ NoSQL database through model transformation. The second phase provides data\
    \ cleansing ability to improve data quality and prepare it for big data analytics.\
    \ The experiments show the proposed approach successfully transforms the relational\
    \ database to a big data database and improve data quality."
  Author: Ramzan, Shabana and Bajwa, Imran Sarwar and Ramzan, Bushra and Anwar, Waheed
  Book Title/Journal: IEEE Access
  DOI: 10.1109/ACCESS.2019.2916912
  JCS_FACTOR: 3.367
  Keywords: Big Data;NoSQL databases;Transforms;Scalability;Servers;Tools;Relational
    databases;NoSQL;big data;data cleansing
  SCI_FACTOR: 0.587
  TITLE_UPPER: IEEE ACCESS
  Title: Intelligent Data Engineering for Migration to NoSQL Based Secure Environments
  Title_JCS: IEEE Access
  Title_SCI: IEEE Access
  Type Publication: article
  Year: 2019
- Abstract: In this paper, a novel variational inference semisupervised Gaussian mixture
    model (VI-S2GMM) model is first proposed for semisupervised predictive modeling
    in multimode processes. Parameters of Gaussian components are identified more
    accurately with extra unlabeled samples, which improve the prediction performance
    of the regression model. Since all labeled and unlabeled data samples are involved
    in each iteration of parameter updating, intractable computing problems occur
    when facing high-dimension datasets. To tackle this problem, a scalable stochastic
    VI-S2GMM (SVI-S2GMM) is further proposed. Through taking advantage of a stochastic
    gradient optimization algorithm to maximize the evidence of lower bound, the VI-based
    algorithm becomes scalable. In the SVI-S2GMM, only one or a minibatch of samples
    is randomly selected to update parameters in each iteration, which is more efficient
    than the VI-S2GMM. Since the whole dataset is divided and transferred to iterations
    batch by batch, the scalable SVI-S2GMM algorithm can easily handle the big data
    modeling issue. In this way, a large number of unlabeled data can be useful in
    the modeling, which will further benefit the prediction performance. The SVI-S2GMM
    is then exploited for the prediction of a quality-related key performance index.
    Two examples demonstrate the feasibility and effectiveness of the proposed algorithms.
  Author: Yao, Le and Ge, Zhiqiang
  Book Title/Journal: IEEE Transactions on Industrial Electronics
  DOI: 10.1109/TIE.2018.2856200
  JCS_FACTOR: 8.236
  Keywords: Data models;Big Data;Predictive models;Inference algorithms;Prediction
    algorithms;Semisupervised learning;Computational modeling;Big data;Gaussian mixture
    model (GMM);multimode process modeling;quality prediction;semisupervised modeling;stochastic
    variational inference (SVI)
  SCI_FACTOR: 2.393
  TITLE_UPPER: IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS
  Title: Scalable Semisupervised GMM for Big Data Quality Prediction in Multimode
    Processes
  Title_JCS: IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS
  Title_SCI: IEEE Transactions on Industrial Electronics
  Type Publication: article
  Year: 2019
- Abstract: This paper analyses the existing problems in on-line monitoring and data
    quality evaluation of substation equipment, and proposes a multi-dimensional fuzzy
    comprehensive evaluation method for on-line monitoring data quality of substation
    equipment. The evaluation index set of online monitoring data quality of substation
    equipment with 5 dimensions and 11 secondary indexes is established. The weight
    is determined by combining subjective and objective methods. The fuzzy transformation
    is completed based on membership function and a multi-dimensional fuzzy comprehensive
    evaluation model is established. Finally, the evaluation grade of online monitoring
    data of substation equipment is obtained. Finally, compared with other methods,
    the validity and accuracy of this method are verified.
  Author: Dehui, Fu and Feng, Wang and Shuai, Yuan and Guangzhen, Wang and Mingxin,
    Shao
  Book Title/Journal: 2019 6th International Conference on Information Science and
    Control Engineering (ICISCE)
  DOI: 10.1109/ICISCE48695.2019.00154
  JCS_FACTOR: 0.0
  Keywords: big data;data quality;subordination;fuzzy comprehensive evaluation;On-line
    monitoring
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 6TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL
    ENGINEERING (ICISCE)
  Title: Fuzzy Comprehensive Evaluation Method for On-line Monitoring Data Quality
    of Substation Equipment
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: This paper studies the characteristics of big data of power, and aims
    at the data quality problems faced by power system. It puts forward an assessment
    method of power system data quality. Based on the characteristics of large power
    data, a series of indicators influencing the data are analyzed and hierarchically
    divided to determine the measurement standard of power production data during
    the process of risk management, namely, the risk index system. Then, the risk
    assessment model of power data is established by referring to the assessment model
    in other fields or the rules of deduction and induction in data mining. It can
    be used to evaluate the quality of power system data, and find a framework and
    solution suitable for large data quality assessment. Finally, the model is implemented
    on Hadoop platform, which proves that it takes into account the completeness of
    the index system, the objectivity of the assessment method and the rapidity of
    the calculation method.
  Author: Zeyong, Wang and Yutian, Hong and Zhongzheng, Tong
  Book Title/Journal: 2019 International Conference on Smart Grid and Electrical Automation
    (ICSGEA)
  DOI: 10.1109/ICSGEA.2019.00028
  JCS_FACTOR: 0.0
  Keywords: risk assessment;electric power;fuzzy comprehensive evaluation;Hadoop;index
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION
    (ICSGEA)
  Title: Risk Assessment Model and Experimental Analysis of Electric Power Production
    Based on Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data preparation for data mining in industrial applications is a key success
    factor which requires considerable repeated efforts. Although the required activities
    need to be repeated in very similar fashion across many projects, details of their
    implementation differ and require both application understanding and experience.
    As a result, data preparation is done by data mining experts with a strong domain
    background and a good understanding of the characteristics of the data to be analyzed.
    Experts with these profiles usually have an engineering background and no strong
    expertise in distributed programming or big data technology. Unfortunately, the
    amount of data can be so large that distributed algorithms are required to allow
    for inspection of results and iteration of preparation steps. This contribution
    introduces an interactive data preparation workflow for signal data from chemical
    plants enabling domain experts without background in distributed computing and
    extensive programming experience to leverage the power of big data technologies.
  Author: Borrison, Reuben and Kloepper, Benjamin and Mullen, Jennifer
  Book Title/Journal: 2019 IEEE 17th International Conference on Industrial Informatics
    (INDIN)
  DOI: 10.1109/INDIN41052.2019.8972078
  JCS_FACTOR: 0.0
  Keywords: Data quality;Soft sensors;Big data
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE 17TH INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS (INDIN)
  Title: Data Preparation for Data Mining in Chemical Plants using Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: "Big Data Cyber Security Analytics (BDCA) leverages big data technologies\
    \ for collecting, storing, and analyzing a large volume of security events data\
    \ to detect cyber-attacks. Accuracy and response time, being the most important\
    \ quality concerns for BDCA, are impacted by changes in security events data.\
    \ Whilst it is promising to adapt a BDCA system's architecture to the changes\
    \ in security events data for optimizing accuracy and response time, it is important\
    \ to consider large search space of architectural configurations. Searching a\
    \ large space of configurations for potential adaptation incurs an overwhelming\
    \ adaptation time, which may cancel the benefits of adaptation. We present an\
    \ adaptation approach, QuickAdapt, to enable quick adaptation of a BDCA system.\
    \ QuickAdapt uses descriptive statistics (e.g., mean and variance) of security\
    \ events data and fuzzy rules to (re) compose a system with a set of components\
    \ to ensure optimal accuracy and response time. We have evaluated QuickAdapt for\
    \ a distributed BDCA system using four datasets. Our evaluation shows that on\
    \ average QuickAdapt reduces adaptation time by 105\xC3\u2014 with a competitive\
    \ adaptation accuracy of 70% as compared to an existing solution."
  Author: Ullah, Faheem and Ali Babar, M.
  Book Title/Journal: 2019 24th International Conference on Engineering of Complex
    Computer Systems (ICECCS)
  DOI: 10.1109/ICECCS.2019.00016
  JCS_FACTOR: 0.0
  Keywords: Big Data;Quality of service;Time factors;Feature extraction;Computer crime;Computer
    architecture;big data, cyber security, adaptation, accuracy
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 24TH INTERNATIONAL CONFERENCE ON ENGINEERING OF COMPLEX COMPUTER
    SYSTEMS (ICECCS)
  Title: 'QuickAdapt: Scalable Adaptation for Big Data Cyber Security Analytics'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: As the core of informatization, data has a huge significance to the development
    of information-based enterprises. Data replication technology is an important
    approach to solve the problem of enterprise data sharing based on distributed
    database system. It plays a crucial role in promoting business integration of
    enterprises and institutions, improving data quality, enhancing data sharing and
    improving the application level of back-end big data analysis [1]. It is necessary
    to do research for making a good data management of the distributed database application
    system, synchronizing the data to the data, preventing data conflicting and being
    able to synchronize or asynchronous replication. Combining with the database design
    model of the ship monitoring and control system, this paper mainly described how
    to complete the construction of distributed database system using Oracle, based
    on advanced replication technology named as the combination of multi-agent replication
    and materialized views hybrid replication technology.
  Author: Jiang, Ying and Zhang, Na and Fang, Ying
  Book Title/Journal: 2019 International Conference on Intelligent Transportation,
    Big Data Smart City (ICITBS)
  DOI: 10.1109/ICITBS.2019.00118
  JCS_FACTOR: 0.0
  Keywords: Distributed databases;Database systems;Synchronization;Business;Monitoring;Marine
    vehicles;Distributed database;advanced replication;materialized view;data conflict
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION, BIG DATA
    SMART CITY (ICITBS)
  Title: The Analysis and Design of Ship Monitoring System Based on Hybrid Replication
    Technology
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Data analysis is a demanding task that involves extracting deep insights
    hidden in data. Many businesses enforce data analysis irrespective of the domain,
    as it is crucial in minimizing developmental risks. Raw data cannot be used to
    perform any analysis as poor-quality data leads to erroneous decision-making.
    This makes data quality assessment a necessary function before data analysis.
    Data quality is a multi-dimensional factor that affects the analysis in multiple
    ways. Among all the dimensions, consistency is one of the most critical dimensions
    to assess. Context of data plays an important role in consistency assessment,
    as the records are inherently related within a dataset. Existing studies are computationally
    expensive and do not consider the context of data. In this paper, we propose a
    comprehensive context-aware data consistency assessment tool that uses machine
    learning to evaluate the consistency of data. Our model was developed on Apache
    Hadoop and Apache Spark to support big data, as well as to boost some computationally
    intensive algorithms.
  Author: Mylavarapu, Goutam and Viswanathan, K. Ashwin and Thomas, Johnson P.
  Book Title/Journal: 2019 IEEE/ACS 16th International Conference on Computer Systems
    and Applications (AICCSA)
  DOI: 10.1109/AICCSA47632.2019.9035250
  JCS_FACTOR: 0.0
  Keywords: Feature extraction;Data analysis;Data integrity;Data models;Machine learning
    algorithms;Context modeling;Task analysis;Data analysis;data context;data quality;data
    consistency;machine learning;word embeddings;approximate dependencies;mutual information;apache
    hadoop;apache spark
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE/ACS 16TH INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS AND
    APPLICATIONS (AICCSA)
  Title: Assessing Context-Aware Data Consistency
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: With the development of Smart Court 3.0, the amount of judicial data that
    can be stored and processed by the computer is increasing rapidly. People gradually
    realize that judicial data contains tremendous social and business value. However,
    we need stronger ability to handle with and apply massive, multi-source and heterogeneous
    judicial data. A complete data governance system should be built in order to make
    full use of the value of data assets. In such a data governance system, data quality
    control is one of the key steps of data governance, and also the bottleneck of
    data service development, because data quality determines the upper limit of data
    application. This paper proposes a judicial data quality measurement framework
    by analyzing some judicial business data, followed by a data governance method
    driven by it.
  Author: He, Tieke and Chen, Shenghao and Hao, Lian and Liu, Jia
  Book Title/Journal: 2019 IEEE 19th International Conference on Software Quality,
    Reliability and Security Companion (QRS-C)
  DOI: 10.1109/QRS-C.2019.00026
  JCS_FACTOR: 0.0
  Keywords: Data integrity;Big Data;Decision making;Organizations;Standards organizations;data
    quality;judicial data governance;quality measurement
  SCI_FACTOR: 0.0
  TITLE_UPPER: 2019 IEEE 19TH INTERNATIONAL CONFERENCE ON SOFTWARE QUALITY, RELIABILITY
    AND SECURITY COMPANION (QRS-C)
  Title: Quality Driven Judicial Data Governance
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: inproceedings
  Year: 2019
- Abstract: Anecdotal evidence suggests that, despite the large variety of data, the
    huge volume of generated data, and the fast velocity of obtaining data (i.e.,
    big data), quality of big data is far from perfect. Therefore, many firms defer
    collecting and integrating big data as they have concerns regarding the impact
    of utilizing big data on data diagnosticity (i.e., retrieval of valuable information
    from data) and firm decision making quality. In this study, we use the Organizational
    Learning Theory and Wang and Strong's data quality framework to explore the impact
    of processing big data on firm decision quality and the mediating role of data
    quality (DQ) and data diagnosticity on this relationship. We validate the proposed
    research model using survey data from 130 firms, obtained from data analysts and
    IT managers. Results confirm the critical role of DQ in increasing data diagnosticity
    and improving firm decision quality when processing big data; suggesting important
    implications for practice and theory. Findings also reveal that while big data
    utilization positively impacts contextual DQ, accessibility DQ, and representational
    DQ, interestingly, it negatively impacts intrinsic DQ. Furthermore, findings show
    that while intrinsic DQ, contextual DQ, and representational DQ significantly
    increase data diagnosticity, accessibility DQ does not influence it. Most importantly,
    the findings show that big data utilization does not significantly impact the
    quality of firm decisions and it is fully mediated through DQ and data diagnosticity.
    The results of this study contribute to practice by providing important guidelines
    for managers to improve firm decision quality through the use of big data.
  Author: Maryam Ghasemaghaei and Goran Calic
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.03.008
  JCS_FACTOR: 5.795
  Keywords: Big data utilization, Data quality, Decision quality, Data diagnosticity
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: Can big data improve firm decision quality? The role of data quality and
    data diagnosticity
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: The essence of an information system lies in the data; if it is not of
    good quality or not sufficiently protected, the consequences will undoubtedly
    be harmful. Quality and Security are two essential aspects that add value to data
    and their implementation has become a real need and must be adopted before any
    data exploitation. Due to the high volume of data, their diversity and their rapid
    generation, effective implementation of such systems requires well thought out
    mechanisms and strategies. This paper provides an overview of Data Quality and
    Data Security in a Big Data context. We want through this paper to highlight the
    conflicts that may exist during the implementation of data security and data quality
    management systems. Such a conflict makes the complexity greater and requires
    new adapted solutions.
  Author: M. TALHA and A. ABOU {EL KALAM} and N. ELMARZOUQI
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.04.127
  JCS_FACTOR: 0.0
  Keywords: Big Data, Data Quality, Data Security, Trade-off between Quality, Security
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: 'Big Data: Trade-off between Data Quality and Data Security'
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: "Human beings share their good or bad opinions about subjects, products,\
    \ and services through internet and social networks. The ability to effectively\
    \ analyze this kind of information is now seen as a key competitive advantage\
    \ to better inform decisions. In order to do so, organizations employ Sentiment\
    \ Analysis (SA) techniques on these data. However, the usage of social media around\
    \ the world is ever-increasing, which considerably accelerates massive data generation\
    \ and makes traditional SA systems unable to deliver useful insights. Such volume\
    \ of data can be efficiently analyzed using the combination of SA techniques and\
    \ Big Data technologies. In fact, big data is not a luxury but an essential necessary\
    \ to make valuable predictions. However, there are some challenges associated\
    \ with big data such as quality that could highly affect the SA systems\xE2\u20AC\
    \u2122 accuracy that use huge volume of data. Thus, the quality aspect should\
    \ be addressed in order to build reliable and credible systems. For this, the\
    \ goal of our research work is to consider Big Data Quality Metrics (BDQM) in\
    \ SA that rely of big data. In this paper, we first highlight the most eloquent\
    \ BDQM that should be considered throughout the Big Data Value Chain (BDVC) in\
    \ any big data project. Then, we measure the impact of BDQM on a novel SA method\
    \ accuracy in a real case study by giving simulation results."
  Author: Imane El Alaoui and Youssef Gahi
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.11.007
  JCS_FACTOR: 0.0
  Keywords: Big Data Quality Metrics, Big Data Value Chain, Big Data, Big Social Data,
    Sentiment Analysis, Opinion Mining
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: The Impact of Big Data Quality on Sentiment Analysis Approaches
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: "Today, as technologies mature and people are encouraged to contribute\
    \ data to organizations\xE2\u20AC\u2122 databases, more transactions are being\
    \ captured than ever before. Meanwhile, improvements in data storage technologies\
    \ have made the cost of evaluating, selecting, and destroying legacy data considerably\
    \ greater than simply letting it accumulate. On the one hand, the excess of stored\
    \ data has considerably increased the opportunities to interrelate and analyze\
    \ them, while the moderate enthusiasm generated by data warehousing and data mining\
    \ in the 1990s has been replaced by a rampant euphoria about big data and data\
    \ analytics. But, is this as wonderful as seems? This paper presents a risk analysis\
    \ of Big Data and Big Data Analytics based on a review of quality factors."
  Author: "Jesus Silva and Omar Bonerge {Pineda Lezama} and Ligia Romero and Darwin\
    \ Solano and Claudia Fern\xC3\xA1ndez"
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.11.052
  JCS_FACTOR: 0.0
  Keywords: Data management, data quality, decision making, data analysis
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: Risk Analysis of Using Big Data in Computer Sciences
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: An IS researcher may obtain Big Data from primary or secondary data sources.
    Sometimes, acquiring primary Big Data is infeasible due to availability, accessibility,
    cost, time, and/or complexity considerations. In this paper, we focus on Big Data-based
    IS research and discuss ways in which one may, post hoc, establish quality thresholds
    for numerical Big Data obtained from secondary sources. We also present guidelines
    for developing journal policies aimed at ensuring the veracity and verifiability
    of such data when used for research purposes.
  Author: Anita Lee-Post and Ram Pakath
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.113135
  JCS_FACTOR: 5.795
  Keywords: Data quality, Big data, Secondary data, Numerical data, Quality threshold
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: Numerical, secondary Big Data quality issues, quality threshold establishment,
    & guidelines for journal policy development
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: Massive amounts of data are available for the organization which will
    influence their business decision. Data collected from the various resources are
    dirty and this will affect the accuracy of prediction result. Data cleansing offers
    a better data quality which will be a great help for the organization to make
    sure their data is ready for the analyzing phase. However, the amount of data
    collected by the organizations has been increasing every year, which is making
    most of the existing methods no longer suitable for big data. Data cleansing process
    mainly consists of identifying the errors, detecting the errors and corrects them.
    Despite the data need to be analyzed quickly, the data cleansing process is complex
    and time-consuming in order to make sure the cleansed data have a better quality
    of data. The importance of domain expert in data cleansing process is undeniable
    as verification and validation are the main concerns on the cleansed data. This
    paper reviews the data cleansing process, the challenge of data cleansing for
    big data and the available data cleansing methods.
  Author: Fakhitah Ridzuan and Wan Mohd Nazmee {Wan Zainon}
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.11.177
  JCS_FACTOR: 0.0
  Keywords: data cleansing, big data, data quality
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: A Review on Data Cleansing Methods for Big Data
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: "Despite great potential, high hopes and big promises, the actual impact\
    \ of big data on the public sector is not always as transformative as the literature\
    \ would suggest. In this paper, we ascribe this predicament to an overly strong\
    \ emphasis the current literature places on technical-rational factors at the\
    \ expense of political decision-making factors. We express these two different\
    \ emphases as two archetypical narratives and use those to illustrate that some\
    \ political decision-making factors should be taken seriously by critiquing some\
    \ of the core \xE2\u20AC\u02DCtechno-optimist\xE2\u20AC\u2122 tenets from a more\
    \ \xE2\u20AC\u02DCpolicy-pessimist\xE2\u20AC\u2122 angle. In the conclusion we\
    \ have these two narratives meet \xE2\u20AC\u02DCeye-to-eye\xE2\u20AC\u2122, facilitating\
    \ a more systematized interrogation of big data promises and shortcomings in further\
    \ research, paying appropriate attention to both technical-rational and political\
    \ decision-making factors. We finish by offering a realist rejoinder of these\
    \ two narratives, allowing for more context-specific scrutiny and balancing both\
    \ technical-rational and political decision-making concerns, resulting in more\
    \ realistic expectations about using big data for policymaking in practice."
  Author: Simon Vydra and Bram Klievink
  Book Title/Journal: Government Information Quarterly
  DOI: https://doi.org/10.1016/j.giq.2019.05.010
  JCS_FACTOR: 7.279
  Keywords: Big data, Analytics, Government, Public administration, Policy-making,
    Decision-making, Science-policy interface, Network governance
  SCI_FACTOR: 2.121
  TITLE_UPPER: GOVERNMENT INFORMATION QUARTERLY
  Title: Techno-optimism and policy-pessimism in the public sector big data debate
  Title_JCS: GOVERNMENT INFORMATION QUARTERLY
  Title_SCI: Government Information Quarterly
  Type Publication: article
  Year: 2019
- Abstract: Scientific research data has an important strategic position for the development
    of enterprises and countries, and is an important basis for management to conduct
    strategic research and decision-making. Compared with the Internet industry, big
    data technology started late in the military enterprises, while military enterprises
    research data often has the characteristics of decentralization, low relevance,
    and diverse data types. It cannot fully utilize the advantages of data resources
    to enhance the core competitiveness of enterprises. To this end, this paper deeply
    explores the application methods of big data technology in military scientific
    research data management, and lays a foundation for the construction of scientific
    research big data platform.
  Author: Wang Kun and Liu Tong and Xie Xiaodan
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.01.221
  JCS_FACTOR: 0.0
  Keywords: big data technology, scientific research data, data analysis, decision
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: Application of Big Data Technology in Scientific Research Data Management
    of Military Enterprises
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: Big data and the Internet of Things (IoT) are considered as the main paradigms
    when defining new information architecture projects. Accordingly, technologies
    that make up these solutions could have an important role to play in business
    information architecture. Solutions that have approached big data and the IoT
    as unique technology initiatives, struggle in finding value in such efforts and
    in the technology itself. A connection to the requirements (volume, velocity,
    and variety) is mandatory to reach the potential business goals. In this context,
    we propose a new architecture for Cognitive Internet of Things (CIoT) and big
    data. The proposed architecture benefits computing mechanisms by combining the
    data WareHouse (DWH) and Data Lake (DL), and defining a tool for heterogeneous
    data collection.
  Author: Mohamed Saifeddine {Hadj Sassi} and Faiza Ghozzi Jedidi and Lamia Chaari
    Fourati
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.09.208
  JCS_FACTOR: 0.0
  Keywords: Internet of Things, Big-Data, Architecture, Cognitive, Data-flow
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: A New Architecture for Cognitive Internet of Things and Big Data
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: The prospering Big data era is emerging in the power grid. Multiple world-wide
    studies are emphasizing the big data applications in the microgrid due to the
    huge amount of produced data. Big data analytics can impact the design and applications
    towards safer, better, more profitable, and effective power grid. This paper presents
    the recognition and challenges of the big data and the microgrid. The construction
    of big data analytics is introduced. The data sources, big data opportunities,
    and enhancement areas in the microgrid like stability improvement, asset management,
    renewable energy prediction, and decision-making support are summarized. Diverse
    case studies are presented including different planning, operation control, decision
    making, load forecasting, data attacks detection, and maintenance aspects of the
    microgrid. Finally, the open challenges of big data in the microgrid are discussed.
  Author: Karim Moharm
  Book Title/Journal: Advanced Engineering Informatics
  DOI: https://doi.org/10.1016/j.aei.2019.100945
  JCS_FACTOR: 5.603
  Keywords: Big data, Microgrid
  SCI_FACTOR: 1.107
  TITLE_UPPER: ADVANCED ENGINEERING INFORMATICS
  Title: 'State of the art in big data applications in microgrid: A review'
  Title_JCS: ADVANCED ENGINEERING INFORMATICS
  Title_SCI: Advanced Engineering Informatics
  Type Publication: article
  Year: 2019
- Abstract: Research in Big Data and analytics offers tremendous opportunities to
    utilize evidence in making decisions in many application domains. To what extent
    can the paradigms of Big Data and analytics be used in the domain of transport?
    This article reports on an outcome of a systematic review of published articles
    in the last five years that discuss Big Data concepts and applications in the
    transportation domain. The goal is to explore and understand the current research,
    opportunities, and challenges relating to the utilization of Big Data and analytics
    in transportation. The review shows the potential of Big Data and analytics to
    garner insights and improve transportation systems through the analysis of various
    forms of data obtained from traffic monitoring systems, connected vehicles, crowdsourcing,
    and social media. We discuss some platforms and software architecture for the
    transport domain, along with a wide array of storage, processing, and analytical
    techniques, and describe challenges associated with the implementation of Big
    Data and analytics. This review contributes broadly to the various ways in which
    cities can utilize Big Data in transportation to guide the creation of sustainable
    and safer traffic systems. Since research in Big Data and transportation is, by
    and large, at infancy, this article does not prescribe recommendations to the
    various challenges identified, which also constitutes the limitation of the article.
  Author: Alex Neilson and  Indratmo and Ben Daniel and Stevanus Tjandra
  Book Title/Journal: Big Data Research
  DOI: https://doi.org/10.1016/j.bdr.2019.03.001
  JCS_FACTOR: 3.578
  Keywords: Big Data, Smart city, Intelligent transportation system, Connected vehicle,
    Road traffic safety, Vision Zero
  SCI_FACTOR: 0.565
  TITLE_UPPER: BIG DATA RESEARCH
  Title: 'Systematic Review of the Literature on Big Data in the Transportation Domain:
    Concepts and Applications'
  Title_JCS: Big Data Research
  Title_SCI: Big Data Research
  Type Publication: article
  Year: 2019
- Abstract: "In oncology, the term \xE2\u20AC\u0153big data\xE2\u20AC\x9D broadly\
    \ describes the rapid acquisition and generation of massive amounts of information,\
    \ typically from population cancer registries, electronic health records, or large-scale\
    \ genetic sequencing studies. The challenge of using big data in cancer research\
    \ lies in interdisciplinary collaboration and information processing to unify\
    \ diverse data sources and provide valid analytics to harness meaningful information.\
    \ This article provides an overview of how big data approaches can be applied\
    \ in cancer research, and how they can be used to translate information into new\
    \ ways to ultimately make informed decisions that improve cancer care and delivery."
  Author: Chiaojung Jillian Tsai and Nadeem Riaz and Scarlett Lin Gomez
  Book Title/Journal: Seminars in Radiation Oncology
  DOI: https://doi.org/10.1016/j.semradonc.2019.05.002
  JCS_FACTOR: 5.934
  Keywords: empty
  SCI_FACTOR: 1.761
  TITLE_UPPER: SEMINARS IN RADIATION ONCOLOGY
  Title: 'Big Data in Cancer Research: Real-World Resources for Precision Oncology
    to Improve Cancer Care Delivery'
  Title_JCS: SEMINARS IN RADIATION ONCOLOGY
  Title_SCI: Seminars in Radiation Oncology
  Type Publication: article
  Year: 2019
- Abstract: This study examines the antecedents and influence of big data decision-making
    capabilities on decision-making quality among Chinese firms. We propose that such
    capabilities are influenced by big data management challenges such as leadership,
    talent management, technology, and organisational culture. By using primary data
    from 108 Chinese firms and utilising partial least squares, we tested the antecedents
    of big data decision-making capability and its impact on decision-making quality.
    Findings suggest that big data management challenges are the key antecedents of
    big data decision-making capability. Furthermore, the latter is vital for big
    data decision-making quality.
  Author: Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan
  Book Title/Journal: Information & Management
  DOI: https://doi.org/10.1016/j.im.2018.12.003
  JCS_FACTOR: 7.555
  Keywords: Big data management, Dynamic capabilities, Big data decision-making capability,
    Decision-making quality, China
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFORMATION & MANAGEMENT
  Title: 'Role of big data management in enhancing big data decision-making capability
    and quality among Chinese firms: A dynamic capabilities view'
  Title_JCS: INFORMATION & MANAGEMENT
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: This study examines the antecedents and influence of big data decision-making
    capabilities on decision-making quality among Chinese firms. We propose that such
    capabilities are influenced by big data management challenges such as leadership,
    talent management, technology, and organisational culture. By using primary data
    from 108 Chinese firms and utilising partial least squares, we tested the antecedents
    of big data decision-making capability and its impact on decision-making quality.
    Findings suggest that big data management challenges are the key antecedents of
    big data decision-making capability. Furthermore, the latter is vital for big
    data decision-making quality.
  Author: Saqib Shamim and Jing Zeng and Syed Muhammad Shariq and Zaheer Khan
  Book Title/Journal: Information & Management
  DOI: https://doi.org/10.1016/j.im.2018.12.003
  JCS_FACTOR: 7.555
  Keywords: Big data management, Dynamic capabilities, Big data decision-making capability,
    Decision-making quality, China
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFORMATION & MANAGEMENT
  Title: 'Role of big data management in enhancing big data decision-making capability
    and quality among Chinese firms: A dynamic capabilities view'
  Title_JCS: INFORMATION & MANAGEMENT
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Prior research articulated the importance of developing a big data analytics
    capability but did not show how to cultivate this development. Drawing on the
    literature on this topic, this study develops the concept of Big Data capability,
    which enhances our understanding of Big Data practice beyond that captured in
    previous literature on the concept of big data analytics capability. This study
    further highlights the strategic implications of the concept by testing its relationship
    to three strategic orientations and one aspect of organizational culture. Findings
    show that customer, entrepreneurial, and technology orientations, and developmental
    culture are important contributors to the development of Big Data capability.
  Author: Canchu Lin and Anand Kunnathur
  Book Title/Journal: Journal of Business Research
  DOI: https://doi.org/10.1016/j.jbusres.2019.07.016
  JCS_FACTOR: 7.55
  Keywords: Big data capability, Customer orientation, Entrepreneurial orientation,
    Technology orientation, And developmental culture
  SCI_FACTOR: 2.049
  TITLE_UPPER: JOURNAL OF BUSINESS RESEARCH
  Title: Strategic orientations, developmental culture, and big data capability
  Title_JCS: JOURNAL OF BUSINESS RESEARCH
  Title_SCI: Journal of Business Research
  Type Publication: article
  Year: 2019
- Abstract: 'In any knowledge discovery process the value of extracted knowledge is
    directly related to the quality of the data used. Big Data problems, generated
    by massive growth in the scale of data observed in recent years, also follow the
    same dictate. A common problem affecting data quality is the presence of noise,
    particularly in classification problems, where label noise refers to the incorrect
    labeling of training instances, and is known to be a very disruptive feature of
    data. However, in this Big Data era, the massive growth in the scale of the data
    poses a challenge to traditional proposals created to tackle noise, as they have
    difficulties coping with such a large amount of data. New algorithms need to be
    proposed to treat the noise in Big Data problems, providing high quality and clean
    data, also known as Smart Data. In this paper, two Big Data preprocessing approaches
    to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous
    ensemble filter, with special emphasis in their scalability and performance traits.
    The obtained results show that these proposals enable the practitioner to efficiently
    obtain a Smart Dataset from any Big Data classification problem.'
  Author: "Diego Garc\xC3\xADa-Gil and Juli\xC3\xA1n Luengo and Salvador Garc\xC3\xAD\
    a and Francisco Herrera"
  Book Title/Journal: Information Sciences
  DOI: https://doi.org/10.1016/j.ins.2018.12.002
  JCS_FACTOR: 6.795
  Keywords: Big Data, Smart Data, Classification, Class noise, Label noise.
  SCI_FACTOR: 1.524
  TITLE_UPPER: INFORMATION SCIENCES
  Title: 'Enabling Smart Data: Noise filtering in Big Data classification'
  Title_JCS: INFORMATION SCIENCES
  Title_SCI: Information Sciences
  Type Publication: article
  Year: 2019
- Abstract: "Big data adoption is a process through which businesses find innovative\
    \ ways to enhance productivity and predict risk to satisfy customers need more\
    \ efficiently. Despite the increase in demand and importance of big data adoption,\
    \ there is still a lack of comprehensive review and classification of the existing\
    \ studies in this area. This research aims to gain a comprehensive understanding\
    \ of the current state-of-the-art by highlighting theoretical models, the influence\
    \ factors, and the research challenges of big data adoption. By adopting a systematic\
    \ selection process, twenty studies were identified in the domain of big data\
    \ adoption and were reviewed in order to extract relevant information that answers\
    \ a set of research questions. According to the findings, Technology\xE2\u20AC\
    \u201COrganization\xE2\u20AC\u201CEnvironment and Diffusion of Innovations are\
    \ the most popular theoretical models used for big data adoption in various domains.\
    \ This research also revealed forty-two factors in technology, organization, environment,\
    \ and innovation that have a significant influence on big data adoption. Finally,\
    \ challenges found in the current research about big data adoption are represented,\
    \ and future research directions are recommended. This study is helpful for researchers\
    \ and stakeholders to take initiatives that will alleviate the challenges and\
    \ facilitate big data adoption in various fields."
  Author: Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi
  Book Title/Journal: Information Processing & Management
  DOI: https://doi.org/10.1016/j.ipm.2019.102095
  JCS_FACTOR: 6.222
  Keywords: "Big data adoption, Technology\xE2\u20AC\u201COrganization\xE2\u20AC\u201C\
    Environment, Diffusion of Innovations"
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFORMATION PROCESSING & MANAGEMENT
  Title: 'Big data adoption: State of the art and research challenges'
  Title_JCS: INFORMATION PROCESSING & MANAGEMENT
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Big data adoption is a process through which businesses find innovative\
    \ ways to enhance productivity and predict risk to satisfy customers need more\
    \ efficiently. Despite the increase in demand and importance of big data adoption,\
    \ there is still a lack of comprehensive review and classification of the existing\
    \ studies in this area. This research aims to gain a comprehensive understanding\
    \ of the current state-of-the-art by highlighting theoretical models, the influence\
    \ factors, and the research challenges of big data adoption. By adopting a systematic\
    \ selection process, twenty studies were identified in the domain of big data\
    \ adoption and were reviewed in order to extract relevant information that answers\
    \ a set of research questions. According to the findings, Technology\xE2\u20AC\
    \u201COrganization\xE2\u20AC\u201CEnvironment and Diffusion of Innovations are\
    \ the most popular theoretical models used for big data adoption in various domains.\
    \ This research also revealed forty-two factors in technology, organization, environment,\
    \ and innovation that have a significant influence on big data adoption. Finally,\
    \ challenges found in the current research about big data adoption are represented,\
    \ and future research directions are recommended. This study is helpful for researchers\
    \ and stakeholders to take initiatives that will alleviate the challenges and\
    \ facilitate big data adoption in various fields."
  Author: Maria Ijaz Baig and Liyana Shuib and Elaheh Yadegaridehkordi
  Book Title/Journal: Information Processing & Management
  DOI: https://doi.org/10.1016/j.ipm.2019.102095
  JCS_FACTOR: 6.222
  Keywords: "Big data adoption, Technology\xE2\u20AC\u201COrganization\xE2\u20AC\u201C\
    Environment, Diffusion of Innovations"
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFORMATION PROCESSING & MANAGEMENT
  Title: 'Big data adoption: State of the art and research challenges'
  Title_JCS: INFORMATION PROCESSING & MANAGEMENT
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Analysis of public transportation data in large cities is a challenging\
    \ problem. Managing data ingestion, data storage, data quality enhancement, modelling\
    \ and analysis requires intensive computing and a non-trivial amount of resources.\
    \ In EUBra-BIGSEA (Europe\xE2\u20AC\u201CBrazil Collaboration of Big Data Scientific\
    \ Research Through Cloud-Centric Applications) we address such problems in a comprehensive\
    \ and integrated way. EUBra-BIGSEA provides a platform for building up data analytic\
    \ workflows on top of elastic cloud services without requiring skills related\
    \ to either programming or cloud services. The approach combines cloud orchestration,\
    \ Quality of Service and automatic parallelisation on a platform that includes\
    \ a toolbox for implementing privacy guarantees and data quality enhancement as\
    \ well as advanced services for sentiment analysis, traffic jam estimation and\
    \ trip recommendation based on estimated crowdedness. All developments are available\
    \ under Open Source licenses (http://github.org/eubr-bigsea, https://hub.docker.com/u/eubrabigsea/)."
  Author: "Andy S. Alic and Jussara Almeida and Giovanni Aloisio and Nazareno Andrade\
    \ and Nuno Antunes and Danilo Ardagna and Rosa M. Badia and Tania Basso and Ignacio\
    \ Blanquer and Tarciso Braz and Andrey Brito and Donatello Elia and Sandro Fiore\
    \ and Dorgival Guedes and Marco Lattuada and Daniele Lezzi and Matheus Maciel\
    \ and Wagner Meira and Demetrio Mestre and Regina Moraes and Fabio Morais and\
    \ Carlos Eduardo Pires and N\xC3\xA1dia P. Kozievitch and Walter dos Santos and\
    \ Paulo Silva and Marco Vieira"
  Book Title/Journal: Future Generation Computer Systems
  DOI: https://doi.org/10.1016/j.future.2019.02.011
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 1.262
  TITLE_UPPER: FUTURE GENERATION COMPUTER SYSTEMS
  Title: 'BIGSEA: A Big Data analytics platform for public transportation information'
  Title_JCS: N/A
  Title_SCI: Future Generation Computer Systems
  Type Publication: article
  Year: 2019
- Abstract: There are numerous emerging studies addressing big data and its application
    in different organizational aspects, especially regarding its impact on the business
    innovation process. This study in particular aims at analyzing the existing relationship
    between Big Data Analytics Capabilities and Co-innovation. To test the hypothesis
    model, structural equations by the partial least squares method were used in a
    sample of 112 Colombian firms. The main findings allow to positively relate Big
    Data Analytics Capabilities with better and more agile processes of product and
    service co-creation and with more robust collaboration networks with stakeholders
    internal and external to the firm.
  Author: "Nelson Lozada and Jose Arias-P\xC3\xA9rez and Geovanny Perdomo-Charry"
  Book Title/Journal: Heliyon
  DOI: https://doi.org/10.1016/j.heliyon.2019.e02541
  JCS_FACTOR: 0.0
  Keywords: Business, Economics, Information science, Big data analytics capabilities,
    Co-innovation, Big data, Co-creation
  SCI_FACTOR: 0.455
  TITLE_UPPER: HELIYON
  Title: 'Big data analytics capability and co-innovation: An empirical study'
  Title_JCS: N/A
  Title_SCI: Heliyon
  Type Publication: article
  Year: 2019
- Abstract: 'Under rapid urbanization, cities are facing many societal challenges
    that impede sustainability. Big data analytics (BDA) gives cities unprecedented
    potential to address these issues. As BDA is still a new concept, there is limited
    knowledge on how to apply BDA in a sustainability context. Thus, this study investigates
    a case using BDA for sustainability, adopting the resource orchestration perspective.
    A process model is generated, which provides novel insights into three aspects:
    data resource orchestration, BDA capability development, and big data value creation.
    This study benefits both researchers and practitioners by contributing to theoretical
    developments as well as by providing practical insights.'
  Author: Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu
  Book Title/Journal: Information & Management
  DOI: https://doi.org/10.1016/j.im.2019.103231
  JCS_FACTOR: 7.555
  Keywords: Big data, Big data analytics, Sustainability, Air pollution, Resource
    orchestration
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFORMATION & MANAGEMENT
  Title: 'Orchestrating big data analytics capability for sustainability: A study
    of air pollution management in China'
  Title_JCS: INFORMATION & MANAGEMENT
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: 'Under rapid urbanization, cities are facing many societal challenges
    that impede sustainability. Big data analytics (BDA) gives cities unprecedented
    potential to address these issues. As BDA is still a new concept, there is limited
    knowledge on how to apply BDA in a sustainability context. Thus, this study investigates
    a case using BDA for sustainability, adopting the resource orchestration perspective.
    A process model is generated, which provides novel insights into three aspects:
    data resource orchestration, BDA capability development, and big data value creation.
    This study benefits both researchers and practitioners by contributing to theoretical
    developments as well as by providing practical insights.'
  Author: Dan Zhang and Shan L. Pan and Jiaxin Yu and Wenyuan Liu
  Book Title/Journal: Information & Management
  DOI: https://doi.org/10.1016/j.im.2019.103231
  JCS_FACTOR: 7.555
  Keywords: Big data, Big data analytics, Sustainability, Air pollution, Resource
    orchestration
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFORMATION & MANAGEMENT
  Title: 'Orchestrating big data analytics capability for sustainability: A study
    of air pollution management in China'
  Title_JCS: INFORMATION & MANAGEMENT
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Big data represent a pioneering development in the field of agriculture.\
    \ By producing intuition, intelligence, and insights, these data have the potential\
    \ to recast conventional process-driven agriculture, plotting the course for a\
    \ smarter, data-driven farming. However, many open issues about the use of big\
    \ data in agriculture remain unanswered. In this work, conceptualizing smart agricultural\
    \ systems as cyber-physical-social systems, and building upon activity theory,\
    \ we aim at highlighting some key questions that need to be addressed. To our\
    \ view, big data constitute a tool reciprocally produced by all the actors involved\
    \ in the agrifood supply chains. The constant flux of this tool and the intricate\
    \ nature of the interactions among the actors who share it complicate the translation\
    \ of big data into value. Moreover, farmers\xE2\u20AC\u2122 limited capacity to\
    \ deal with data complexity, along with their dual role as producers and users\
    \ of big data, impedes the institutionalization of this tool at the farm level.\
    \ Although the approach used left us with more questions than answers, we suggest\
    \ that unraveling the institutional arrangements that govern value co-creation,\
    \ capturing the motivations of farmers and other actors, and detailing the direct\
    \ and indirect effects that big data (and the technologies used to generate them)\
    \ have in farms are important preconditions for setting forth rules that facilitate\
    \ the extraction and equal exchange of value from big data."
  Author: Evagelos D. Lioutas and Chrysanthi Charatsari and Giuseppe {La Rocca} and
    Marcello {De Rosa}
  Book Title/Journal: NJAS - Wageningen Journal of Life Sciences
  DOI: https://doi.org/10.1016/j.njas.2019.04.003
  JCS_FACTOR: 0.0
  Keywords: Big data, Smart farming, Value, Farmers, Cyber-physical-social systems,
    Activity theory
  SCI_FACTOR: 1.023
  TITLE_UPPER: NJAS - WAGENINGEN JOURNAL OF LIFE SCIENCES
  Title: 'Key questions on the use of big data in farming: An activity theory approach'
  Title_JCS: N/A
  Title_SCI: NJAS - Wageningen Journal of Life Sciences
  Type Publication: article
  Year: 2019
- Abstract: 'The field of functional neuroimaging has substantially advanced as a
    big data science in the past decade, thanks to international collaborative projects
    and community efforts. Here we conducted a literature review on functional neuroimaging,
    with focus on three general challenges in big data tasks: data collection and
    sharing, data infrastructure construction, and data analysis methods. The review
    covers a wide range of literature types including perspectives, database descriptions,
    methodology developments, and technical details. We show how each of the challenges
    was proposed and addressed, and how these solutions formed the three core foundations
    for the functional neuroimaging as a big data science and helped to build the
    current data-rich and data-driven community. Furthermore, based on our review
    of recent literature on the upcoming challenges and opportunities toward future
    scientific discoveries, we envisioned that the functional neuroimaging community
    needs to advance from the current foundations to better data integration infrastructure,
    methodology development toward improved learning capability, and multi-discipline
    translational research framework for this new era of big data.'
  Author: Xiang Li and Ning Guo and Quanzheng Li
  Book Title/Journal: Genomics, Proteomics & Bioinformatics
  DOI: https://doi.org/10.1016/j.gpb.2018.11.005
  JCS_FACTOR: 0.0
  Keywords: Big data, Neuroimaging, Machine learning, Health informatics, fMRI
  SCI_FACTOR: 0.0
  TITLE_UPPER: GENOMICS, PROTEOMICS & BIOINFORMATICS
  Title: Functional Neuroimaging in the New Era of Big Data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The application of big data to the quality assurance of radiation therapy
    is multifaceted. Big data can be used to detect anomalies and suboptimal quality
    metrics through both statistical means and more advanced machine learning and
    artificial intelligence. The application of these methods to clinical practice
    is discussed through examples of guideline adherence, contour integrity, treatment
    delivery mechanics, and treatment plan quality. The ultimate goal is to apply
    big data methods to direct measures of patient outcomes for care quality. The
    era of big data and machine learning is maturing and the implementation for quality
    assurance promises to improve the quality of care for patients.
  Author: Todd R. McNutt and Kevin L. Moore and Binbin Wu and Jean L. Wright
  Book Title/Journal: Seminars in Radiation Oncology
  DOI: https://doi.org/10.1016/j.semradonc.2019.05.006
  JCS_FACTOR: 5.934
  Keywords: empty
  SCI_FACTOR: 1.761
  TITLE_UPPER: SEMINARS IN RADIATION ONCOLOGY
  Title: Use of Big Data for Quality Assurance in Radiation Therapy
  Title_JCS: SEMINARS IN RADIATION ONCOLOGY
  Title_SCI: Seminars in Radiation Oncology
  Type Publication: article
  Year: 2019
- Abstract: Grounded in gestalt insight learning theory and organizational learning
    theory, we collected data from 280 middle and top-level managers to investigate
    the impact of each big data characteristic (i.e., data volume, data velocity,
    data variety, and data veracity) on firm innovation competency (i.e., exploitation
    competency and exploration competency), mediated through data-driven insight generation
    (i.e., descriptive insight, predictive insight, and prescriptive insight). Findings
    show that while data velocity, variety, and veracity enhance data-driven insight
    generation, data volume does not impact it. Additionally, results of the post
    hoc analysis indicate that while descriptive and predictive insights improve innovation
    competency, prescriptive insight does not affect it. These results provide interesting
    and unique theoretical and practical insights.
  Author: Maryam Ghasemaghaei and Goran Calic
  Book Title/Journal: Journal of Business Research
  DOI: https://doi.org/10.1016/j.jbusres.2019.07.006
  JCS_FACTOR: 7.55
  Keywords: Big data characteristics, Descriptive insight, Predictive insight, Prescriptive
    insight, Innovation competency
  SCI_FACTOR: 2.049
  TITLE_UPPER: JOURNAL OF BUSINESS RESEARCH
  Title: Does big data enhance firm innovation competency? The mediating role of data-driven
    insights
  Title_JCS: JOURNAL OF BUSINESS RESEARCH
  Title_SCI: Journal of Business Research
  Type Publication: article
  Year: 2019
- Abstract: 'Introduction

    Machine learning capability holds promise to inform disease models, the discovery
    and development of novel disease modifying therapeutics and prevention strategies
    in psychiatry. Herein, we provide an introduction on how machine learning/Artificial
    Intelligence (AI) may instantiate such capabilities, as well as provide rationale
    for its application to psychiatry in both research and clinical ecosystems.

    Methods

    Databases PubMed and PsycINFO were searched from 1966 to June 2016 for keywords:Big
    Data, Machine Learning, Precision Medicine, Artificial Intelligence, Mental Health,
    Mental Disease, Psychiatry, Data Mining, RDoC, and Research Domain Criteria. Articles
    selected for review were those that were determined to be aligned with the objective
    of this particular paper.

    Results

    Results indicate that AI is a viable option to build useful predictors of outcome
    while offering objective and comparable accuracy metrics, a unique opportunity,
    particularly in mental health research. The approach has also consistently brought
    notable insight into disease models through processing the vast amount of already
    available multi-domain, semi-structured medical data. The opportunity for AI in
    psychiatry, in addition to disease-model refinement, is in characterizing those
    at risk, and it is likely also relevant to personalizing and discovering therapeutics.

    Conclusions

    Machine learning currently provides an opportunity to parse disease models in
    complex, multi-factorial disease states (e.g. mental disorders) and could possibly
    inform treatment selection with existing therapies and provide bases for domain-based
    therapeutic discovery.'
  Author: Andy M.Y. Tai and Alcides Albuquerque and Nicole E. Carmona and Mehala Subramanieapillai
    and Danielle S. Cha and Margarita Sheko and Yena Lee and Rodrigo Mansur and Roger
    S. McIntyre
  Book Title/Journal: Artificial Intelligence in Medicine
  DOI: https://doi.org/10.1016/j.artmed.2019.101704
  JCS_FACTOR: 5.326
  Keywords: Big data, Machine learning, Precision medicine, AI, Mental health, Mental
    disease, Psychiatry, Data mining, RDoC, Research domain criteria, DSM-5. Schizophrenia,
    ADHD, Alzheimer, Depression, fMRI, MRI, Algorithms, IBM Watson, Neuro networking,
    Random forests, Decision trees, Support vector machines
  SCI_FACTOR: 0.98
  TITLE_UPPER: ARTIFICIAL INTELLIGENCE IN MEDICINE
  Title: 'Machine learning and big data: Implications for disease modeling and therapeutic
    discovery in psychiatry'
  Title_JCS: ARTIFICIAL INTELLIGENCE IN MEDICINE
  Title_SCI: Artificial Intelligence in Medicine
  Type Publication: article
  Year: 2019
- Abstract: Solar power is expected to play a substantial role globally, due to it
    being one of the leading renewable electricity sources for future use. Even though
    the use of solar irradiation to generate electricity is currently at a fast deployment
    pace and technological evolution, its natural variability still presents an important
    barrier to overcome. Machine learning and data mining techniques arise as alternatives
    to aid solar electricity generation forecast reducing the impacts of its natural
    inconstant power supply. This paper presents a literature review on big data models
    for solar photovoltaic electricity generation forecasts, aiming to evaluate the
    most applicable and accurate state-of-art techniques to the problem, including
    the motivation behind each project proposal, the characteristics and quality of
    data used to address the problem, among other issues. A Systematic Literature
    Review (SLR) method was used, in which research questions were defined and translated
    into search strings. The search returned 38 papers for final evaluation, affirming
    that the use of these models to predict solar electricity generation is currently
    an ongoing academic research question. Machine learning is widely used, and neural
    networks is considered the most accurate algorithm. Extreme learning machine learning
    has reduced time and raised precision.
  Author: Gabriel {de Freitas Viscondi} and Solange N. Alves-Souza
  Book Title/Journal: Sustainable Energy Technologies and Assessments
  DOI: https://doi.org/10.1016/j.seta.2018.11.008
  JCS_FACTOR: 5.353
  Keywords: Systematic Literature Review, Solar energy forecasting, Machine learning,
    Data mining
  SCI_FACTOR: 1.04
  TITLE_UPPER: SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS
  Title: A Systematic Literature Review on big data for solar photovoltaic electricity
    generation forecasting
  Title_JCS: Sustainable Energy Technologies and Assessments
  Title_SCI: Sustainable Energy Technologies and Assessments
  Type Publication: article
  Year: 2019
- Abstract: "Extracting knowledge from Big Data is the process of transforming this\
    \ data into actionable information. The exponential growth of data has initiated\
    \ a myriad of new opportunities, and made data become the most valuable raw material\
    \ of production for many organizations. Mining Big Data is coupled with some challenges,\
    \ known as the 3V\xE2\u20AC\u2122s of Big Data: Volume, Variety and Velocity.\
    \ However, a major challenge that needs to be addressed, and often is ignored\
    \ in the literature, concerns reliability. Actually, data is agglomerated from\
    \ multiple disparate sources, and each of Knowledge Discovery (KDD) process steps\
    \ may be carried out by different organizations. These considerations lead us\
    \ to ask a critical question that is weather the information we have at each step\
    \ is reliable enough to proceed to the next one? This paper therefore aims to\
    \ provide a framework that automatically assesses reliability of the knowledge\
    \ discovery process. We focus on Linked Open Data (LOD) as a source of data, as\
    \ it constitutes a relevant data provider in many Big Data applications. However,\
    \ our framework can also be adapted for unstructured data. This framework will\
    \ assist scientists to automatically and efficiently measure the reliability of\
    \ each KDD process stage as well as detect unreliable steps that should be revised.\
    \ Following this methodology, KDD process will be optimized and therefore produce\
    \ knowledge with higher quality."
  Author: Hicham Moad Safhi and Bouchra Frikh and Brahim Ouhbi
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.01.005
  JCS_FACTOR: 0.0
  Keywords: Knowledge discovery, Reliability, Trustworthiness, Quality, Big Data mining
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: Assessing reliability of Big Data Knowledge Discovery process
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: 'Recently, big data (BD) has attracted researchers and practitioners due
    to its potential usefulness in decision-making processes. Big data analytics (BDA)
    is becoming increasingly popular among manufacturing companies as it helps gain
    insights and make decisions based on BD. However, there many barriers to the adoption
    of BDA in manufacturing supply chains. It is therefore necessary for manufacturing
    companies to identify and examine the nature of each barrier. Previous studies
    have mostly built conceptual frameworks for BDA in a given situation and have
    ignored examining the nature of the barriers to BDA. Due to the significance of
    both BD and BDA, this research aims to identify and examine the critical barriers
    to the adoption of BDA in manufacturing supply chains in the context of Bangladesh.
    This research explores the existing body of knowledge by examining these barriers
    using a Delphi-based analytic hierarchy process (AHP). Data were obtained from
    five Bangladeshi manufacturing companies. The findings of this research are as
    follows: (i) data-related barriers are most important, (ii) technology-related
    barriers are second, and (iii) the five most important components of these barriers
    are (a) lack of infrastructure, (b) complexity of data integration, (c) data privacy,
    (d) lack of availability of BDA tools and (e) high cost of investment. The findings
    can assist industrial managers to understand the actual nature of the barriers
    and potential benefits of using BDA and to make policy regarding BDA adoption
    in manufacturing supply chains. A sensitivity analysis was carried out to justify
    the robustness of the barrier rankings.'
  Author: Md. Abdul Moktadir and Syed Mithun Ali and Sanjoy Kumar Paul and Nagesh
    Shukla
  Book Title/Journal: Computers & Industrial Engineering
  DOI: https://doi.org/10.1016/j.cie.2018.04.013
  JCS_FACTOR: 5.431
  Keywords: AHP, Big data analytics, Barriers to BDA, Delphi, Information and communication
    technology (ICT), Manufacturing supply chains
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMPUTERS & INDUSTRIAL ENGINEERING
  Title: 'Barriers to big data analytics in manufacturing supply chains: A case study
    from Bangladesh'
  Title_JCS: COMPUTERS & INDUSTRIAL ENGINEERING
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The business concept of the circular economy (CE) has gained significant
    momentum among practitioners and researchers alike. However, successful adoption
    and implementation of this paradigm of managing business remains a challenge.
    In this article, we build a case for utilizing big data analytics (BDA) as a fundamental
    basis for informed and data driven decision making in supply chain networks supporting
    CE. We view this from a stakeholder perspective and argue that a collaborative
    association among all supply chain members can positively affect CE implementation.
    We propose a model highlighting the facilitating role of big data analytics for
    achieving shared sustainability goals. The model is based on integrating thematic
    categories coming out of 10 semi-structured interviews with key position holders
    in industry. We argue that mutual support and coordination driven by a stakeholder
    perspective coupled with holistic information processing and sharing along the
    entire supply chain network can effectively create a basis for achieving the triple
    bottom line of economic, ecological and social benefits. The proposed model is
    useful for managers in that it provides a reference point for aligning activities
    with the circular economy paradigm. The conceptual model provides a theoretical
    basis for future empirical research in this domain.
  Author: "Shivam Gupta and Haozhe Chen and Benjamin T. Hazen and Sarabjot Kaur and\
    \ Ernesto D.R. {Santiba\xC3\xB1ez Gonzalez}"
  Book Title/Journal: Technological Forecasting and Social Change
  DOI: https://doi.org/10.1016/j.techfore.2018.06.030
  JCS_FACTOR: 8.593
  Keywords: Circular economy, Big data, Stakeholder theory, Relational view, Supply
    chain management, Sustainability
  SCI_FACTOR: 2.226
  TITLE_UPPER: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title: 'Circular economy and big data analytics: A stakeholder perspective'
  Title_JCS: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title_SCI: Technological Forecasting and Social Change
  Type Publication: article
  Year: 2019
- Abstract: As one of the bottleneck technologies of electric vehicles (EVs), the
    battery hosts complex and hardly observable internal chemical reactions. Therefore,
    a precise mathematical model is crucial for the battery management system (BMS)
    to ensure the secure and stable operation of the battery in a multi-variable environment.
    First, a Cloud-based BMS (C-BMS) is established based on a database containing
    complete battery status information. Next, a data cleaning method based on machine
    learning is applied to the big data of batteries. Meanwhile, to improve the model
    stability under dynamic conditions, an F-divergence-based data distribution quality
    assessment method and a sampling-based data preprocess method is designed. Then,
    a lithium-ion battery temperature-dependent model is built based on Stacked Denoising
    Autoencoders- Extreme Learning Machine (SDAE-ELM) algorithm, and a new training
    method combined with data preprocessing is also proposed to improve the model
    accuracy. Finally, to improve reliability, a conjunction working mode between
    the C-BMS and the BMS in vehicles (V-BMS) is also proposed, providing as an applied
    case of the model. Using the battery data extracted from electric buses, the effectiveness
    and accuracy of the model are validated. The error of the estimated battery terminal
    voltage is within 2%, and the error of the estimated State of Charge (SoC) is
    within 3%.
  Author: Shuangqi Li and Hongwen He and Jianwei Li
  Book Title/Journal: Applied Energy
  DOI: https://doi.org/10.1016/j.apenergy.2019.03.154
  JCS_FACTOR: 9.746
  Keywords: Electric vehicles, Battery energy storage, Temperature-dependent model,
    Battery management system, Big data, Deep learning
  SCI_FACTOR: 3.035
  TITLE_UPPER: APPLIED ENERGY
  Title: Big data driven lithium-ion battery modeling method based on SDAE-ELM algorithm
    and data pre-processing technology
  Title_JCS: APPLIED ENERGY
  Title_SCI: Applied Energy
  Type Publication: article
  Year: 2019
- Abstract: "Although big data analytics (BDA) is considered the next \xE2\u20AC\u0153\
    frontier\xE2\u20AC\x9D in data science by creating potential business opportunities,\
    \ the way to extract those opportunities is unclear. This paper aims to understand\
    \ the antecedents of BDA value at a firm level. The authors performed a study\
    \ using a mixed methodology approach. First, by carrying out a Delphi study to\
    \ explore and rank the antecedents affecting the creation of BDA value. Based\
    \ on the Delphi results, we propose an empirically validated model supported by\
    \ a survey conducted on 175 European firms to explain the antecedents of BDA sustained\
    \ value. The results show that the proposed model explains 62% of BDA sustained\
    \ value at the firm level, where the most critical contributor is BDA use. We\
    \ provide directions for managers to support their decisions on BDA strategy definition\
    \ and refinement. For academics, we extend BDA value literature and outline some\
    \ potential research opportunities."
  Author: "Nadine C\xC3\xB4rte-Real and Pedro Ruivo and Tiago Oliveira and Ale\xC5\
    \xA1 Popovi\xC4\x8D"
  Book Title/Journal: Journal of Business Research
  DOI: https://doi.org/10.1016/j.jbusres.2018.12.072
  JCS_FACTOR: 7.55
  Keywords: IT business value, Big data analytics (BDA), Delphi method, Mixed methodology,
    Competitive advantage
  SCI_FACTOR: 2.049
  TITLE_UPPER: JOURNAL OF BUSINESS RESEARCH
  Title: Unlocking the drivers of big data analytics value in firms
  Title_JCS: JOURNAL OF BUSINESS RESEARCH
  Title_SCI: Journal of Business Research
  Type Publication: article
  Year: 2019
- Abstract: "Data quality assessment has gained attention in the recent years since\
    \ more and more companies and medical centers are highlighting the importance\
    \ of an automated framework to effectively manage the quality of their big data.\
    \ Data cleaning, also known as data curation, lies in the heart of the data quality\
    \ assessment and is a key aspect prior to the development of any data analytics\
    \ services. In this work, we present the objectives, functionalities and methodological\
    \ advances of an automated framework for data curation from a medical perspective.\
    \ The steps towards the development of a system for data quality assessment are\
    \ first described along with multidisciplinary data quality measures. A three-layer\
    \ architecture which realizes these steps is then presented. Emphasis is given\
    \ on the detection and tracking of inconsistencies, missing values, outliers,\
    \ and similarities, as well as, on data standardization to finally enable data\
    \ harmonization. A case study is conducted in order to demonstrate the applicability\
    \ and reliability of the proposed framework on two well-established cohorts with\
    \ clinical data related to the primary Sj\xC3\xB6gren's Syndrome (pSS). Our results\
    \ confirm the validity of the proposed framework towards the automated and fast\
    \ identification of outliers, inconsistencies, and highly-correlated and duplicated\
    \ terms, as well as, the successful matching of more than 85% of the pSS-related\
    \ medical terms in both cohorts, yielding more accurate, relevant, and consistent\
    \ clinical data."
  Author: Vasileios C. Pezoulas and Konstantina D. Kourou and Fanis Kalatzis and Themis
    P. Exarchos and Aliki Venetsanopoulou and Evi Zampeli and Saviana Gandolfo and
    Fotini Skopouli and Salvatore {De Vita} and Athanasios G. Tzioufas and Dimitrios
    I. Fotiadis
  Book Title/Journal: Computers in Biology and Medicine
  DOI: https://doi.org/10.1016/j.compbiomed.2019.03.001
  JCS_FACTOR: 4.589
  Keywords: Big data, Data quality, Data quality assessment, Data curation, Data standardization
  SCI_FACTOR: 0.884
  TITLE_UPPER: COMPUTERS IN BIOLOGY AND MEDICINE
  Title: 'Medical data quality assessment: On the development of an automated framework
    for medical data curation'
  Title_JCS: COMPUTERS IN BIOLOGY AND MEDICINE
  Title_SCI: Computers in Biology and Medicine
  Type Publication: article
  Year: 2019
- Abstract: Smart manufacturing has received increased attention from academia and
    industry in recent years, as it provides competitive advantage for manufacturing
    companies making industry more efficient and sustainable. As one of the most important
    technologies for smart manufacturing, big data analytics can uncover hidden knowledge
    and other useful information like relations between lifecycle decisions and process
    parameters helping industrial leaders to make more-informed business decisions
    in complex management environments. However, according to the literature, big
    data analytics and smart manufacturing were individually researched in academia
    and industry. To provide theoretical foundations for the research community to
    further develop scientific insights in applying big data analytics to smart manufacturing,
    it is necessary to summarize the existing research progress and weakness. In this
    paper, through combining the key technologies of smart manufacturing and the idea
    of ubiquitous servitization in the whole lifecycle, the term of sustainable smart
    manufacturing was coined. A comprehensive overview of big data in smart manufacturing
    was conducted, and a conceptual framework was proposed from the perspective of
    product lifecycle. The proposed framework allows analyzing potential applications
    and key advantages, and the discussion of current challenges and future research
    directions provides valuable insights for academia and industry.
  Author: Shan Ren and Yingfeng Zhang and Yang Liu and Tomohiko Sakao and Donald Huisingh
    and Cecilia M.V.B. Almeida
  Book Title/Journal: Journal of Cleaner Production
  DOI: https://doi.org/10.1016/j.jclepro.2018.11.025
  JCS_FACTOR: 9.297
  Keywords: Big data analytics, Smart manufacturing, Servitization, Sustainable production,
    Conceptual framework, Product lifecycle
  SCI_FACTOR: 1.937
  TITLE_UPPER: JOURNAL OF CLEANER PRODUCTION
  Title: 'A comprehensive review of big data analytics throughout product lifecycle
    to support sustainable smart manufacturing: A framework, challenges and future
    research directions'
  Title_JCS: Journal of Cleaner Production
  Title_SCI: Journal of Cleaner Production
  Type Publication: article
  Year: 2019
- Abstract: "Operating modern multi-modal surface transportation systems are becoming\
    \ increasingly automated and driven by decision support systems. One aspect necessary\
    \ for successful, safe, reliable, and efficient operation of any transportation\
    \ network is real-time and forecasted weather and pavement condition information.\
    \ Providing such information requires an adaptive system capable of blending large\
    \ amounts of observational and model data that arrives quickly, in disparate formats\
    \ and times, and blends and optimizes their use via expert systems and machine-learning\
    \ algorithms. Quality control of the data is also essential, and historical data\
    \ is required to both develop expert-based empirical algorithms and train machine\
    \ learning models. This paper reports on the open-source Pikalert\xC2\xAE system\
    \ that brings together weather information and real-time data from connected vehicles\
    \ to provide crucial information to enhance the safety and efficiency of surface\
    \ transportation systems. This robust framework can be applied to a diverse array\
    \ of user community specifications and is designed to rapidly ingest more, unique\
    \ data sets as they become available. Ultimately, the developmental framework\
    \ of this system will provide critical environmental information necessary to\
    \ promote the development, growth, refinement, and expanded adoption of automated\
    \ and connected multi-modal vehicular systems globally."
  Author: Amanda R. Siems-Anderson and Curtis L. Walker and Gerry Wiener and William
    P. Mahoney and Sue Ellen Haupt
  Book Title/Journal: Transportation Research Interdisciplinary Perspectives
  DOI: https://doi.org/10.1016/j.trip.2019.100071
  JCS_FACTOR: 0.0
  Keywords: Big data, Pikalert, Road weather, Surface transportation, Pavement condition,
    Weather forecasts
  SCI_FACTOR: 0.383
  TITLE_UPPER: TRANSPORTATION RESEARCH INTERDISCIPLINARY PERSPECTIVES
  Title: An adaptive big data weather system for surface transportation
  Title_JCS: N/A
  Title_SCI: Transportation Research Interdisciplinary Perspectives
  Type Publication: article
  Year: 2019
- Abstract: In big data era, information integration often requires abundant data
    extracted from massive data sources. Due to a large number of data sources, data
    source selection plays a crucial role in information integration, since it is
    costly and even impossible to access all data sources. Data Source selection should
    consider both efficiency and effectiveness issues. For efficiency, the approach
    should scale to large data source amount. From effectiveness aspect, data quality
    and overlapping of sources are to be considered. In this paper, we study source
    selection problem in Big Data and propose methods which can scale to datasets
    with up to millions of data sources and guarantee the quality of results. Motivated
    by this, we propose a new metric taking the expected number of true values a source
    can provide as a criteria to evaluate the contribution of a data source. Based
    on our proposed index, we present a scalable algorithm and two pruning strategies
    to improve the efficiency without sacrificing precision. Experimental results
    on both real world and synthetic data sets show that our methods can select sources
    providing a large proportion of true values efficiently and can scale to massive
    data sources.
  Author: Yiming Lin and Hongzhi Wang and Jianzhong Li and Hong Gao
  Book Title/Journal: Information Sciences
  DOI: https://doi.org/10.1016/j.ins.2018.11.029
  JCS_FACTOR: 6.795
  Keywords: Source selection, Data integration, Data cleaning
  SCI_FACTOR: 1.524
  TITLE_UPPER: INFORMATION SCIENCES
  Title: Data source selection for information integration in big data era
  Title_JCS: INFORMATION SCIENCES
  Title_SCI: Information Sciences
  Type Publication: article
  Year: 2019
- Abstract: "Big data promises to transform public decision-making for the better\
    \ by making it more responsive to actual needs and policy effects. However, much\
    \ recent work on big data in public decision-making assumes a rational view of\
    \ decision-making, which has been much criticized in the public administration\
    \ debate. In this paper, we apply this view, and a more political one, to the\
    \ context of big data and offer a qualitative study. We question the impact of\
    \ big data on decision-making, realizing that big data \xE2\u20AC\u201C including\
    \ its new methods and functions \xE2\u20AC\u201C must inevitably encounter existing\
    \ political and managerial institutions. By studying two illustrative cases of\
    \ big data use processes, we explore how these two worlds meet. Specifically,\
    \ we look at the interaction between data analysts and decision makers. In this\
    \ we distinguish between a rational view and a political view, and between an\
    \ information logic and a decision logic. We find that big data provides ample\
    \ opportunities for both analysts and decision makers to do a better job, but\
    \ this doesn't necessarily imply better decision-making, because big data also\
    \ provides opportunities for actors to pursue their own interests. Big data enables\
    \ both data analysts and decision makers to act as autonomous agents rather than\
    \ as links in a functional chain. Therefore, big data's impact cannot be interpreted\
    \ only in terms of its functional promise; it must also be acknowledged as a phenomenon\
    \ set to impact our policymaking institutions, including their legitimacy."
  Author: H.G. {van der Voort} and A.J. Klievink and M. Arnaboldi and A.J. Meijer
  Book Title/Journal: Government Information Quarterly
  DOI: https://doi.org/10.1016/j.giq.2018.10.011
  JCS_FACTOR: 7.279
  Keywords: empty
  SCI_FACTOR: 2.121
  TITLE_UPPER: GOVERNMENT INFORMATION QUARTERLY
  Title: Rationality and politics of algorithms. Will the promise of big data survive
    the dynamics of public decision making?
  Title_JCS: GOVERNMENT INFORMATION QUARTERLY
  Title_SCI: Government Information Quarterly
  Type Publication: article
  Year: 2019
- Abstract: The Spatial Urban Data System (SUDS) is a spatial big data infrastructure
    to support UK-wide analytics of the social and economic aspects of cities and
    city-regions. It utilises data generated from traditional as well as new and emerging
    sources of urban data. The SUDS deploys geospatial technology, synthetic small
    area urban metrics, and cloud computing to enable urban analytics, and geovisualization
    with the goal of deriving actionable knowledge for better urban management and
    data-driven urban decision making. At the core of the system is a programme of
    urban indicators generated by using novel forms of data and urban modelling and
    simulation programme. SUDS differs from other similar systems by its emphasis
    on the generation and use of regularly updated spatially-activated urban area
    metrics from real or near-real time data sources, to enhance understanding of
    intra-city interactions and dynamics. By deploying public transport, labour market
    accessibility and housing advertisement data in the system, we were able to identify
    spatial variations of key urban services at intra-city levels as well as social
    and economically-marginalised output areas in major cities across the UK. This
    paper discusses the design and implementation of SUDS, the challenges and limitations
    encountered, and considerations made during its development. The innovative approach
    adopted in the design of SUDS will enable it to support research and analysis
    of urban areas, policy and city administration, business decision-making, private
    sector innovation, and public engagement. Having been tested with housing, transport
    and employment metrics, efforts are ongoing to integrate information from other
    sources such as IoT, and User Generated Content into the system to enable urban
    predictive analytics.
  Author: Obinna C.D. Anejionu and Piyushimita (Vonu) Thakuriah and Andrew McHugh
    and Yeran Sun and David McArthur and Phil Mason and Rod Walpole
  Book Title/Journal: Future Generation Computer Systems
  DOI: https://doi.org/10.1016/j.future.2019.03.052
  JCS_FACTOR: 0.0
  Keywords: Urban big data infrastructure, Urban analytics, Spatial urban indicators,
    Small area assessment, Spatial big data
  SCI_FACTOR: 1.262
  TITLE_UPPER: FUTURE GENERATION COMPUTER SYSTEMS
  Title: 'Spatial urban data system: A cloud-enabled big data infrastructure for social
    and economic urban analytics'
  Title_JCS: N/A
  Title_SCI: Future Generation Computer Systems
  Type Publication: article
  Year: 2019
- Abstract: Recently, patient safety and healthcare have gained high attention in
    professional and health policy-makers. This rapid growth causes generating a high
    amount of data, which is known as big data. Therefore, handling and processing
    of this data are attracted great attention. Cloud computing is one of the main
    choices for handling and processing of this type of data. But, as far as we know,
    the detailed review and deep discussion in this filed are very rare. Therefore,
    this paper reviews and discusses the recently introduced mechanisms in this field
    as well as providing a deep analysis of their applied mechanisms. Moreover, the
    drawbacks and benefits of the reviewed mechanisms have been discussed and the
    main challenges of these mechanisms are highlighted for developing more efficient
    healthcare big data processing techniques over cloud computing in the future.
  Author: Lila Rajabion and Abdusalam Abdulla Shaltooki and Masoud Taghikhah and Amirhossein
    Ghasemi and Arshad Badfar
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2019.05.017
  JCS_FACTOR: 14.098
  Keywords: Cloud computing, Processing, Healthcare, Big data, Review
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: 'Healthcare big data processing mechanisms: The role of cloud computing'
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Yoshiaki Ueda and Shuichi Yanagisawa
  Book Title/Journal: Molecular Plant
  DOI: https://doi.org/10.1016/j.molp.2019.01.008
  JCS_FACTOR: 13.164
  Keywords: empty
  SCI_FACTOR: 4.588
  TITLE_UPPER: MOLECULAR PLANT
  Title: 'Delineation of Nitrogen Signaling Networks: Computational Approaches in
    the Big Data Era'
  Title_JCS: Molecular Plant
  Title_SCI: Molecular Plant
  Type Publication: article
  Year: 2019
- Abstract: The landscape of mental health has undergone tremendous changes within
    the last two decades, but the research on mental health is still at the initial
    stage with substantial knowledge gaps and the lack of precise diagnosis. Nowadays,
    big data and artificial intelligence offer new opportunities for the screening
    and prediction of mental problems. In this review paper, we outline the vision
    of digital phenotyping of mental health (DPMH) by fusing the enriched data from
    ubiquitous sensors, social media and healthcare systems, and present a broad overview
    of DPMH from sensing and computing perspectives. We first conduct a systematical
    literature review and propose the research framework, which highlights the key
    aspects related with mental health, and discuss the challenges elicited by the
    enriched data for digital phenotyping. Next, five key research strands including
    affect recognition, cognitive analytics, behavioral anomaly detection, social
    analytics, and biomarker analytics are unfolded in the psychiatric context. Finally,
    we discuss various open issues and the corresponding solutions to underpin the
    digital phenotyping of mental health.
  Author: Yunji Liang and Xiaolong Zheng and Daniel D. Zeng
  Book Title/Journal: Information Fusion
  DOI: https://doi.org/10.1016/j.inffus.2019.04.001
  JCS_FACTOR: 12.975
  Keywords: Digital phenotyping, Big data, Mental health, Data mining, Information
    fusion
  SCI_FACTOR: 2.776
  TITLE_UPPER: INFORMATION FUSION
  Title: A survey on big data-driven digital phenotyping of mental health
  Title_JCS: Information Fusion
  Title_SCI: Information Fusion
  Type Publication: article
  Year: 2019
- Abstract: The emergence of powerful software has created conditions and approaches
    for large datasets to be collected and analyzed which has led to informed decision-making
    towards tackling health issues. The objective of this study is to systematically
    review 804 scholarly publications related to big data analytics in health in order
    to identify the organizational and social values along with associated challenges.
    Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses
    (PRISMA) methodology were followed for conducting systematic reviews. Following
    a research path, we present the values, challenges and future directions of the
    scientific area using indicative examples from relevant published articles. The
    study reveals that one of the main values created is the development of analytical
    techniques which provides personalized health services to users and supports human
    decision-making using automated algorithms, challenging the power issues in the
    doctor-patient relationship and creating new working conditions. A main challenge
    to data analytics is data management and security when processing large volumes
    of sensitive, personal health data. Future research is directed towards the development
    of systems that will standardize and secure the process of extracting private
    healthcare datasets from relevant organizations. Our systematic literature review
    aims to provide to governments and health policy-makers a better understanding
    of how the development of a data driven strategy can improve public health and
    the functioning of healthcare organizations but also how can create challenges
    that need to be addressed in the near future to avoid societal malfunctions.
  Author: P. Galetsi and K. Katsaliaki and S. Kumar
  Book Title/Journal: Social Science & Medicine
  DOI: https://doi.org/10.1016/j.socscimed.2019.112533
  JCS_FACTOR: 4.634
  Keywords: Systematic review, Big data analytics, Health-medicine, Decision-making,
    Organizational and societal values, Preferred reporting items for systematic reviews
    and meta-analyses
  SCI_FACTOR: 0.0
  TITLE_UPPER: SOCIAL SCIENCE & MEDICINE
  Title: 'Values, challenges and future directions of big data analytics in healthcare:
    A systematic review'
  Title_JCS: SOCIAL SCIENCE & MEDICINE
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The emergence of powerful software has created conditions and approaches
    for large datasets to be collected and analyzed which has led to informed decision-making
    towards tackling health issues. The objective of this study is to systematically
    review 804 scholarly publications related to big data analytics in health in order
    to identify the organizational and social values along with associated challenges.
    Key principles of Preferred Reporting Items for Systematic Reviews and Meta-Analyses
    (PRISMA) methodology were followed for conducting systematic reviews. Following
    a research path, we present the values, challenges and future directions of the
    scientific area using indicative examples from relevant published articles. The
    study reveals that one of the main values created is the development of analytical
    techniques which provides personalized health services to users and supports human
    decision-making using automated algorithms, challenging the power issues in the
    doctor-patient relationship and creating new working conditions. A main challenge
    to data analytics is data management and security when processing large volumes
    of sensitive, personal health data. Future research is directed towards the development
    of systems that will standardize and secure the process of extracting private
    healthcare datasets from relevant organizations. Our systematic literature review
    aims to provide to governments and health policy-makers a better understanding
    of how the development of a data driven strategy can improve public health and
    the functioning of healthcare organizations but also how can create challenges
    that need to be addressed in the near future to avoid societal malfunctions.
  Author: P. Galetsi and K. Katsaliaki and S. Kumar
  Book Title/Journal: Social Science & Medicine
  DOI: https://doi.org/10.1016/j.socscimed.2019.112533
  JCS_FACTOR: 4.634
  Keywords: Systematic review, Big data analytics, Health-medicine, Decision-making,
    Organizational and societal values, Preferred reporting items for systematic reviews
    and meta-analyses
  SCI_FACTOR: 0.0
  TITLE_UPPER: SOCIAL SCIENCE & MEDICINE
  Title: 'Values, challenges and future directions of big data analytics in healthcare:
    A systematic review'
  Title_JCS: SOCIAL SCIENCE & MEDICINE
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Big data production in industrial Internet of Things (IIoT) is evident
    due to the massive deployment of sensors and Internet of Things (IoT) devices.
    However, big data processing is challenging due to limited computational, networking
    and storage resources at IoT device-end. Big data analytics (BDA) is expected
    to provide operational- and customer-level intelligence in IIoT systems. Although
    numerous studies on IIoT and BDA exist, only a few studies have explored the convergence
    of the two paradigms. In this study, we investigate the recent BDA technologies,
    algorithms and techniques that can lead to the development of intelligent IIoT
    systems. We devise a taxonomy by classifying and categorising the literature on
    the basis of important parameters (e.g. data sources, analytics tools, analytics
    techniques, requirements, industrial analytics applications and analytics types).
    We present the frameworks and case studies of the various enterprises that have
    benefited from BDA. We also enumerate the considerable opportunities introduced
    by BDA in IIoT. We identify and discuss the indispensable challenges that remain
    to be addressed, serving as future research directions.
  Author: Muhammad Habib {ur Rehman} and Ibrar Yaqoob and Khaled Salah and Muhammad
    Imran and Prem Prakash Jayaraman and Charith Perera
  Book Title/Journal: Future Generation Computer Systems
  DOI: https://doi.org/10.1016/j.future.2019.04.020
  JCS_FACTOR: 0.0
  Keywords: Internet of Things, Cyber-physical systems, Cloud computing, Analytics,
    Big data
  SCI_FACTOR: 1.262
  TITLE_UPPER: FUTURE GENERATION COMPUTER SYSTEMS
  Title: The role of big data analytics in industrial Internet of Things
  Title_JCS: N/A
  Title_SCI: Future Generation Computer Systems
  Type Publication: article
  Year: 2019
- Abstract: 'Background

    Big data analytics are becoming more prevalent due to the recent availability
    of health data. Yet in spite of evidence supporting the potential contribution
    of big data analytics to health policy makers and care providers, these tools
    are still too complex to be routinely used. Further, access to comprehensive datasets
    required for more accurate results is complex and costly. Consequently, big data
    analytics are mostly used by researchers and experts who are far removed from
    actual clinical practice. Hence, policy makers should allocate resources to encourage
    studies that clarify and simplify big data analytics so it can be used by non-experts
    (e.g., clinicians, practitioners and decision-makers who may not have advanced
    computer skills). It is also important to fund data collection and integration
    from various health IT, a pre-condition for any big data analytics project.

    Objectives

    To methodologically clarify the rationale and logic behind several analytics algorithms
    to help non-expert users employ big data analytics by understanding how to implement
    relatively easy to use platforms as Azure ML.

    Methods

    We demonstrate the predictive power of four known algorithms and compare their
    accuracy in predicting early mortality of Congestive Heart Failure (CHF) patients.

    Results

    The results of our models outperform those reported in the literature, attesting
    to the strength of some of the models, and the utility of comprehensive data.

    Conclusions

    The results support our call to policy makers to allocate resources to establishing
    comprehensive, integrated health IT systems, and to projects aimed at simplifying
    ML analytics.'
  Author: Ofir Ben-Assuli and Tsipi Heart and Nir Shlomo and Robert Klempfner
  Book Title/Journal: Health Policy and Technology
  DOI: https://doi.org/10.1016/j.hlpt.2018.12.003
  JCS_FACTOR: 1.931
  Keywords: Congestive heart failure, Machine learning, Logistic regression, Boosted
    decision tree, Support vector machine, Neural network
  SCI_FACTOR: 0.393
  TITLE_UPPER: HEALTH POLICY AND TECHNOLOGY
  Title: 'Bringing big data analytics closer to practice: A methodological explanation
    and demonstration of classification algorithms'
  Title_JCS: Health Policy and Technology
  Title_SCI: Health Policy and Technology
  Type Publication: article
  Year: 2019
- Abstract: Big data analytics is becoming very popular concept in academia as well
    as in industry. It has come up with new decision tools to design data-driven supply
    chains. The manufacturing industry is under huge pressure to integrate sustainable
    practices into their overall business for sustainbale operations management. The
    purpose of this study is to analyse the predictors of sustainable business performance
    through big data analytics in the context of developing countries. Data was collected
    from manufacturing firms those have adopted sustainable practices. A hybrid Structural
    Equation Modelling - Artificial Neural Network model is used to analyse 316 responses
    of Indian professional experts. Factor analysis results shows that management
    and leadership style, state and central-government policy, supplier integration,
    internal business process, and customer integration have a significant influence
    on big data analytics and sustainability practices. Furthermore, the results obtained
    from structural equation modelling were feed as input to the artificial neural
    network model. The study findings shows that management and leadership style,
    state and central-government policy as the two most important predictors of big
    data analytics and sustainability practices. The results provide unique insights
    into manufacturing firms to improve their sustainable business performance from
    an operations management viewpoint. The study provides theoretical and practical
    insights into big data implementation issues in accomplishing sustainability practices
    in business organisations of emerging economies.
  Author: Rakesh D. Raut and Sachin Kumar Mangla and Vaibhav S. Narwane and Bhaskar
    B. Gardas and Pragati Priyadarshinee and Balkrishna E. Narkhede
  Book Title/Journal: Journal of Cleaner Production
  DOI: https://doi.org/10.1016/j.jclepro.2019.03.181
  JCS_FACTOR: 9.297
  Keywords: Big-data analytics, Ecological-economic-social sustainability, Green practices,
    SustainableOperations management, Structural equation modelling-artificial neural
    network, Emerging economies
  SCI_FACTOR: 1.937
  TITLE_UPPER: JOURNAL OF CLEANER PRODUCTION
  Title: Linking big data analytics and operational sustainability practices for sustainable
    business management
  Title_JCS: Journal of Cleaner Production
  Title_SCI: Journal of Cleaner Production
  Type Publication: article
  Year: 2019
- Abstract: "Big data analytics has recently emerged as an important research area\
    \ due to the popularity of the Internet and the advent of the Web 2.0 technologies.\
    \ Moreover, the proliferation and adoption of social media applications have provided\
    \ extensive opportunities and challenges for researchers and practitioners. The\
    \ massive amount of data generated by users using social media platforms is the\
    \ result of the integration of their background details and daily activities.\
    \ This enormous volume of generated data known as \xE2\u20AC\u0153big data\xE2\
    \u20AC\x9D has been intensively researched recently. A review of the recent works\
    \ is presented to obtain a broad perspective of the social media big data analytics\
    \ research topic. We classify the literature based on important aspects. This\
    \ study also compares possible big data analytics techniques and their quality\
    \ attributes. Moreover, we provide a discussion on the applications of social\
    \ media big data analytics by highlighting the state-of-the-art techniques, methods,\
    \ and the quality attributes of various studies. Open research challenges in big\
    \ data analytics are described as well."
  Author: Norjihan Abdul Ghani and Suraya Hamid and Ibrahim Abaker {Targio Hashem}
    and Ejaz Ahmed
  Book Title/Journal: Computers in Human Behavior
  DOI: https://doi.org/10.1016/j.chb.2018.08.039
  JCS_FACTOR: 6.829
  Keywords: Big data, Social media, Machine learning, Analytics
  SCI_FACTOR: 2.108
  TITLE_UPPER: COMPUTERS IN HUMAN BEHAVIOR
  Title: 'Social media big data analytics: A survey'
  Title_JCS: COMPUTERS IN HUMAN BEHAVIOR
  Title_SCI: Computers in Human Behavior
  Type Publication: article
  Year: 2019
- Abstract: "This paper considers the implications of so-called \xE2\u20AC\u02DCbig\
    \ data\xE2\u20AC\u2122 for the analysis, modelling and planning of transport systems.\
    \ The primary conceptual focus is on the needs of the practical context of medium-term\
    \ planning and decision-making, from which perspective the paper seeks to achieve\
    \ three goals: (i) to try to identify what is truly \xE2\u20AC\u02DCspecial\xE2\
    \u20AC\u2122 about big data; (ii) to provoke debate on the future relationship\
    \ between transport planning and big data; and (iii) to try to identify promising\
    \ themes for research and application. Differences in the information that can\
    \ be derived from the data compared to more traditional surveys are discussed,\
    \ and the respects in which they may impact on the role of models in supporting\
    \ transport planning and decision-making are identified. It is argued that, over\
    \ time, changes to the nature of data may lead to significant differences in both\
    \ modelling approaches and in the expectations placed upon them. Furthermore,\
    \ it is suggested that the potential widespread availability of data to commercial\
    \ actors and travellers will affect the performance of the transport systems themselves,\
    \ which might be expected to have knock-on effects for planning functions. We\
    \ conclude by proposing a series of research challenges that we believe need to\
    \ be addressed and warn against adaptations based on minimising change from the\
    \ status quo."
  Author: Dave Milne and David Watling
  Book Title/Journal: Journal of Transport Geography
  DOI: https://doi.org/10.1016/j.jtrangeo.2017.11.004
  JCS_FACTOR: 4.986
  Keywords: empty
  SCI_FACTOR: 1.809
  TITLE_UPPER: JOURNAL OF TRANSPORT GEOGRAPHY
  Title: Big data and understanding change in the context of planning transport systems
  Title_JCS: Journal of Transport Geography
  Title_SCI: Journal of Transport Geography
  Type Publication: article
  Year: 2019
- Abstract: Today, we are undoubtedly in the era of data. Big Data Analytics (BDA)
    is no longer a perspective for all level of the organization. This is of special
    interest in the manufacturing process with their high capital intensity, time
    constraints and given the huge amount of data already captured. However, there
    is a paucity in past literature on BDA to develop better understanding of the
    capabilities and strategic implications to extract value from BDA. In that vein,
    the central aim of this paper is to develop a novel model that summarizes the
    main capabilities of BDA in the context of manufacturing process. This is carried
    out by relying on the findings of a review of the ongoing research along with
    a multiple case studies within a leading phosphate derivatives manufacturer to
    point out the capabilities of BDA in manufacturing processes and outline recommendations
    to advance research in the field. The findings will help companies to understand
    the big data analytics capabilities and its potential implications for their manufacturing
    processes and support them seeking to design more effective BDA-enabler infrastructure.
  Author: Amine Belhadi and Karim Zkik and Anass Cherrafi and Sha'ri M. Yusof and
    Said {El fezazi}
  Book Title/Journal: Computers & Industrial Engineering
  DOI: https://doi.org/10.1016/j.cie.2019.106099
  JCS_FACTOR: 5.431
  Keywords: Big Data Analytics, Manufacturing process, Big Data Analytics capabilities,
    Business intelligence, Literature review, Multiple case study
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMPUTERS & INDUSTRIAL ENGINEERING
  Title: 'Understanding Big Data Analytics for Manufacturing Processes: Insights from
    Literature Review and Multiple Case Studies'
  Title_JCS: COMPUTERS & INDUSTRIAL ENGINEERING
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Disruptive innovations are usually identified as ideas that are created\
    \ \xE2\u20AC\u02DCoutside the box\xE2\u20AC\u2122. They are expected to fundamentally\
    \ change existing business models and processes founded on technological applications.\
    \ Disruptive innovations can be challenging to define. Information technology\
    \ (IT) solutions focus on collecting, processing, and reporting different types\
    \ of data. Commonly, is the solutions are expected (in cybernetics or self-regulating\
    \ processes) to provide feedback to original processes and to steer them based\
    \ on the data. To achieve continuous improvement with regard to environmental\
    \ responsibility and profitability, new thinking and, in particular, accurate\
    \ and reliable data are needed for decision-making. Very large data storages,\
    \ known as big data, contain an increasing mass of different types of homogenous\
    \ and non-homogenous information, as well as extensive time-series. New, innovative\
    \ algorithms are required to reveal relevant information and opportunities hidden\
    \ in these data storages. Global environmental challenges and zero-emission responsible\
    \ production issues can only be solved using relevant and reliable continuous\
    \ data as the basis. The final goal should be the creation of scalable environmental\
    \ solutions based on disruptive innovations and accurate data. The aim of this\
    \ paper is to determine the explicit steps for replacing silo-based reporting\
    \ with company-wide, refined information, which enables decision-makers in all\
    \ industries the chance to make responsible choices."
  Author: "Esa H\xC3\xA4m\xC3\xA4l\xC3\xA4inen and Tommi Inkinen"
  Book Title/Journal: Journal of Industrial Information Integration
  DOI: https://doi.org/10.1016/j.jii.2019.100105
  JCS_FACTOR: 10.063
  Keywords: Big data, Disruption, Responsible, Process industry, Economic efficiency,
    Economic geography
  SCI_FACTOR: 2.042
  TITLE_UPPER: JOURNAL OF INDUSTRIAL INFORMATION INTEGRATION
  Title: Industrial applications of big data in disruptive innovations supporting
    environmental reporting
  Title_JCS: Journal of Industrial Information Integration
  Title_SCI: Journal of Industrial Information Integration
  Type Publication: article
  Year: 2019
- Abstract: The era of big data has brought new challenges to chemical enterprises.
    In order to maximize the benefits, enterprises are considering to implement intelligent
    service technology into traditional production systems to improve the level of
    intelligence in business. This paper proposes a service framework based on big
    data driven prediction, which includes information perception layer, information
    application layer and big data service layer. In this paper, the composition of
    big data service layer is described in detail, and a sales predicting method based
    on neural network is introduced. The salability of products is divided, and the
    qualitative economic production volume mechanism is finally given. Based on the
    framework, an intelligent service system for enterprises with the characteristics
    of mass production is implemented. Experimental results show that the big data
    service framework can support chemical enterprises to make decisions to reduce
    costs, and provides an effective method for Smart Product Service System (PSS).
  Author: Yongheng Zhang and Rui Zhang and Yizhong Wang and Hongfei Guo and Ray Y
    Zhong and Ting Qu and Zhiwu Li
  Book Title/Journal: Procedia CIRP
  DOI: https://doi.org/10.1016/j.procir.2019.05.023
  JCS_FACTOR: 0.0
  Keywords: Big data, Smart Product-Service System, Sales predict, Economic batch
    quantity, production plan
  SCI_FACTOR: 0.683
  TITLE_UPPER: PROCEDIA CIRP
  Title: Big data driven decision-making for batch-based production systems
  Title_JCS: N/A
  Title_SCI: Procedia CIRP
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Aadia I. Rana and Michael J. Mugavero
  Book Title/Journal: Infectious Disease Clinics of North America
  DOI: https://doi.org/10.1016/j.idc.2019.05.009
  JCS_FACTOR: 5.982
  Keywords: HIV, AIDS, Prevention, Treatment, Continuum, Data, Surveillance
  SCI_FACTOR: 1.854
  TITLE_UPPER: INFECTIOUS DISEASE CLINICS OF NORTH AMERICA
  Title: How Big Data Science Can Improve Linkage and Retention in Care
  Title_JCS: INFECTIOUS DISEASE CLINICS OF NORTH AMERICA
  Title_SCI: Infectious Disease Clinics of North America
  Type Publication: article
  Year: 2019
- Abstract: Despite some of the initial hype from marketers and consultants, the use
    of big data is now firmly established in many organisations worldwide. Big data
    analytics (BDA) is making use of huge volumes of data from a wide range of structured
    and unstructured sources. Surveys have however reported a number of barriers to
    organisational effectiveness with BDA. This research aims to determine what capabilities
    large organisations require to be ready for a successful BDA initiative. Drawing
    mainly on relevant results of two published research articles, key informed stakeholders
    from a large South African telecommunications company were interviewed on this
    topic. Thematic analysis identified the key themes and sub-themes relating to
    capabilities needed for the organization to be ready for effective BDA. These
    proved to be very similar to those given in the earlier research, although a new
    capability of legal compliance for data protection was now added.
  Author: Jenifer Pedro and Irwin Brown and Mike Hart
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.12.147
  JCS_FACTOR: 0.0
  Keywords: Big data analytics, organisational readiness, organisational capabilities,
    frameworks, business analytics, thematic analysis
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: Capabilities and Readiness for Big Data Analytics
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: Big data analytics has been widely regarded as a breakthrough technological
    development in academic and business communities. Despite the growing number of
    firms that are launching big data initiatives, there is still limited understanding
    on how firms translate the potential of such technologies into business value.
    The literature argues that to leverage big data analytics and realize performance
    gains, firms must develop strong big data analytics capabilities. Nevertheless,
    most studies operate under the assumption that there is limited heterogeneity
    in the way firms build their big data analytics capabilities and that related
    resources are of similar importance regardless of context. This paper draws on
    complexity theory and investigates the configurations of resources and contextual
    factors that lead to performance gains from big data analytics investments. Our
    empirical investigation followed a mixed methods approach using survey data from
    175 chief information officers and IT managers working in Greek firms, and three
    case studies to show that depending on the context, big data analytics resources
    differ in significance when considering performance gains. Applying a fuzzy-set
    qualitative comparative analysis (fsQCA) method on the quantitative data, we show
    that there are four different patterns of elements surrounding big data analytics
    that lead to high performance. Outcomes of the three case studies highlight the
    inter-relationships between these elements and outline challenges that organizations
    face when orchestrating big data analytics resources.
  Author: Patrick Mikalef and Maria Boura and George Lekakos and John Krogstie
  Book Title/Journal: Journal of Business Research
  DOI: https://doi.org/10.1016/j.jbusres.2019.01.044
  JCS_FACTOR: 7.55
  Keywords: Big data analytics, Complexity theory, fsQCA, Business value, Mixed-method,
    Environmental uncertainty
  SCI_FACTOR: 2.049
  TITLE_UPPER: JOURNAL OF BUSINESS RESEARCH
  Title: 'Big data analytics and firm performance: Findings from a mixed-method approach'
  Title_JCS: JOURNAL OF BUSINESS RESEARCH
  Title_SCI: Journal of Business Research
  Type Publication: article
  Year: 2019
- Abstract: The advent of connected devices and omnipresence of Internet have paved
    way for intruders to attack networks, which leads to cyber-attack, financial loss,
    information theft in healthcare, and cyber war. Hence, network security analytics
    has become an important area of concern and has gained intensive attention among
    researchers, off late, specifically in the domain of anomaly detection in network,
    which is considered crucial for network security. However, preliminary investigations
    have revealed that the existing approaches to detect anomalies in network are
    not effective enough, particularly to detect them in real time. The reason for
    the inefficacy of current approaches is mainly due the amassment of massive volumes
    of data though the connected devices. Therefore, it is crucial to propose a framework
    that effectively handles real time big data processing and detect anomalies in
    networks. In this regard, this paper attempts to address the issue of detecting
    anomalies in real time. Respectively, this paper has surveyed the state-of-the-art
    real-time big data processing technologies related to anomaly detection and the
    vital characteristics of associated machine learning algorithms. This paper begins
    with the explanation of essential contexts and taxonomy of real-time big data
    processing, anomalous detection, and machine learning algorithms, followed by
    the review of big data processing technologies. Finally, the identified research
    challenges of real-time big data processing in anomaly detection are discussed.
  Author: Riyaz Ahamed {Ariyaluran Habeeb} and Fariza Nasaruddin and Abdullah Gani
    and Ibrahim Abaker {Targio Hashem} and Ejaz Ahmed and Muhammad Imran
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2018.08.006
  JCS_FACTOR: 14.098
  Keywords: Real-time, Big data processing, Anomaly detection and machine learning
    algorithms
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: 'Real-time big data processing for anomaly detection: A Survey'
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: Battery is the bottleneck technology of electric vehicles. The complex
    chemical reactions inside the battery are difficult to monitor directly. The establishment
    of a precise mathematical model for the battery is of great significance in ensuring
    the secure and stable operation of the battery management system. First of all,
    a data cleaning method based on machine learning is put forward, which is applicable
    to the characteristics of big data from batteries in electric vehicles. Secondly,
    this paper establishes a lithium-ion battery model based on deep learning algorithm
    and the error of model based on different algorithms is compared. The data of
    electric buses are used for validating the effectiveness of the model. The result
    shows that the data cleaning method achieves good results, in the case of the
    terminal voltage missing, the mean absolute percentage error of filling is within
    4%, and the battery modeling method in this paper is able to simulate the battery
    characteristics accurately, and the mean absolute percentage error of the terminal
    voltage estimation is within 2.5%.
  Author: Shuangqi Li and Jianwei Li and Hongwen He and Hanxiao Wang
  Book Title/Journal: Energy Procedia
  DOI: https://doi.org/10.1016/j.egypro.2018.12.046
  JCS_FACTOR: 0.0
  Keywords: electric vehicle, lithium-ion power battery, modeling, battery management,
    bigdata, deeplearning
  SCI_FACTOR: 0.474
  TITLE_UPPER: ENERGY PROCEDIA
  Title: Lithium-ion battery modeling based on Big Data
  Title_JCS: N/A
  Title_SCI: Energy Procedia
  Type Publication: article
  Year: 2019
- Abstract: "In common with much contemporary discourse around big data, recent discussion\
    \ of datafication in the Journal of Strategic Information Systems has focused\
    \ on its effects on individuals, organisations and society. Generally missing\
    \ from such analysis, however, is any consideration of data themselves. What is\
    \ it that is having these effects? In this Viewpoint article I therefore present\
    \ a critical analysis of a number of widely-held assumptions about data in general\
    \ and big data in particular. Rather than being a referential, natural, foundational,\
    \ objective and equal representation of the world, it will be argued, data are\
    \ partial and contingent and are brought into being through situated practices\
    \ of conceptualization, recording and use. Big data are also not as revolutionary\
    \ voluminous, universal or exhaustive as they are often presented. Some initial\
    \ implications of this reconceptualization of data are explored. A distinction\
    \ is made between \xE2\u20AC\u0153data in principle\xE2\u20AC\x9D as they are\
    \ recorded, and the \xE2\u20AC\u0153data in practice\xE2\u20AC\x9D as they are\
    \ used. It is only the latter, typically a small and not necessarily representative\
    \ subset of the former, that will contribute directly to the effects of datafication."
  Author: Matthew Jones
  Book Title/Journal: The Journal of Strategic Information Systems
  DOI: https://doi.org/10.1016/j.jsis.2018.10.005
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE JOURNAL OF STRATEGIC INFORMATION SYSTEMS
  Title: What we talk about when we talk about (big) data
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Big data has caused the scientific community to re-examine the scientific
    research methodologies and has triggered a revolution in scientific thinking.
    As a branch of scientific research, production safety management is also exploring
    methods to take advantage of big data. This research aims to provide a theoretical
    basis for promoting the application of big data in production safety management.
    First, four different types of production safety management paradigms were identified,
    namely small-data-based, static-oriented, interpretation-based and causal-oriented
    paradigm, and the challenges to these paradigms in the presence of big data were
    introduced. Second, the opportunities of employing big data in production safety
    management were identified from four aspects, including better predict the future
    production safety phenomena, promote production safety management highlight relevance,
    achieve the balance between deductive and inductive approaches and promote the
    interdisciplinary development of production safety management. Third, the paradigm
    shifting trend of production safety management was concluded, and the discipline
    foundation of the new paradigm was considered as the integration of data science,
    production management and safety science. Fourth, a new big-data-driven production
    safety management paradigm was developed, which consists of the logical line of
    production safety management, the macro-meso-micro data spectrum, the key big
    data analytics, and the four-dimensional morphology. At last, the strengths (e.g.,
    supporting better-informed safety description, safety inquisition, safety prediction)
    and future research direction (e.g., theory research focuses on safety-related
    data mining/capturing/cleansing) of the new paradigm were discussed. The research
    results not only can provide theoretical and practical basis for big-data-driven
    production safety management, but also can offer advice to managerial consideration
    and scholarly investigation.
  Author: Lang Huang and Chao Wu and Bing Wang
  Book Title/Journal: Journal of Cleaner Production
  DOI: https://doi.org/10.1016/j.jclepro.2019.05.245
  JCS_FACTOR: 9.297
  Keywords: Big data, Production safety management, Big-data-driven, Challenges, Opportunities
  SCI_FACTOR: 1.937
  TITLE_UPPER: JOURNAL OF CLEANER PRODUCTION
  Title: 'Challenges, opportunities and paradigm of applying big data to production
    safety management: From a theoretical perspective'
  Title_JCS: Journal of Cleaner Production
  Title_SCI: Journal of Cleaner Production
  Type Publication: article
  Year: 2019
- Abstract: Energy economy models are central to decision making on energy and climate
    issues in the 21st century, such as informing the design of deep decarbonisation
    strategies under the Paris Agreement. Designing policies that are aimed at achieving
    such radical transitions in the energy system will require ever more in-depth
    modelling of end-use demand, efficiency and fuel switching, as well as an increasing
    need for regional, sectoral, and agent disaggregation to capture technological,
    jurisdictional and policy detail. Building and using these models entails complex
    trade-offs between the level of detail, the size of the system boundary, and the
    available computing resources. The availability of data to characterise key energy
    system sectors and interactions is also a key driver of model structure and parameterisation,
    and there are many blind spots and design compromises that are caused by data
    scarcity. We may soon, however, live in a world of data abundance, potentially
    enabling previously impossible levels of resolution and coverage in energy economy
    models. But while big data concepts and platforms have already begun to be used
    in a number of selected energy research applications, their potential to improve
    or even completely revolutionise energy economy modelling has been almost completely
    overlooked in the existing literature. In this paper, we explore the challenges
    and possibilities of this emerging frontier. We identify critical gaps and opportunities
    for the field, as well as developing foundational concepts for guiding the future
    application of big data to energy economy modelling, with reference to the existing
    literature on decision making under uncertainty, scenario analysis and the philosophy
    of science.
  Author: Francis G.N. Li and Chris Bataille and Steve Pye and Aidan O'Sullivan
  Book Title/Journal: Applied Energy
  DOI: https://doi.org/10.1016/j.apenergy.2019.02.002
  JCS_FACTOR: 9.746
  Keywords: Energy modelling, Climate policy, Energy policy, Decarbonisation, Energy
    data, Big data
  SCI_FACTOR: 3.035
  TITLE_UPPER: APPLIED ENERGY
  Title: 'Prospects for energy economy modelling with big data: Hype, eliminating
    blind spots, or revolutionising the state of the art?'
  Title_JCS: APPLIED ENERGY
  Title_SCI: Applied Energy
  Type Publication: article
  Year: 2019
- Abstract: 'Context

    Big Data Cybersecurity Analytics (BDCA) systems leverage big data technologies
    for analyzing security events data to protect organizational networks, computers,
    and data from cyber attacks.

    Objective

    We aimed at identifying the most frequently reported quality attributes and architectural
    tactics for BDCA systems.

    Method

    We used Systematic Literature Review (SLR) method for reviewing 74 papers.

    Result

    Our findings are twofold: (i) identification of 12 most frequently reported quality
    attributes for BDCA systems; and (ii) identification and codification of 17 architectural
    tactics for addressing the identified quality attributes. The identified tactics
    include six performance tactics, four accuracy tactics, two scalability tactics,
    three reliability tactics, and one security and usability tactic each.

    Conclusion

    Our study reveals that in the context of BDCA (a) performance, accuracy and scalability
    are the most important quality concerns (b) data analytics is the most critical
    architectural component (c) despite the significance of interoperability, modifiability,
    adaptability, generality, stealthiness, and privacy assurance, these quality attributes
    lack explicit architectural support (d) empirical investigation is required to
    evaluate the impact of the codified tactics and explore the quality trade-offs
    and dependencies among the tactics and (e) the reported tactics need to be modelled
    using a standardized modelling language such as UML.'
  Author: Faheem Ullah and Muhammad {Ali Babar}
  Book Title/Journal: Journal of Systems and Software
  DOI: https://doi.org/10.1016/j.jss.2019.01.051
  JCS_FACTOR: 2.829
  Keywords: Big data, Cybersecurity, Quality attribute, Architectural tactic
  SCI_FACTOR: 0.642
  TITLE_UPPER: JOURNAL OF SYSTEMS AND SOFTWARE
  Title: 'Architectural Tactics for Big Data Cybersecurity Analytics Systems: A Review'
  Title_JCS: JOURNAL OF SYSTEMS AND SOFTWARE
  Title_SCI: Journal of Systems and Software
  Type Publication: article
  Year: 2019
- Abstract: 'Today, the Big Data term has a multidimensional approach where five main
    characteristics stand out: volume, velocity, veracity, value and variety. It has
    changed from being an emerging theme to a growing research area. In this respect,
    this study analyses the literature on Big Data in the Economics, Econometrics
    and Finance field. To do that, 1.034 publications from 2015 to 2019 were evaluated
    using SciMAT as a bibliometric and network analysis software. SciMAT offers a
    complete approach of the field and evaluates the most cited and productive authors,
    countries and subject areas related to Big Data. Lastly, a science map is performed
    to understand the intellectual structure and the main research lines (themes).'
  Author: "Jos\xC3\xA9 Ricardo L\xC3\xB3pez-Robles and Marisela Rodr\xC3\xADguez-Salvador\
    \ and Nadia Karina Gamboa-Rosales and Selene Ramirez-Rosales and Manuel Jes\xC3\
    \xBAs Cobo"
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.12.044
  JCS_FACTOR: 0.0
  Keywords: Type your keywords here, separated by semicolons
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: 'The last five years of Big Data Research in Economics, Econometrics and
    Finance: Identification and conceptual analysis'
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: The idea of big data is mainly reflected in its dimensions, which are
    popularly known as the Big Vs, which stands for Volume, Variety, Velocity, and
    Veracity. However, the concept goes beyond the Big Vs and testing of hypotheses,
    to focus on data analysis, hypothesis generation, and ascertaining the progressive
    strength of association. Preliminary study reveals that big data analytics adopts
    many data mining methods, such as descriptive, diagnostic, predictive, and prescriptive
    analytics. This evolving technology has tremendous application in healthcare,
    such as surveillance of safety or disease, predictive modeling, public health,
    pharma data analytics, clinical data analytics, healthcare analytics, and research.
    Moreover, the journey of big data in the medical domain is proving to be one of
    the important research thrusts of recent times. Study reveals that medical data
    is very specific and heterogeneous due to varied data sources such as scanned
    images, CT scan reports, doctor prescriptions, electronic health records (EHRs),
    etc. Medical data analytics faces some bottlenecks due to missing data, high dimensions,
    bias, and limitations of the study of patients through observation. Therefore,
    special big data techniques are required to handle them. Besides, many ethical,
    legal, social, clinical, and utility challenges are also a part of the data-handling
    process, which makes the role of big data in the medical field very challenging.
    Nevertheless, big data analytics is a fuel to the healthcare system that will
    provide a healthier life to patients; the issues and bottlenecks when removed
    from the system will be a boon for the entire human race. The chapter focuses
    on understanding the big data characteristics in medical big data, medical big
    data analytics, and its various applications in the interest of society.
  Author: Neha Sharma and Malini M. Patil and Madhavi Shamkuwar
  Book Title/Journal: Internet of Things in Biomedical Engineering
  DOI: https://doi.org/10.1016/B978-0-12-817356-5.00010-3
  JCS_FACTOR: 0.0
  Keywords: Big data, Medical big data, Healthcare data, Medical big data analytics,
    Healthcare data analytics, Data analytics, Pharmacology data analytics
  SCI_FACTOR: 0.0
  TITLE_UPPER: INTERNET OF THINGS IN BIOMEDICAL ENGINEERING
  Title: Chapter 8 - Why Big Data and What Is It? Basic to Advanced Big Data Journey
    for the Medical Industry
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: empty
  Author: Nikos Bikakis and George Papastefanatos and Olga Papaemmanouil
  Book Title/Journal: Big Data Research
  DOI: https://doi.org/10.1016/j.bdr.2019.100123
  JCS_FACTOR: 3.578
  Keywords: empty
  SCI_FACTOR: 0.565
  TITLE_UPPER: BIG DATA RESEARCH
  Title: Big Data Exploration, Visualization and Analytics
  Title_JCS: Big Data Research
  Title_SCI: Big Data Research
  Type Publication: article
  Year: 2019
- Abstract: In this contribution, a data-driven approach towards the prediction of
    maintenance for the critical component of an injection molding machine is presented.
    We present our path from exploring and cleaning the data towards the implementation
    of a prediction algorithm based on kernel density estimation. We give first analytical
    evidence of the algorithms potential. Moreover, we compare the approach described
    here with our previous work where we went a model-based approach and present advantages
    and disadvantages of the two approaches. We try to contribute to a non-comprehensive
    guide on the implementation of predictive maintenance systems for industrial mass
    production facilities.
  Author: "Domingo Llorente Rivera and Markus R. Scholz and Christoph B\xC3\xBChl\
    \ and Markus Krauss and Klaus Schilling"
  Book Title/Journal: IFAC-PapersOnLine
  DOI: https://doi.org/10.1016/j.ifacol.2019.12.364
  JCS_FACTOR: 0.0
  Keywords: industrial analytics, anomaly detection, predictive maintenance, hydraulic
    pump, stochastic modeling
  SCI_FACTOR: 0.308
  TITLE_UPPER: IFAC-PAPERSONLINE
  Title: Is Big Data About to Retire Expert Knowledge? A Predictive Maintenance Study
  Title_JCS: N/A
  Title_SCI: IFAC-PapersOnLine
  Type Publication: article
  Year: 2019
- Abstract: In the current scenario, sustainable auditing, for example roundtable
    of sustainable palm oil (RSPO), requires a huge amount of data to be manually
    collected and entered into paper forms by farmers. Such systems are inherently
    inefficient, time-consuming, and, prone to errors. Researchers have proposed Big
    Data Analytics (BDA) based framework for next-generation smart sustainable auditing
    systems. Though theoretically feasible, real-life implementation of such frameworks
    is extremely difficult. Thus, this paper aims to identify the critical barriers
    that hinder the application of BDA based smart sustainable auditing system. It
    also aims to explore the dynamic interrelations among the barriers. We applied
    Interpretive Structural Modelling (ISM) approach to develop the model that extrapolates
    BDA adoption barriers and their relationships. The proposed model illustrates
    how barriers are spread over various levels and how specific barriers impact other
    barriers through direct and/or transitive links. This study provides practitioners
    with a roadmap to prioritise the interventions to facilitate the adoption of BDA
    in the sustainable auditing systems. Insights of this study could be used by academics
    to enhance understanding of the barriers to BDA applications.
  Author: Manish Shukla and Lana Mattar
  Book Title/Journal: Computers & Industrial Engineering
  DOI: https://doi.org/10.1016/j.cie.2018.04.055
  JCS_FACTOR: 5.431
  Keywords: Big Data Analytics, Sustainable auditing systems, Barriers, RSPO, Interpretive
    Structural Modelling
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMPUTERS & INDUSTRIAL ENGINEERING
  Title: 'Next generation smart sustainable auditing systems using Big Data Analytics:
    Understanding the interaction of critical barriers'
  Title_JCS: COMPUTERS & INDUSTRIAL ENGINEERING
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Peng Jia and Hong Xue and Shiyong Liu and Hao Wang and Lijian Yang and Therese
    Hesketh and Lu Ma and Hongwei Cai and Xin Liu and Yaogang Wang and Youfa Wang
  Book Title/Journal: Science Bulletin
  DOI: https://doi.org/10.1016/j.scib.2019.09.011
  JCS_FACTOR: 11.78
  Keywords: empty
  SCI_FACTOR: 1.983
  TITLE_UPPER: SCIENCE BULLETIN
  Title: Opportunities and challenges of using big data for global health
  Title_JCS: Science Bulletin
  Title_SCI: Science Bulletin
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Tao Hong and Pierre Pinson
  Book Title/Journal: International Journal of Forecasting
  DOI: https://doi.org/10.1016/j.ijforecast.2019.05.004
  JCS_FACTOR: 3.779
  Keywords: empty
  SCI_FACTOR: 1.268
  TITLE_UPPER: INTERNATIONAL JOURNAL OF FORECASTING
  Title: Energy forecasting in the big data world
  Title_JCS: INTERNATIONAL JOURNAL OF FORECASTING
  Title_SCI: International Journal of Forecasting
  Type Publication: article
  Year: 2019
- Abstract: The domain of healthcare acquired its influence by the impact of big data
    since the data sources involved in the healthcare organizations are well-known
    for their volume, heterogeneous complexity and high dynamism. Though the role
    of big data analytical techniques, platforms, tools are realized among various
    domains, their impact on healthcare organization for implementing and delivering
    novel use-cases for potential healthcare applications shows promising research
    directions. In the context of big data, the success of healthcare applications
    solely depends on the underlying architecture and utilization of appropriate tools
    as evidenced in pioneering research attempts. Novel research works have been carried
    out for deriving application specific healthcare frameworks that offer diversified
    data analytical capabilities for handling sources of data ranging from electronic
    health records to medical images. In this paper, we have presented various analytical
    avenues that exist in the patient-centric healthcare system from the perspective
    of various stakeholders. We have also reviewed various big data frameworks with
    respect to underlying data sources, analytical capability and application areas.
    In addition, the implication of big data tools in developing healthcare eco system
    is also presented.
  Author: Venketesh Palanisamy and Ramkumar Thirunavukarasu
  Book Title/Journal: Journal of King Saud University - Computer and Information Sciences
  DOI: https://doi.org/10.1016/j.jksuci.2017.12.007
  JCS_FACTOR: 0.0
  Keywords: Big data, Healthcare, Framework, Infrastructure, Analytics, Patterns,
    Tools
  SCI_FACTOR: 0.617
  TITLE_UPPER: JOURNAL OF KING SAUD UNIVERSITY - COMPUTER AND INFORMATION SCIENCES
  Title: "Implications of big data analytics in developing healthcare frameworks \xE2\
    \u20AC\u201C A review"
  Title_JCS: N/A
  Title_SCI: Journal of King Saud University - Computer and Information Sciences
  Type Publication: article
  Year: 2019
- Abstract: "Background\nBuilding cancer risk models from real-world data requires\
    \ overcoming challenges in data preprocessing, efficient representation, and computational\
    \ performance. We present a case study of a cloud-based approach to learning from\
    \ de-identified electronic health record data and demonstrate its effectiveness\
    \ for melanoma risk prediction.\nMethods\nWe used a hybrid distributed and non-distributed\
    \ approach to computing in the cloud: distributed processing with Apache Spark\
    \ for data preprocessing and labeling, and non-distributed processing for machine\
    \ learning model training with scikit-learn. Moreover, we explored the effects\
    \ of sampling the training dataset to improve computational performance. Risk\
    \ factors were evaluated using regression weights as well as tree SHAP values.\n\
    Results\nAmong 4,061,172 patients who did not have melanoma through the 2016 calendar\
    \ year, 10,129 were diagnosed with melanoma within one year. A gradient-boosted\
    \ classifier achieved the best predictive performance with cross-validation (AUC\xE2\
    \u20AC\xAF=\xE2\u20AC\xAF0.799, Sensitivity\xE2\u20AC\xAF=\xE2\u20AC\xAF0.753,\
    \ Specificity\xE2\u20AC\xAF=\xE2\u20AC\xAF0.688). Compared to a model built on\
    \ the original data, a dataset two orders of magnitude smaller could achieve statistically\
    \ similar or better performance with less than 1% of the training time and cost.\n\
    Conclusions\nWe produced a model that can effectively predict melanoma risk for\
    \ a diverse dermatology population in the U.S. by using hybrid computing infrastructure\
    \ and data sampling. For this de-identified clinical dataset, sampling approaches\
    \ significantly shortened the time for model building while retaining predictive\
    \ accuracy, allowing for more rapid machine learning model experimentation on\
    \ familiar computing machinery. A large number of risk factors (>300) were required\
    \ to produce the best model."
  Author: Aaron N. Richter and Taghi M. Khoshgoftaar
  Book Title/Journal: Computers in Biology and Medicine
  DOI: https://doi.org/10.1016/j.compbiomed.2019.04.039
  JCS_FACTOR: 4.589
  Keywords: Big data, Cloud computing, Machine learning, Electronic health records,
    Early detection of cancer
  SCI_FACTOR: 0.884
  TITLE_UPPER: COMPUTERS IN BIOLOGY AND MEDICINE
  Title: 'Efficient learning from big data for cancer risk modeling: A case study
    with melanoma'
  Title_JCS: COMPUTERS IN BIOLOGY AND MEDICINE
  Title_SCI: Computers in Biology and Medicine
  Type Publication: article
  Year: 2019
- Abstract: Based on the concept and research status of big data, we analyze and examine
    the importance of constructing the knowledge system of nursing science for the
    development of the nursing discipline in the context of big data and propose that
    it is necessary to establish big data centers for nursing science to share resources,
    unify language standards, improve professional nursing databases, and establish
    a knowledge system structure.
  Author: Ruifang Zhu and Shifan Han and Yanbing Su and Chichen Zhang and Qi Yu and
    Zhiguang Duan
  Book Title/Journal: International Journal of Nursing Sciences
  DOI: https://doi.org/10.1016/j.ijnss.2019.03.001
  JCS_FACTOR: 0.0
  Keywords: Artificial intelligence, Data mining, Knowledge bases, Nursing
  SCI_FACTOR: 0.703
  TITLE_UPPER: INTERNATIONAL JOURNAL OF NURSING SCIENCES
  Title: 'The application of big data and the development of nursing science: A discussion
    paper'
  Title_JCS: N/A
  Title_SCI: International Journal of Nursing Sciences
  Type Publication: article
  Year: 2019
- Abstract: "The introduction of clinical information systems (CIS) in Intensive Care\
    \ Units (ICUs) offers the possibility of storing a huge amount of machine-ready\
    \ clinical data that can be used to improve patient outcomes and the allocation\
    \ of resources, as well as suggest topics for randomized clinical trials. Clinicians,\
    \ however, usually lack the necessary training for the analysis of large databases.\
    \ In addition, there are issues referred to patient privacy and consent, and data\
    \ quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning\
    \ experts, statisticians, epidemiologists and other information scientists may\
    \ overcome these problems. A multidisciplinary event (Critical Care Datathon)\
    \ was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of\
    \ the Spanish Critical Care Society (SEMICYUC), the event was organized by the\
    \ Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge,\
    \ MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic\
    \ Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University.\
    \ After presentations referred to big data in the critical care environment, clinicians,\
    \ data scientists and other health data science enthusiasts and lawyers worked\
    \ in collaboration using an anonymized database (MIMIC III). Eight groups were\
    \ formed to answer different clinical research questions elaborated prior to the\
    \ meeting. The event produced analyses for the questions posed and outlined several\
    \ future clinical research opportunities. Foundations were laid to enable future\
    \ use of ICU databases in Spain, and a timeline was established for future meetings,\
    \ as an example of how big data analysis tools have tremendous potential in our\
    \ field.\nResumen\nLa aparici\xC3\xB3n de los sistemas de informaci\xC3\xB3n cl\xC3\
    \xADnica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad\
    \ de almacenar una ingente cantidad de datos cl\xC3\xADnicos en formato electr\xC3\
    \xB3nico durante el ingreso de los pacientes. Estos datos pueden ser empleados\
    \ posteriormente para obtener respuestas a preguntas cl\xC3\xADnicas, para su\
    \ uso en la gesti\xC3\xB3n de recursos o para sugerir l\xC3\xADneas de investigaci\xC3\
    \xB3n que luego pueden ser explotadas mediante ensayos cl\xC3\xADnicos aleatorizados.\
    \ Sin embargo, los m\xC3\xA9dicos cl\xC3\xADnicos carecen de la formaci\xC3\xB3\
    n necesaria para la explotaci\xC3\xB3n de grandes bases de datos, lo que supone\
    \ un obst\xC3\xA1culo para aprovechar esta oportunidad. Adem\xC3\xA1s, existen\
    \ cuestiones de \xC3\xADndole legal (seguridad, privacidad, consentimiento de\
    \ los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta.\
    \ El trabajo multidisciplinar con otros profesionales (analistas de datos, estad\xC3\
    \xADsticos, epidemi\xC3\xB3logos, especialistas en derecho aplicado a grandes\
    \ bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta\
    \ para investigaci\xC3\xB3n cl\xC3\xADnica o an\xC3\xA1lisis de resultados (benchmarking).\
    \ Se describe la reuni\xC3\xB3n multidisciplinar (Critical Care Datathon) realizada\
    \ en Madrid los d\xC3\xADas 1, 2 y 3 de diciembre de 2017. Esta reuni\xC3\xB3\
    n, celebrada bajo los auspicios de la Sociedad Espa\xC3\xB1ola de Medicina Intensiva,\
    \ Cr\xC3\xADtica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada\
    \ por el Massachusetts Institute of Technology (MIT), la Unidad de Innovaci\xC3\
    \xB3n y el Servicio de Medicina Intensiva del Hospital Cl\xC3\xADnico San Carlos,\
    \ as\xC3\xAD como el grupo de investigaci\xC3\xB3n \xC2\xABLife Supporting Technologies\xC2\
    \xBB de la Universidad Polit\xC3\xA9cnica de Madrid. Tras unas ponencias de formaci\xC3\
    \xB3n sobre big data, seguridad y calidad de los datos, y su aplicaci\xC3\xB3\
    n al entorno de la medicina intensiva, un grupo de cl\xC3\xADnicos, analistas\
    \ de datos, estad\xC3\xADsticos, expertos en seguridad inform\xC3\xA1tica de datos\
    \ realizaron sesiones de trabajo colaborativo en grupos utilizando una base de\
    \ datos reales anonimizada (MIMIC III), para analizar varias preguntas cl\xC3\xAD\
    nicas establecidas previamente a la reuni\xC3\xB3n. El trabajo colaborativo permiti\xC3\
    \xB3 establecer resultados relevantes con respecto a las preguntas planteadas\
    \ y esbozar varias l\xC3\xADneas de investigaci\xC3\xB3n cl\xC3\xADnica a desarrollar\
    \ en el futuro. Adem\xC3\xA1s, se sentaron las bases para poder utilizar las bases\
    \ de datos de las UCI con las que contamos en Espa\xC3\xB1a, y se estableci\xC3\
    \xB3 un calendario de trabajo para planificar futuras reuniones contando con los\
    \ datos de nuestras unidades. El empleo de herramientas de big data y el trabajo\
    \ colaborativo con otros profesionales puede permitir ampliar los horizontes en\
    \ aspectos como el control de calidad de nuestra labor cotidiana, la comparaci\xC3\
    \xB3n de resultados entre unidades o la elaboraci\xC3\xB3n de nuevas l\xC3\xAD\
    neas de investigaci\xC3\xB3n cl\xC3\xADnica."
  Author: "A. {N\xC3\xBA\xC3\xB1ez Reiz}"
  Book Title/Journal: Medicina Intensiva (English Edition)
  DOI: https://doi.org/10.1016/j.medine.2018.06.006
  JCS_FACTOR: 0.0
  Keywords: "Big data, Machine learning, Artificial intelligence, Clinical databases,\
    \ MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases\
    \ de datos cl\xC3\xADnicos, MIMIC III, Datathon, Trabajo colaborativo"
  SCI_FACTOR: 0.0
  TITLE_UPPER: MEDICINA INTENSIVA (ENGLISH EDITION)
  Title: 'Big data and machine learning in critical care: Opportunities for collaborative
    research'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "An accurate estimation of population exposure to particulate matter with\
    \ an aerodynamic diameter <2.5\xE2\u20AC\xAF\xCE\xBCm (PM2.5) is crucial to hazard\
    \ assessment and epidemiology. This study integrated annual data from 1146 in-home\
    \ air monitors, air quality monitoring network, public applications, and traffic\
    \ smart cards to determine the pattern of PM2.5 concentrations and activities\
    \ in different microenvironments (including outdoors, indoors, subways, buses,\
    \ and cars). By combining massive amounts of signaling data from cell phones,\
    \ this study applied a spatio-temporally weighted model to improve the estimation\
    \ of PM2.5 exposure. Using Shanghai as a case study, the annual average indoor\
    \ PM2.5 concentration was estimated to be 29.3\xE2\u20AC\xAF\xC2\xB1\xE2\u20AC\
    \xAF27.1\xE2\u20AC\xAF\xCE\xBCg/m3 (n\xE2\u20AC\xAF=\xE2\u20AC\xAF365), with an\
    \ average infiltration factor of 0.63. The spatio-temporally weighted PM2.5 exposure\
    \ was estimated to be 32.1\xE2\u20AC\xAF\xC2\xB1\xE2\u20AC\xAF13.9\xE2\u20AC\xAF\
    \xCE\xBCg/m3 (n\xE2\u20AC\xAF=\xE2\u20AC\xAF365), with indoor PM2.5 contributing\
    \ the most (85.1%), followed by outdoor (7.6%), bus (3.7%), subway (3.1%), and\
    \ car (0.5%). However, considering that outdoor PM2.5 makes a significant contribution\
    \ to indoor PM2.5, outdoor PM2.5 was responsible for most of the exposure in Shanghai.\
    \ A heatmap of PM2.5 exposure indicated that the inner-city exposure index was\
    \ significantly higher than that of the outskirts city, which demonstrated that\
    \ the importance of spatial differences in population exposure estimation."
  Author: YuJie Ben and FuJun Ma and Hao Wang and Muhammad Azher Hassan and Romanenko
    Yevheniia and WenHong Fan and Yubiao Li and ZhaoMin Dong
  Book Title/Journal: Environmental Pollution
  DOI: https://doi.org/10.1016/j.envpol.2019.07.034
  JCS_FACTOR: 8.071
  Keywords: Exposure assessment, Indoor PM, Ambient PM, In-home monitors, Shanghai
  SCI_FACTOR: 2.136
  TITLE_UPPER: ENVIRONMENTAL POLLUTION
  Title: 'A spatio-temporally weighted hybrid model to improve estimates of personal
    PM2.5 exposure: Incorporating big data from multiple data sources'
  Title_JCS: ENVIRONMENTAL POLLUTION
  Title_SCI: Environmental Pollution
  Type Publication: article
  Year: 2019
- Abstract: Internet of Things (IoT) analytics is an essential mean to derive knowledge
    and support applications for smart homes. Connected appliances and devices inside
    the smart home produce a significant amount of data about consumers and how they
    go about their daily activities. IoT analytics can aid in personalizing applications
    that benefit both homeowners and the ever growing industries that need to tap
    into consumers profiles. This article presents a new platform that enables innovative
    analytics on IoT captured data from smart homes. We propose the use of fog nodes
    and cloud system to allow data-driven services and address the challenges of complexities
    and resource demands for online and offline data processing, storage, and classification
    analysis. We discuss in this paper the requirements and the design components
    of the system. To validate the platform and present meaningful results, we present
    a case study using a dataset acquired from real smart home in Vancouver, Canada.
    The results of the experiments show clearly the benefit and practicality of the
    proposed platform.
  Author: Abdulsalam Yassine and Shailendra Singh and M. Shamim Hossain and Ghulam
    Muhammad
  Book Title/Journal: Future Generation Computer Systems
  DOI: https://doi.org/10.1016/j.future.2018.08.040
  JCS_FACTOR: 0.0
  Keywords: Internet of Things (IoT), Cloud computing, Fog computing, Big data analytics,
    Energy management, Smart homes
  SCI_FACTOR: 1.262
  TITLE_UPPER: FUTURE GENERATION COMPUTER SYSTEMS
  Title: IoT big data analytics for smart homes with fog and cloud computing
  Title_JCS: N/A
  Title_SCI: Future Generation Computer Systems
  Type Publication: article
  Year: 2019
- Abstract: "In the era of pervasive computing, human living has become smarter by\
    \ the latest advancements in IoMT (Internet of Medical Things), wearable sensors\
    \ and telecommunication technologies in order to deliver smart healthcare services.\
    \ IoMT has the potential to revolutionize the healthcare industry. IoMT interconnects\
    \ wearable sensors, patients, healthcare providers and caregivers via software\
    \ and ICT (Information and Communication Technology). AAL (Ambient Assisted Living)\
    \ enables integration of new technologies to be part of our daily life activities.\
    \ In this paper, we have provided a novel smart healthcare framework for AAL to\
    \ monitor the physical activities of elderly people using IoMT and intelligent\
    \ machine learning algorithms for faster analysis, decision making and better\
    \ treatment recommendations. Data is collected from multiple wearable sensors\
    \ placed on subject\xE2\u20AC\u2122s left ankle, right arm, and chest, is transmitted\
    \ through IoMT devices to the integrated cloud and data analytics layer. To process\
    \ huge amounts of data in parallel, Hadoop MapReduce techniques are used. Multinomial\
    \ Na\xC3\xAFve Bayes classifier, which fits into the MapReduce paradigm, is utilized\
    \ to recognize the motion experienced by different body parts and provides higher\
    \ scalability and better performance with parallel processing when compared to\
    \ serial processor. Our proposed framework predicts 12 physical activities with\
    \ an overall accuracy of 97.1%. This can be considered as an optimal solution\
    \ for recognizing physical activities to remotely monitor health conditions of\
    \ elderly people."
  Author: Liyakathunisa Syed and Saima Jabeen and Manimala S. and Abdullah Alsaeedi
  Book Title/Journal: Future Generation Computer Systems
  DOI: https://doi.org/10.1016/j.future.2019.06.004
  JCS_FACTOR: 0.0
  Keywords: Ambient Assisted Living (AAL), Big data analytics, Internet of Medical
    Things (IoMT), Machine learning techniques, Physical activities, Wearable sensors
  SCI_FACTOR: 1.262
  TITLE_UPPER: FUTURE GENERATION COMPUTER SYSTEMS
  Title: Smart healthcare framework for ambient assisted living using IoMT and big
    data analytics techniques
  Title_JCS: N/A
  Title_SCI: Future Generation Computer Systems
  Type Publication: article
  Year: 2019
- Abstract: The smart grid applications are related with monitoring and control operations
    of conventional power grid. The integration of information and communication technologies
    (ICT) to existing power network has leveraged interaction of different generators,
    controllers, monitoring and measurement devices, and intelligent loads. The two-way
    communication infrastructure is comprised by numerous sensor networks that increased
    deployment of massive data from measurement nodes to monitoring centers. Big data
    is a widespread concept which become a trend for massive data streams that are
    transferred and processed in an ecosystem. The enormous amount of data are generated,
    transferred and stored to improve operating and management quality of smart grid.
    The big data analytics are performed to improve quality of service in terms of
    grid operators and consumers. The big data acquisition, processing, storing, and
    clustering stages are widely researched by a wide variety of specialist. In this
    chapter, the all these stages, big data acquisition technologies, machine learning
    methods used in big data analytics, privacy and security of big data infrastructure
    are introduced in detail. The privacy preserving methods, big data processing
    technologies and firmware infrastructures are presented in the context of this
    chapter.
  Author: Ersan Kabalci and Yasin Kabalci
  Book Title/Journal: From Smart Grid to Internet of Energy
  DOI: https://doi.org/10.1016/B978-0-12-819710-3.00008-9
  JCS_FACTOR: 0.0
  Keywords: Internet of things (IoT), Machine-to-machine communication (M2M), Human
    to machine (H2M), Machine learning, Smart grid security, Hadoop, Data mining,
    Security, Privacy
  SCI_FACTOR: 0.0
  TITLE_UPPER: FROM SMART GRID TO INTERNET OF ENERGY
  Title: Chapter 8 - Big data, privacy and security in smart grids
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: We examine the existing goals of business- and geographic - information
    systems and their influence on logistics and supply chain management systems.
    Modelling supply chain management systems is held back because of lack of consistent
    and poorly aligned data with supply chain elements and processes. The issues constraining
    the decision-making process limit the connectivity between supply chains and geographically
    controlled database systems. The heterogeneous and unstructured data are added
    challenges to connectivity and integration processes. The research focus is on
    analysing the data heterogeneity and multidimensionality relevant to supply chain
    systems and geographically controlled databases. In pursuance of the challenges,
    a unified methodological framework is designed with data structuring, data warehousing
    and mining, visualization and interpretation artefacts to support connectivity
    and integration process. Multidimensional ontologies, ecosystem conceptualization
    and Big Data novelty are added motivations, facilitating the relationships between
    events of supply chain operations. The models construed for optimizing the resources
    are analysed in terms of effectiveness of the integrated framework articulations
    in global supply chains that obey laws of geography. The integrated articulations
    analysed with laws of geography can affect the operational costs, sure for better
    with reduced lead times and enhanced stock management.
  Author: Shastri L Nimmagadda and Torsten Reiners and Lincoln C Wood
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.09.284
  JCS_FACTOR: 0.0
  Keywords: Supply Chain Management, Project Management, Laws of Geography, Domain
    Ontologies, Data Mining
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: On Modelling Big Data Guided Supply Chains in Knowledge-Base Geographic Information
    Systems
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: The construction of medical big data includes several problems that need
    to be solved, such as integration and data sharing of many heterogeneous information
    systems, efficient processing and analysis of large-scale medical data with complex
    structure or low degree of structure, and narrow application range of medical
    data. Therefore, medical big data construction is not only a simple collection
    and application of medical data but also a complex systematic project. This paper
    introduces China's experience in the construction of a regional medical big data
    ecosystem, including the overall goal of the project; establishment of policies
    to encourage data sharing; handling the relationship between personal privacy,
    information security, and information availability; establishing a cooperation
    mechanism between agencies; designing a polycentric medical data acquisition system;
    and establishing a large data centre. From the experience gained from one of China's
    earliest established medical big data projects, we outline the challenges encountered
    during its development and recommend approaches to overcome these challenges to
    design medical big data projects in China more rationally. Clear and complete
    top-level design of a project requires to be planned in advance and considered
    carefully. It is essential to provide a culture of information sharing and to
    facilitate the opening of data, and changes in ideas and policies need the guidance
    of the government. The contradiction between data sharing and data security must
    be handled carefully, that is not to say data openness could be abandoned. The
    construction of medical big data involves many institutions, and high-level management
    and cooperation can significantly improve efficiency and promote innovation. Compared
    with infrastructure construction, it is more challenging and time-consuming to
    develop appropriate data standards, data integration tools and data mining tools.
  Author: Bei Li and Jianbin Li and Yuqiao Jiang and Xiaoyun Lan
  Book Title/Journal: Journal of Biomedical Informatics
  DOI: https://doi.org/10.1016/j.jbi.2019.103149
  JCS_FACTOR: 6.317
  Keywords: Medical big data, Data sharing, Information security, Cooperation mechanism,
    Medical data acquisition, Medical data centre
  SCI_FACTOR: 1.057
  TITLE_UPPER: JOURNAL OF BIOMEDICAL INFORMATICS
  Title: "Experience and reflection from China\xE2\u20AC\u2122s Xiangya medical big\
    \ data project"
  Title_JCS: JOURNAL OF BIOMEDICAL INFORMATICS
  Title_SCI: Journal of Biomedical Informatics
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Walter Verbrugghe and Kirsten Colpaert
  Book Title/Journal: Journal of Critical Care
  DOI: https://doi.org/10.1016/j.jcrc.2019.09.005
  JCS_FACTOR: 3.425
  Keywords: empty
  SCI_FACTOR: 1.149
  TITLE_UPPER: JOURNAL OF CRITICAL CARE
  Title: 'The electronic medical record: Big data, little information?'
  Title_JCS: JOURNAL OF CRITICAL CARE
  Title_SCI: Journal of Critical Care
  Type Publication: article
  Year: 2019
- Abstract: Today, big data processing has become a challenging task due to the amount
    of data collected using various sensors increasingly significantly. To build knowledge
    and predict the data, traditional data mining methods calculate all numerical
    attributes into the memory simultaneously. The data stream method is a solution
    for processing and calculating data. The method streams incrementally in batch
    form; therefore, infrastructure memory is sufficient to develop knowledge. The
    existing method for data stream prediction is FIMT-DD (Fast Incremental Model
    Tree-Drift Detection). Using this method, knowledge is developed in tree form
    for every instance. In this paper, enhanced FIMT-DD is proposed using ARDEV (Average
    Restrain Divider of Evaluation Value). ARDEV utilizes the Chernoff bound approach
    with error evaluation, improvement in learning rate, modification of perceptron
    rule calculation, and utilization of activation function. Standard FIMT-DD separates
    the tree formation process and perceptron prediction. The proposed method evaluates
    and connects the development of the tree for knowledge formation and the perceptron
    rule for prediction. The prediction accuracy of the proposed method is measured
    using MAE, RMSE and MAPE. From the experiment performed, the utilization of ARDEV
    enhancement shows significant improvement in terms of accuracy prediction. Statistically,
    the overall accuracy prediction improvement is approximately 6.99 % compared to
    standard FIMT-DD with a traffic dataset.
  Author: Ari Wibisono and Devvi Sarwinda
  Book Title/Journal: Knowledge-Based Systems
  DOI: https://doi.org/10.1016/j.knosys.2019.03.019
  JCS_FACTOR: 8.038
  Keywords: ARDEV, Big data prediction, FIMT-DD, Tree regression
  SCI_FACTOR: 1.587
  TITLE_UPPER: KNOWLEDGE-BASED SYSTEMS
  Title: Average Restrain Divider of Evaluation Value (ARDEV) in data stream algorithm
    for big data prediction
  Title_JCS: KNOWLEDGE-BASED SYSTEMS
  Title_SCI: Knowledge-Based Systems
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Susanna K.P. Lau and Patrick C.Y. Woo
  Book Title/Journal: Diagnostic Microbiology and Infectious Disease
  DOI: https://doi.org/10.1016/j.diagmicrobio.2018.12.006
  JCS_FACTOR: 2.803
  Keywords: empty
  SCI_FACTOR: 1.027
  TITLE_UPPER: DIAGNOSTIC MICROBIOLOGY AND INFECTIOUS DISEASE
  Title: 'Pitfalls in big data analysis: next-generation technologies, last-generation
    data'
  Title_JCS: DIAGNOSTIC MICROBIOLOGY AND INFECTIOUS DISEASE
  Title_SCI: Diagnostic Microbiology and Infectious Disease
  Type Publication: article
  Year: 2019
- Abstract: Any given health system needs to increase efficiency and effectiveness
    up to the point of requiring a transformation of their current model to ensure
    their sustainability and continuity. The electronic medical record (EMR) is the
    main source of knowledge to improve the quality of healthcare, clinical research,
    epidemiological surveillance, patient empowerment, personalized medicine, and
    clinical decision-making support systems. There is also a huge amount of available
    information related to diseases and other medical conditions, such as drugs and
    therapies, omics data (genetic and proteomic), social networks, and wearable devices.
    Big Data technologies allow the processing of this data to reach the final goal,
    which is a learning health system. The great diversity of data, sources, structures,
    and uses requires a data linkage procedure to integrate and harmonize these data.
    This generation of knowledge allows the transition from evidence-based medicine,
    which still prevails, to practice-based medicine. The key points for any Big Data
    project based on EMRs and other medical information sources are semantic interoperability,
    data structure and granularity, information quality, patient privacy, legal framework,
    and bioethics.
  Author: Javier Carnicero and David Rojas
  Book Title/Journal: Leveraging Biomedical and Healthcare Data
  DOI: https://doi.org/10.1016/B978-0-12-809556-0.00008-3
  JCS_FACTOR: 0.0
  Keywords: Big Data, Electronic medical record, Practice-based medicine, Learning
    health system, Semantic interoperability
  SCI_FACTOR: 0.0
  TITLE_UPPER: LEVERAGING BIOMEDICAL AND HEALTHCARE DATA
  Title: 'Chapter 8 - Healthcare Decision-Making Support Based on the Application
    of Big Data to Electronic Medical Records: A Knowledge Management Cycle'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: 'ABSTRACT

    The information-rich vessel movement data provided by the Automatic Identification
    System (AIS) has gained much popularity over the past decade, during which the
    employment of satellite-based receivers has enabled wide coverage and improved
    data quality. The application of AIS data has developed from simply navigation-oriented
    research to now include trade flow estimation, emission accounting, and vessel
    performance monitoring. The AIS now provides high frequency, real-time positioning
    and sailing patterns for almost the whole world''s commercial fleet, and therefore,
    in combination with supplementary databases and analyses, AIS data has arguably
    kickstarted the era of digitisation in the shipping industry. In this study, we
    conduct a comprehensive review of the literature regarding AIS applications by
    dividing it into three development stages, namely, basic application, extended
    application, and advanced application. Each stage contains two to three application
    fields, and in total we identified seven application fields, including (1) AIS
    data mining, (2) navigation safety, (3) ship behaviour analysis, (4) environmental
    evaluation, (5) trade analysis, (6) ship and port performance, and (7) Arctic
    shipping. We found that the original application of AIS data to navigation safety
    has, with the improvement of data accessibility, evolved into diverse applications
    in various directions. Moreover, we summarised the major methodologies in the
    literature into four categories, these being (1) data processing and mining, (2)
    index measurement, (3) causality analysis, and (4) operational research. Undoubtedly,
    the applications of AIS data will be further expanded in the foreseeable future.
    This will not only provide a more comprehensive understanding of voyage performance
    and allow researchers to examine shipping market dynamics from the micro level,
    but also the abundance of AIS data may also open up the rather opaque aspect of
    how shipping companies release information to external authorities, including
    the International Maritime Organization, port states, scientists and researchers.
    It is expected that more multi-disciplinary AIS studies will emerge in the coming
    years. We believe that this study will shed further light on the future development
    of AIS studies.'
  Author: Dong Yang and Lingxiao Wu and Shuaian Wang and Haiying Jia and Kevin X.
    Li
  Book Title/Journal: Transport Reviews
  DOI: https://doi.org/10.1080/01441647.2019.1649315
  JCS_FACTOR: 9.643
  Keywords: AIS data, data mining, navigation safety, ship behaviour analysis, environmental
    evaluation, advanced applications of AIS data
  SCI_FACTOR: 3.046
  TITLE_UPPER: TRANSPORT REVIEWS
  Title: "How big data enriches maritime research \xE2\u20AC\u201C a critical review\
    \ of Automatic Identification System (AIS) data applications"
  Title_JCS: TRANSPORT REVIEWS
  Title_SCI: Transport Reviews
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and
    H. Li
  Book Title/Journal: Value in Health
  DOI: https://doi.org/10.1016/j.jval.2019.04.303
  JCS_FACTOR: 5.725
  Keywords: empty
  SCI_FACTOR: 1.859
  TITLE_UPPER: VALUE IN HEALTH
  Title: 'PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS'
  Title_JCS: VALUE IN HEALTH
  Title_SCI: Value in Health
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Z.G. Hui and Q. Guo and W.Z. Shi and M.C. Gong and C. Liu and H. Xu and
    H. Li
  Book Title/Journal: Value in Health
  DOI: https://doi.org/10.1016/j.jval.2019.04.303
  JCS_FACTOR: 5.725
  Keywords: empty
  SCI_FACTOR: 1.859
  TITLE_UPPER: VALUE IN HEALTH
  Title: 'PCN181 THE NATIONAL CANCER BIG DATA PLATFORM OF CHINA: VISION AND STATUS'
  Title_JCS: VALUE IN HEALTH
  Title_SCI: Value in Health
  Type Publication: article
  Year: 2019
- Abstract: "The use of data repositories for parameterizing ecological models and\
    \ storing model runs is becoming more common, yet often these data archives do\
    \ not contain the appropriate metadata, nor are they maintained for others to\
    \ use. Data archiving and sharing are additional steps in the scientific process\
    \ that add value to a researcher\xD7\xB3s work, and more importantly, facilitate\
    \ transparency and repeatability of a researcher\xD7\xB3s work. Historically,\
    \ peer-reviewed publications did not allow for the full presentation of underlying\
    \ datasets, which were only shared through personal contact with a scientist.\
    \ However, with the expanding use of \xE2\u20AC\u0153supporting online material\xE2\
    \u20AC\x9D (SOM) files that accompany digital publication there is an increased\
    \ expectation that even large datasets can be made accessible to readers. Thus,\
    \ researchers are faced with the additional task of becoming their own archivist\
    \ and depositing data in a repository where it can be used by others. This article\
    \ introduces basic concepts in data archiving and sharing, including major digital\
    \ repositories for life science data, commonly used digital file formats, and\
    \ why metadata is an essential element to successful data sharing when machine-readable\
    \ data is increasingly used in large-scale studies."
  Author: Marin M. Kress
  Book Title/Journal: Encyclopedia of Ecology (Second Edition)
  DOI: https://doi.org/10.1016/B978-0-12-409548-9.10557-3
  JCS_FACTOR: 0.0
  Keywords: Big data, Crowdsourcing or crowdsourced, Data discovery, Data discovery,
    Data science, Database, Dryad, Environmental health, Interdisciplinary, Machine
    readable, Metadata, Remote sensing, Social media
  SCI_FACTOR: 0.0
  TITLE_UPPER: ENCYCLOPEDIA OF ECOLOGY (SECOND EDITION)
  Title: Big Data for Ecological Models
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: "Rapid advances in the collection, storage, and analysis of large volumes\
    \ of data\xE2\u20AC\u201DBig Data\xE2\u20AC\u201Doffer the much-needed help to\
    \ identify and treat the various pathological conditions triggered by traumatic\
    \ brain injury (TBI). Big Data (BD) is defined as extremely large, complex, and\
    \ mostly unstructured data that cannot be analyzed using traditional approaches.\
    \ BD can be only analyzed by using text mining (TM), artificial intelligence (AI),\
    \ or machine learning (ML). These approaches can reveal patterns, trends, and\
    \ associations, critical for understanding the \xE2\u20AC\u0153most complex disease\
    \ of the most complex organ.\xE2\u20AC\x9D While powerful and successfully tested\
    \ computational tools are available, using BD approaches in TBI is currently hampered\
    \ by the limited availability of legacy and/or primary data, by incompatible data\
    \ formats and standards. This chapter introduces Big Data and Big Data approaches\
    \ such as text mining, artificial intelligence, and machine learning; outlines\
    \ the benefits of using BD approaches; and suggests potential solutions that can\
    \ help using the full potential of BD in TBI. It also identifies necessary changes\
    \ of how researchers can help ushering in a new era of preclinical and clinical\
    \ TBI research by recording and storing ALL the data generated and making ALL\
    \ the data available for BD approaches\xE2\u20AC\u201Dtext mining, artificial\
    \ intelligence, and machine learning so new correlations, relationships, and trends\
    \ can be identified. In turn, these new information will help to develop novel\
    \ diagnostics, evidence-based treatments, and improve outcomes."
  Author: Denes V. Agoston
  Book Title/Journal: Leveraging Biomedical and Healthcare Data
  DOI: https://doi.org/10.1016/B978-0-12-809556-0.00004-6
  JCS_FACTOR: 0.0
  Keywords: Big Data, Artificial intelligence and machine learning in neurotrauma
  SCI_FACTOR: 0.0
  TITLE_UPPER: LEVERAGING BIOMEDICAL AND HEALTHCARE DATA
  Title: Chapter 4 - Big Data, Artificial Intelligence, and Machine Learning in Neurotrauma
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: Deep learning methods are extensively applied to various fields of science
    and engineering such as speech recognition, image classifications, and learning
    methods in language processing. Similarly, traditional data processing techniques
    have several limitations of processing large amount of data. In addition, Big
    Data analytics requires new and sophisticated algorithms based on machine and
    deep learning techniques to process data in real-time with high accuracy and efficiency.
    However, recently, research incorporated various deep learning techniques with
    hybrid learning and training mechanisms of processing data with high speed. Most
    of these techniques are specific to scenarios and based on vector space thus,
    shows poor performance in generic scenarios and learning features in big data.
    In addition, one of the reason of such failure is high involvement of humans to
    design sophisticated and optimized algorithms based on machine and deep learning
    techniques. In this article, we bring forward an approach of comparing various
    deep learning techniques for processing huge amount of data with different number
    of neurons and hidden layers. The comparative study shows that deep learning techniques
    can be built by introducing a number of methods in combination with supervised
    and unsupervised training techniques.
  Author: Bilal Jan and Haleem Farman and Murad Khan and Muhammad Imran and Ihtesham
    Ul Islam and Awais Ahmad and Shaukat Ali and Gwanggil Jeon
  Book Title/Journal: Computers & Electrical Engineering
  DOI: https://doi.org/10.1016/j.compeleceng.2017.12.009
  JCS_FACTOR: 3.818
  Keywords: Big data, Deep learning, Deep belief networks, Convolutional Neural Networks
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMPUTERS & ELECTRICAL ENGINEERING
  Title: 'Deep learning in big data Analytics: A comparative study'
  Title_JCS: COMPUTERS & ELECTRICAL ENGINEERING
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: C.J. Puranik and Sreenivasa Rao and S. Chennamaneni
  Book Title/Journal: The Ocular Surface
  DOI: https://doi.org/10.1016/j.jtos.2019.07.010
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE OCULAR SURFACE
  Title: The perils and pitfalls of big data analysis in medicine
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "The introduction of clinical information systems (CIS) in Intensive Care\
    \ Units (ICUs) offers the possibility of storing a huge amount of machine-ready\
    \ clinical data that can be used to improve patient outcomes and the allocation\
    \ of resources, as well as suggest topics for randomized clinical trials. Clinicians,\
    \ however, usually lack the necessary training for the analysis of large databases.\
    \ In addition, there are issues referred to patient privacy and consent, and data\
    \ quality. Multidisciplinary collaboration among clinicians, data engineers, machine-learning\
    \ experts, statisticians, epidemiologists and other information scientists may\
    \ overcome these problems. A multidisciplinary event (Critical Care Datathon)\
    \ was held in Madrid (Spain) from 1 to 3 December 2017. Under the auspices of\
    \ the Spanish Critical Care Society (SEMICYUC), the event was organized by the\
    \ Massachusetts Institute of Technology (MIT) Critical Data Group (Cambridge,\
    \ MA, USA), the Innovation Unit and Critical Care Department of San Carlos Clinic\
    \ Hospital, and the Life Supporting Technologies group of Madrid Polytechnic University.\
    \ After presentations referred to big data in the critical care environment, clinicians,\
    \ data scientists and other health data science enthusiasts and lawyers worked\
    \ in collaboration using an anonymized database (MIMIC III). Eight groups were\
    \ formed to answer different clinical research questions elaborated prior to the\
    \ meeting. The event produced analyses for the questions posed and outlined several\
    \ future clinical research opportunities. Foundations were laid to enable future\
    \ use of ICU databases in Spain, and a timeline was established for future meetings,\
    \ as an example of how big data analysis tools have tremendous potential in our\
    \ field.\nResumen\nLa aparici\xC3\xB3n de los sistemas de informaci\xC3\xB3n cl\xC3\
    \xADnica (SIC) en el entorno de los cuidados intensivos brinda la posibilidad\
    \ de almacenar una ingente cantidad de datos cl\xC3\xADnicos en formato electr\xC3\
    \xB3nico durante el ingreso de los pacientes. Estos datos pueden ser empleados\
    \ posteriormente para obtener respuestas a preguntas cl\xC3\xADnicas, para su\
    \ uso en la gesti\xC3\xB3n de recursos o para sugerir l\xC3\xADneas de investigaci\xC3\
    \xB3n que luego pueden ser explotadas mediante ensayos cl\xC3\xADnicos aleatorizados.\
    \ Sin embargo, los m\xC3\xA9dicos cl\xC3\xADnicos carecen de la formaci\xC3\xB3\
    n necesaria para la explotaci\xC3\xB3n de grandes bases de datos, lo que supone\
    \ un obst\xC3\xA1culo para aprovechar esta oportunidad. Adem\xC3\xA1s, existen\
    \ cuestiones de \xC3\xADndole legal (seguridad, privacidad, consentimiento de\
    \ los pacientes) que deben ser abordadas para poder utilizar esta potente herramienta.\
    \ El trabajo multidisciplinar con otros profesionales (analistas de datos, estad\xC3\
    \xADsticos, epidemi\xC3\xB3logos, especialistas en derecho aplicado a grandes\
    \ bases de datos), puede resolver estas cuestiones y permitir utilizar esta herramienta\
    \ para investigaci\xC3\xB3n cl\xC3\xADnica o an\xC3\xA1lisis de resultados (benchmarking).\
    \ Se describe la reuni\xC3\xB3n multidisciplinar (Critical Care Datathon) realizada\
    \ en Madrid los d\xC3\xADas 1, 2 y 3 de diciembre de 2017. Esta reuni\xC3\xB3\
    n, celebrada bajo los auspicios de la Sociedad Espa\xC3\xB1ola de Medicina Intensiva,\
    \ Cr\xC3\xADtica y Unidades Coronarias (SEMICYUC) entre otros, fue organizada\
    \ por el Massachusetts Institute of Technology (MIT), la Unidad de Innovaci\xC3\
    \xB3n y el Servicio de Medicina Intensiva del Hospital Cl\xC3\xADnico San Carlos,\
    \ as\xC3\xAD como el grupo de investigaci\xC3\xB3n \xC2\xABLife Supporting Technologies\xC2\
    \xBB de la Universidad Polit\xC3\xA9cnica de Madrid. Tras unas ponencias de formaci\xC3\
    \xB3n sobre big data, seguridad y calidad de los datos, y su aplicaci\xC3\xB3\
    n al entorno de la medicina intensiva, un grupo de cl\xC3\xADnicos, analistas\
    \ de datos, estad\xC3\xADsticos, expertos en seguridad inform\xC3\xA1tica de datos\
    \ realizaron sesiones de trabajo colaborativo en grupos utilizando una base de\
    \ datos reales anonimizada (MIMIC III), para analizar varias preguntas cl\xC3\xAD\
    nicas establecidas previamente a la reuni\xC3\xB3n. El trabajo colaborativo permiti\xC3\
    \xB3 establecer resultados relevantes con respecto a las preguntas planteadas\
    \ y esbozar varias l\xC3\xADneas de investigaci\xC3\xB3n cl\xC3\xADnica a desarrollar\
    \ en el futuro. Adem\xC3\xA1s, se sentaron las bases para poder utilizar las bases\
    \ de datos de las UCI con las que contamos en Espa\xC3\xB1a, y se estableci\xC3\
    \xB3 un calendario de trabajo para planificar futuras reuniones contando con los\
    \ datos de nuestras unidades. El empleo de herramientas de big data y el trabajo\
    \ colaborativo con otros profesionales puede permitir ampliar los horizontes en\
    \ aspectos como el control de calidad de nuestra labor cotidiana, la comparaci\xC3\
    \xB3n de resultados entre unidades o la elaboraci\xC3\xB3n de nuevas l\xC3\xAD\
    neas de investigaci\xC3\xB3n cl\xC3\xADnica."
  Author: "Antonio {N\xC3\xBA\xC3\xB1ez Reiz} and Fernando {Mart\xC3\xADnez Sagasti}\
    \ and Manuel {\xC3\x81lvarez Gonz\xC3\xA1lez} and Antonio {Blesa Malpica} and\
    \ Juan Carlos {Mart\xC3\xADn Ben\xC3\xADtez} and Mercedes {Nieto Cabrera} and\
    \ \xC3\x81ngela {del Pino Ram\xC3\xADrez} and Jos\xC3\xA9 Miguel {Gil Perdomo}\
    \ and Jes\xC3\xBAs {Prada Alonso} and Leo Anthony Celi and Miguel \xC3\x81ngel\
    \ {Armengol de la Hoz} and Rodrigo Deliberato and Kenneth Paik and Tom Pollard\
    \ and Jesse Raffa and Felipe Torres and Julio Mayol and Joan Chafer and Arturo\
    \ {Gonz\xC3\xA1lez Ferrer} and \xC3\x81ngel Rey and Henar {Gonz\xC3\xA1lez Luengo}\
    \ and Giuseppe Fico and Ivana Lombroni and Liss Hernandez and Laura L\xC3\xB3\
    pez and Beatriz Merino and Mar\xC3\xADa Fernanda Cabrera and Mar\xC3\xADa Teresa\
    \ Arredondo and Mar\xC3\xADa Bod\xC3\xAD and Josep G\xC3\xB3mez and Alejandro\
    \ Rodr\xC3\xADguez and Miguel {S\xC3\xA1nchez Garc\xC3\xADa}"
  Book Title/Journal: Medicina Intensiva
  DOI: https://doi.org/10.1016/j.medin.2018.06.002
  JCS_FACTOR: 2.491
  Keywords: "Big data, Machine learning, Artificial intelligence, Clinical databases,\
    \ MIMIC III, Datathon, Collaborative work, , , Inteligencia artificial, Bases\
    \ de datos cl\xC3\xADnicos, MIMIC III, Datathon, Trabajo colaborativo"
  SCI_FACTOR: 0.336
  TITLE_UPPER: MEDICINA INTENSIVA
  Title: 'Big data and machine learning in critical care: Opportunities for collaborative
    research'
  Title_JCS: Medicina Intensiva
  Title_SCI: Medicina Intensiva
  Type Publication: article
  Year: 2019
- Abstract: We describe the infrastructure and functionality for a centralized preclinical
    and clinical data repository and analytic platform to support importing heterogeneous
    multi-modal data, automatically and manually linking data across modalities and
    sites, and searching content. We have developed and applied innovative image and
    electrophysiology processing methods to identify candidate biomarkers from MRI,
    EEG, and multi-modal data. Based on heterogeneous biomarkers, we present novel
    analytic tools designed to study epileptogenesis in animal model and human with
    the goal of tracking the probability of developing epilepsy over time.
  Author: "Dominique Duncan and Paul Vespa and Asla Pitk\xC3\xA4nen and Adebayo Braimah\
    \ and Niina Lapinlampi and Arthur W. Toga"
  Book Title/Journal: Neurobiology of Disease
  DOI: https://doi.org/10.1016/j.nbd.2018.05.026
  JCS_FACTOR: 5.996
  Keywords: Biomarkers, EEG, Epilepsy, Epileptogenesis, Informatics, MRI, Neuroimaging,
    TBI
  SCI_FACTOR: 2.205
  TITLE_UPPER: NEUROBIOLOGY OF DISEASE
  Title: Big data sharing and analysis to advance research in post-traumatic epilepsy
  Title_JCS: NEUROBIOLOGY OF DISEASE
  Title_SCI: Neurobiology of Disease
  Type Publication: article
  Year: 2019
- Abstract: Historically, personalised medicine has been synonymous with pharmacogenomics
    and oncology. We argue for a new framework for personalised medicine analytics
    that capitalises on more detailed patient-level data and leverages recent advances
    in causal inference and machine learning tailored towards decision support applicable
    to critically ill patients. We discuss how advances in data technology and statistics
    are providing new opportunities for asking more targeted questions regarding patient
    treatment, and how this can be applied in the intensive care unit to better predict
    patient-centred outcomes, help in the discovery of new treatment regimens associated
    with improved outcomes, and ultimately how these rules can be learned in real-time
    for the patient.
  Author: Romain Pirracchio and Mitchell J Cohen and Ivana Malenica and Jonathan Cohen
    and Antoine Chambaz and Maxime Cannesson and Christine Lee and Matthieu Resche-Rigon
    and Alan Hubbard
  Book Title/Journal: Anaesthesia Critical Care & Pain Medicine
  DOI: https://doi.org/10.1016/j.accpm.2018.09.008
  JCS_FACTOR: 4.132
  Keywords: empty
  SCI_FACTOR: 0.0
  TITLE_UPPER: ANAESTHESIA CRITICAL CARE & PAIN MEDICINE
  Title: Big data and targeted machine learning in action to assist medical decision
    in the ICU
  Title_JCS: Anaesthesia Critical Care & Pain Medicine
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Physical exercise and sleep have independent, yet synergistic, impacts
    on the health. However, the effects of acute exercise level on sleep quality have
    not been well investigated. We utilize statistical methods to investigate the
    differences of exercise level between the good and bad sleep nights. Our results
    present a complex interrelation between physical exercise and sleep quality with
    analyzing large personal data sets collected from wearables. As far as we know,
    this is the first study to investigate insights of interrelation of physical exercise
    and sleep quality based on a big volume of data collected from wearable devices
    of real users.
  Author: "Xiaoli Liu and Satu Tamminen and Topi Korhonen and Juha R\xC3\xB6ning"
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.08.035
  JCS_FACTOR: 0.0
  Keywords: Data analytics, wearables, sleep quality, statistical methods
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: How Physical Exercise Level Affects Sleep Quality? Analyzing Big Data Collected
    from Wearables
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: "Metabolic phenotyping is entering the era of Big Data, leading to new\
    \ opportunities and challenges. Cloud computing has been proposed as a novel paradigm,\
    \ but as yet is not widely understood or used. In this chapter we introduce the\
    \ concepts of Big Data and cloud computing, and discuss how they might change\
    \ the landscape of metabolic phenotyping and analysis. We highlight some of the\
    \ reasons for the increase in data size and explain advantages and disadvantages\
    \ of large-scale computing in this context. We illustrate the area with a survey\
    \ of software tools and databases currently available, and describe the newly\
    \ developed cloud infrastructure \xE2\u20AC\u0153PhenoMeNal,\xE2\u20AC\x9D which\
    \ will enable widespread use of these approaches. We conclude the chapter with\
    \ a discussion of the important ethical, legal, and social implications (ELSI)\
    \ of large-scale computing in this rapidly developing field."
  Author: Timothy M.D. Ebbels and Jake T.M. Pearce and Noureddin Sadawi and Jianliang
    Gao and Robert C. Glen
  Book Title/Journal: The Handbook of Metabolic Phenotyping
  DOI: https://doi.org/10.1016/B978-0-12-812293-8.00011-6
  JCS_FACTOR: 0.0
  Keywords: Metabolomics, Metabonomics, Metabolic phenotyping, Big data, Cloud computing,
    High-performance computing, Software tools, Databases, PhenoMeNal, Ethical, Legal,
    Social implications, ELSI
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE HANDBOOK OF METABOLIC PHENOTYPING
  Title: Chapter 11 - Big Data and Databases for Metabolic Phenotyping
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: "Background and Objectives\nData Quality (DQ) programs are recognized\
    \ as a critical aspect of new-generation research platforms using electronic health\
    \ record (EHR) data for building Learning Healthcare Systems. The AP-HP Clinical\
    \ Data Repository aggregates EHR data from 37 hospitals to enable large-scale\
    \ research and secondary data analysis. This paper describes the DQ program currently\
    \ in place at AP-HP and the lessons learned from two DQ campaigns initiated in\
    \ 2017.\nMaterials and Methods\nAs part of the AP-HP DQ program, two domains -\
    \ patient identification (PI) and healthcare services (HS) - were selected for\
    \ conducting DQ campaigns consisting of 5 phases: defining the scope, measuring,\
    \ analyzing, improving and controlling DQ. Semi-automated DQ profiling was conducted\
    \ in two data sets \xE2\u20AC\u201C the PI data set containing 8.8\xE2\u20AC\xAF\
    M patients and the HS data set containing 13,099 consultation agendas and 2122\
    \ care units. Seventeen DQ measures were defined and DQ issues were classified\
    \ using a unified DQ reporting framework. For each domain, actions plans were\
    \ defined for improving and monitoring prioritized DQ issues.\nResults\nEleven\
    \ identified DQ issues (8 for the PI data set and 3 for the HS data set) were\
    \ categorized into completeness (n\xE2\u20AC\xAF=\xE2\u20AC\xAF6), conformance\
    \ (n\xE2\u20AC\xAF=\xE2\u20AC\xAF3) and plausibility (n\xE2\u20AC\xAF=\xE2\u20AC\
    \xAF2) DQ issues. DQ issues were caused by errors from data originators, ETL issues\
    \ or limitations of the EHR data entry tool. The action plans included sixteen\
    \ actions (9 for the PI domain and 7 for the HS domain). Though only partial implementation,\
    \ the DQ campaigns already resulted in significant improvement of DQ measures.\n\
    Conclusion\nDQ assessments of hospital information systems are largely unpublished.\
    \ The preliminary results of two DQ campaigns conducted at AP-HP illustrate the\
    \ benefit of the engagement into a DQ program. The adoption of a unified DQ reporting\
    \ framework enables the communication of DQ findings in a well-defined manner\
    \ with a shared vocabulary. Dedicated tooling is needed to automate and extend\
    \ the scope of the generic DQ program. Specific DQ checks will be additionally\
    \ defined on a per-study basis to evaluate whether EHR data fits for specific\
    \ uses."
  Author: "Christel Daniel and Patricia Serre and Nina Orlova and St\xC3\xA9phane\
    \ Br\xC3\xA9ant and Nicolas Paris and Nicolas Griffon"
  Book Title/Journal: Computer Methods and Programs in Biomedicine
  DOI: https://doi.org/10.1016/j.cmpb.2018.10.016
  JCS_FACTOR: 5.428
  Keywords: Data accuracy, Data quality, Electronic health records, Data warehousing,
    Observational Studies as Topic
  SCI_FACTOR: 0.924
  TITLE_UPPER: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title: Initializing a hospital-wide data quality program. The AP-HP experience.
  Title_JCS: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title_SCI: Computer Methods and Programs in Biomedicine
  Type Publication: article
  Year: 2019
- Abstract: The Iowa Flood Center (IFC) developed a pilot infrastructure to explore
    rainfall metadata (descriptive statistics) and generate rainfall products over
    the Iowa domain based on the NEXRAD Level II data directly accessible through
    cloud storage (e.g., Amazon Web Services). Known as IFC-Cloud-NEXRAD, it resembles
    the Hydro-NEXRAD portal that provided researchers with ready access to NEXRAD
    radar data. Taking advantage of the cloud storage benefits (unlimited storage
    and instant access), IFC-Cloud-NEXRAD reduces the common challenges of most data
    exploration systems, which often lead to massive data acquisition/ingestion and
    rapid filling of limited system storage. Its map-based interface allows researchers
    to select a space-time domain of interest, retrieve and visualize pre-calculated
    rainfall metadata, and generate radar-derived rainfall products. Because the system
    provides generalized approaches to compute metadata and process data for rainfall
    estimation, the framework presented in this study would be readily transferrable
    to other geographic regions and larger scale applications.
  Author: Bong-Chul Seo and Munsung Keem and Raymond Hammond and Ibrahim Demir and
    Witold F. Krajewski
  Book Title/Journal: Environmental Modelling & Software
  DOI: https://doi.org/10.1016/j.envsoft.2019.03.008
  JCS_FACTOR: 5.288
  Keywords: NEXRAD, Rainfall, Cloud computing, Level II data, Hydrology
  SCI_FACTOR: 0.0
  TITLE_UPPER: ENVIRONMENTAL MODELLING & SOFTWARE
  Title: A pilot infrastructure for searching rainfall metadata and generating rainfall
    product using the big data of NEXRAD
  Title_JCS: ENVIRONMENTAL MODELLING & SOFTWARE
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "Background and Objectives\nData curation is a tedious task but of paramount\
    \ relevance for data analytics and more specially in the health context where\
    \ data-driven decisions must be extremely accurate. The ambition of TAQIH is to\
    \ support non-technical users on 1) the exploratory data analysis (EDA) process\
    \ of tabular health data, and 2) the assessment and improvement of its quality.\n\
    Methods\nA web-based tool has been implemented with a simple yet powerful visual\
    \ interface. First, it provides interfaces to understand the dataset, to gain\
    \ the understanding of the content, structure and distribution. Then, it provides\
    \ data visualization and improvement utilities for the data quality dimensions\
    \ of completeness, accuracy, redundancy and readability.\nResults\nIt has been\
    \ applied in two different scenarios. (1) The Northern Ireland General Practitioners\
    \ (GPs) Prescription Data, an open data set containing drug prescriptions. (2)\
    \ A glucose monitoring tele health system dataset. Findings on (1) include: Features\
    \ that had significant amount of missing values (e.g. AMP_NM variable 53.39%);\
    \ instances that have high percentage of variable values missing (e.g. 0.21% of\
    \ the instances with\xE2\u20AC\xAF>\xE2\u20AC\xAF75% of missing values); highly\
    \ correlated variables (e.g. Gross and Actual cost almost completely correlated\
    \ (\xE2\u02C6\xBC\xE2\u20AC\xAF+\xE2\u20AC\xAF1.0)). Findings on (2) include:\
    \ Features that had significant amount of missing values (e.g. patient height,\
    \ weight and body mass index (BMI) (>\xE2\u20AC\xAF70%), date of diagnosis 13%));\
    \ highly correlated variables (e.g. height, weight and BMI). Full detail of the\
    \ testing and insights related to findings are reported.\nConclusions\nTAQIH enables\
    \ and supports users to carry out EDA on tabular health data and to assess and\
    \ improve its quality. Having the layout of the application menu arranged sequentially\
    \ as the conventional EDA pipeline helps following a consistent analysis process.\
    \ The general description of the dataset and features section is very useful for\
    \ the first overview of the dataset. The missing value heatmap is also very helpful\
    \ in visually identifying correlations among missing values. The correlations\
    \ section has proved to be supportive as a preliminary step before further data\
    \ analysis pipelines, as well as the outliers section. Finally, the data quality\
    \ section provides a quantitative value to the dataset improvements."
  Author: "Roberto {\xC3\x81lvarez S\xC3\xA1nchez} and Andoni {Beristain Iraola} and\
    \ Gorka {Epelde Unanue} and Paul Carlin"
  Book Title/Journal: Computer Methods and Programs in Biomedicine
  DOI: https://doi.org/10.1016/j.cmpb.2018.12.029
  JCS_FACTOR: 5.428
  Keywords: Data quality, Exploratory data analysis, Data pre-processing
  SCI_FACTOR: 0.924
  TITLE_UPPER: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title: TAQIH, a tool for tabular data quality assessment and improvement in the
    context of health data
  Title_JCS: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title_SCI: Computer Methods and Programs in Biomedicine
  Type Publication: article
  Year: 2019
- Abstract: "Purpose\nTo prepare for big data analyses on radiation therapy data,\
    \ we developed Stature, a tool-supported approach for standardization of structure\
    \ names in existing radiation therapy plans. We applied the widely endorsed nomenclature\
    \ standard TG-263 as the mapping target and quantified the structure name inconsistency\
    \ in 2 real-world data sets.\nMethods and Materials\nThe clinically relevant structures\
    \ in the radiation therapy plans were identified by reference to randomized controlled\
    \ trials. The Stature approach was used by clinicians to identify the synonyms\
    \ for each relevant structure, which was then mapped to the corresponding TG-263\
    \ name. We applied Stature to standardize the structure names for 654 patients\
    \ with prostate cancer (PCa) and 224 patients with head and neck squamous cell\
    \ carcinoma (HNSCC) who received curative radiation therapy at our institution\
    \ between 2007 and 2017. The accuracy of the Stature process was manually validated\
    \ in a random sample from each cohort. For the HNSCC cohort we measured the resource\
    \ requirements for Stature, and for the PCa cohort we demonstrated its impact\
    \ on an example clinical analytics scenario.\nResults\nAll but 1 synonym group\
    \ (\xE2\u20AC\u0153Hydrogel\xE2\u20AC\x9D) was mapped to the corresponding TG-263\
    \ name, resulting in a TG-263 relabel rate of 99% (8837 of 8925 structures). For\
    \ the PCa cohort, Stature matched a total of 5969 structures. Of these, 5682 structures\
    \ were exact matches (ie, following local naming convention), 284 were matched\
    \ via a synonym, and 3 required manual matching. This original radiation therapy\
    \ structure names therefore had a naming inconsistency rate of 4.81%. For the\
    \ HNSCC cohort, Stature mapped a total of 2956 structures (2638 exact, 304 synonym,\
    \ 14 manual; 10.76% inconsistency rate) and required 7.5 clinician hours. The\
    \ clinician hours required were one-fifth of those that would be required for\
    \ manual relabeling. The accuracy of Stature was 99.97% (PCa) and 99.61% (HNSCC).\n\
    Conclusions\nThe Stature approach was highly accurate and had significant resource\
    \ efficiencies compared with manual curation."
  Author: Thilo Schuler and John Kipritidis and Thomas Eade and George Hruby and Andrew
    Kneebone and Mario Perez and Kylie Grimberg and Kylie Richardson and Sally Evill
    and Brooke Evans and Blanca Gallego
  Book Title/Journal: Advances in Radiation Oncology
  DOI: https://doi.org/10.1016/j.adro.2018.09.013
  JCS_FACTOR: 0.0
  Keywords: empty
  SCI_FACTOR: 0.989
  TITLE_UPPER: ADVANCES IN RADIATION ONCOLOGY
  Title: 'Big Data Readiness in Radiation Oncology: An Efficient Approach for Relabeling
    Radiation Therapy Structures With Their TG-263 Standard Name in Real-World Data
    Sets'
  Title_JCS: N/A
  Title_SCI: Advances in Radiation Oncology
  Type Publication: article
  Year: 2019
- Abstract: Industrial enterprises rely on prediction of market behavior, monitoring
    of performance measures, evaluation of production processes and other data analyses
    to support strategic and operational decisions. However, although an adequate
    data quality (DQ) is essential for any data analysis and several methodologies
    for DQ assessment exist, not all organizations consider DQ in decision-making
    processes. E.g., inaccurate and delayed data acquisition leads to imprecise master
    data and poor knowledge of machine utilization. While these aspects should influence
    production planning and control, current approaches to data evaluation are too
    complex to use them on a-day-to-day basis. In this paper, we propose a methodology
    that simplifies the execution of DQ evaluations and improves the understandability
    of its results. One of its main concerns is to make DQ assessment usable to small
    and medium-sized enterprises (SME). The approach takes selected, context related
    structured or semi-structured data as input and uses a set of generic test criteria
    applicable to different tasks and domains. It combines data and domain driven
    aspects and can be partly executed automated and without context specific domain
    knowledge. The results of the assessment can be summarized into quality dimensions
    and used for benchmarking. The methodology is validated using data from the enterprise
    resource planning (ERP) and manufacturing execution system (MES) of a sheet metal
    manufacturer covering a year of time. The particular application aims at calculating
    logistic key performance indicators. Based on these conditions, data requirements
    are defined and the available data is evaluated considering domain specific characteristics.
  Author: "Lisa C. G\xC3\xBCnther and Eduardo Colangelo and Hans-Hermann Wiendahl\
    \ and Christian Bauer"
  Book Title/Journal: Procedia Manufacturing
  DOI: https://doi.org/10.1016/j.promfg.2019.02.114
  JCS_FACTOR: 0.0
  Keywords: Data quality assessment, Data quality control, Information quality, Benchmarking,
    Production planning, control
  SCI_FACTOR: 0.504
  TITLE_UPPER: PROCEDIA MANUFACTURING
  Title: 'Data quality assessment for improved decision-making: a methodology for
    small and medium-sized enterprises'
  Title_JCS: N/A
  Title_SCI: Procedia Manufacturing
  Type Publication: article
  Year: 2019
- Abstract: "In the past few decades, extensive epidemiological studies have focused\
    \ on exploring the adverse effects of PM2.5 (particulate matters with aerodynamic\
    \ diameters less than 2.5\xE2\u20AC\xAF\xCE\xBCm) on public health. However, most\
    \ of them failed to consider the dynamic changes of population distribution adequately\
    \ and were limited by the accuracy of PM2.5 estimations. Therefore, in this study,\
    \ location-based service (LBS) data from social media and satellite-derived high-quality\
    \ PM2.5 concentrations were collected to perform highly spatiotemporal exposure\
    \ assessments for thirteen cities in the Beijing-Tianjin-Hebei (BTH) region, China.\
    \ The city-scale exposure levels and the corresponding health outcomes were first\
    \ estimated. Then the uncertainties in exposure risk assessments were quantified\
    \ based on in-situ PM2.5 observations and static population data. The results\
    \ showed that approximately half of the population living in the BTH region were\
    \ exposed to monthly mean PM2.5 concentration greater than 80\xE2\u20AC\xAF\xCE\
    \xBCg/m3 in 2015, and the highest risk was observed in December. In terms of all-cause,\
    \ cardiovascular, and respiratory disease, the premature deaths attributed to\
    \ PM2.5 were estimated to be 138,150, 80,945, and 18,752, respectively. A comparative\
    \ analysis between five different exposure models further illustrated that the\
    \ dynamic population distribution and accurate PM2.5 estimations showed great\
    \ influence on environmental exposure and health assessments and need be carefully\
    \ considered. Otherwise, the results would be considerably over- or under-estimated."
  Author: Yimeng Song and Bo Huang and Qingqing He and Bin Chen and Jing Wei and Rashed
    Mahmood
  Book Title/Journal: Environmental Pollution
  DOI: https://doi.org/10.1016/j.envpol.2019.06.057
  JCS_FACTOR: 8.071
  Keywords: Human mobility, Spatiotemporal heterogeneity, Remote sensing, Big data,
    Environmental health
  SCI_FACTOR: 2.136
  TITLE_UPPER: ENVIRONMENTAL POLLUTION
  Title: Dynamic assessment of PM2.5 exposure and health risk using remote sensing
    and geo-spatial big data
  Title_JCS: ENVIRONMENTAL POLLUTION
  Title_SCI: Environmental Pollution
  Type Publication: article
  Year: 2019
- Abstract: 'Background and objective

    In recent years, several data quality conceptual frameworks have been proposed
    across the Data Quality and Information Quality domains towards assessment of
    quality of data. These frameworks are diverse, varying from simple lists of concepts
    to complex ontological and taxonomical representations of data quality concepts.
    The goal of this study is to design, develop and implement a platform agnostic
    computable data quality knowledge repository for data quality assessments.

    Methods

    We identified computable data quality concepts by performing a comprehensive literature
    review of articles indexed in three major bibliographic data sources. From this
    corpus, we extracted data quality concepts, their definitions, applicable measures,
    their computability and identified conceptual relationships. We used these relationships
    to design and develop a data quality meta-model and implemented it in a quality
    knowledge repository.

    Results

    We identified three primitives for programmatically performing data quality assessments:
    data quality concept, its definition, its measure or rule for data quality assessment,
    and their associations. We modeled a computable data quality meta-data repository
    and extended this framework to adapt, store, retrieve and automate assessment
    of other existing data quality assessment models.

    Conclusion

    We identified research gaps in data quality literature towards automating data
    quality assessments methods. In this process, we designed, developed and implemented
    a computable data quality knowledge repository for assessing quality and characterizing
    data in health data repositories. We leverage this knowledge repository in a service-oriented
    architecture to perform scalable and reproducible framework for data quality assessments
    in disparate biomedical data sources.'
  Author: Naresh Sundar Rajan and Ramkiran Gouripeddi and Peter Mo and Randy K. Madsen
    and Julio C. Facelli
  Book Title/Journal: Computer Methods and Programs in Biomedicine
  DOI: https://doi.org/10.1016/j.cmpb.2019.05.017
  JCS_FACTOR: 5.428
  Keywords: Data Quality Metadata Repository, Knowledge representation, Data quality
    assessment, Data quality dimensions, Data quality framework
  SCI_FACTOR: 0.924
  TITLE_UPPER: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title: Towards a content agnostic computable knowledge repository for data quality
    assessment
  Title_JCS: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title_SCI: Computer Methods and Programs in Biomedicine
  Type Publication: article
  Year: 2019
- Abstract: "Economic, social and environmental requirements make planning for a sustainable\
    \ electricity generation mix a demanding endeavour. Technological innovation offers\
    \ a range of renewable generation and energy management options which require\
    \ fine tuning and accurate control to be successful, which calls for the use of\
    \ large-scale, detailed datasets. In this paper, we focus on the UK and use Multi-Criteria\
    \ Decision Making (MCDM) to evaluate electricity generation options against technical,\
    \ environmental and social criteria. Data incompleteness and redundancy, usual\
    \ in large-scale datasets, as well as expert opinion ambiguity are dealt with\
    \ using a comprehensive grey TOPSIS model. We used evaluation scores to develop\
    \ a multi-objective optimization model to maximize the technical, environmental\
    \ and social utility of the electricity generation mix and to enable a larger\
    \ role for innovative technologies. Demand uncertainty was handled with an interval\
    \ range and we developed our problem with multi-objective grey linear programming\
    \ (MOGLP). Solving the mathematical model provided us with the electricity generation\
    \ mix for every 5\xE2\u20AC\xAFmin of the period under study. Our results indicate\
    \ that nuclear and renewable energy options, specifically wind, solar, and hydro,\
    \ but not biomass energy, perform better against all criteria indicating that\
    \ interindustry architectural innovation in the power generation mix is key to\
    \ sustainable UK electricity production and supply."
  Author: Konstantinos J. Chalvatzis and Hanif Malekpoor and Nishikant Mishra and
    Fiona Lettice and Sonal Choudhary
  Book Title/Journal: Technological Forecasting and Social Change
  DOI: https://doi.org/10.1016/j.techfore.2018.04.031
  JCS_FACTOR: 8.593
  Keywords: Energy innovation, Interindustry architectural innovation, Sustainable
    energy, Fuel mix, Grey TOPSIS, grey linear programming
  SCI_FACTOR: 2.226
  TITLE_UPPER: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title: 'Sustainable resource allocation for power generation: The role of big data
    in enabling interindustry architectural innovation'
  Title_JCS: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title_SCI: Technological Forecasting and Social Change
  Type Publication: article
  Year: 2019
- Abstract: In emerging markets, there are data quality problems. In this paper, we
    establish theoretical models to explore how data quality problems affect sustainable
    fashion supply chain operations. We start with the decentralized supply chain
    and find that poor data quality lowers supply chain profit and social welfare.
    We consider the implementation of blockchain to help and identify the situation
    in which blockchain helps enhance social welfare but brings harm to supply chain
    profitability. We propose a government sponsor scheme as well as an environment
    taxation waiving scheme to help. We further extend the study to the centralized
    supply chain setting.
  Author: Tsan-Ming Choi and Suyuan Luo
  Book Title/Journal: 'Transportation Research Part E: Logistics and Transportation
    Review'
  DOI: https://doi.org/10.1016/j.tre.2019.09.019
  JCS_FACTOR: 0.0
  Keywords: Fashion business operations, Supply chain centralization, Emerging markets,
    Sustainable operations, Social welfare
  SCI_FACTOR: 0.0
  TITLE_UPPER: 'TRANSPORTATION RESEARCH PART E: LOGISTICS AND TRANSPORTATION REVIEW'
  Title: 'Data quality challenges for sustainable fashion supply chain operations
    in emerging markets: Roles of blockchain, government sponsors and environment
    taxes'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: The increasing presence of geo-distributed sensor networks implies the
    generation of huge volumes of data from multiple geographical locations at an
    increasing rate. This raises important issues which become more challenging when
    the final goal is that of the analysis of the data for forecasting purposes or,
    more generally, for predictive tasks. This paper proposes a framework which supports
    predictive modeling tasks from streaming data coming from multiple geo-referenced
    sensors. In particular, we propose a distance-based anomaly detection strategy
    which considers objects described by embedding features learned via a stacked
    auto-encoder. We then devise a repair strategy which repairs the data detected
    as anomalous exploiting non-anomalous data measured by sensors in nearby spatial
    locations. Subsequently, we adopt Gradient Boosted Trees (GBTs) to predict/forecast
    values assumed by a target variable of interest for the repaired newly arriving
    (unlabeled) data, using the original feature representation or the embedding feature
    representation learned via the stacked auto-encoder. The workflow is implemented
    with distributed Apache Spark programming primitives and tested on a cluster environment.
    We perform experiments to assess the performance of each module, separately and
    in a combined manner, considering the predictive modeling of one-day-ahead energy
    production, for multiple renewable energy sites. Accuracy results show that the
    proposed framework allows reducing the error up to 13.56%. Moreover, scalability
    results demonstrate the efficiency of the proposed framework in terms of speedup,
    scaleup and execution time under a stress test.
  Author: Roberto Corizzo and Michelangelo Ceci and Nathalie Japkowicz
  Book Title/Journal: Big Data Research
  DOI: https://doi.org/10.1016/j.bdr.2019.04.001
  JCS_FACTOR: 3.578
  Keywords: Anomaly detection, Data repair, Geo-distributed big data, Spatial autocorrelation,
    Neural networks, Gradient-boosting
  SCI_FACTOR: 0.565
  TITLE_UPPER: BIG DATA RESEARCH
  Title: Anomaly Detection and Repair for Accurate Predictions in Geo-distributed
    Big Data
  Title_JCS: Big Data Research
  Title_SCI: Big Data Research
  Type Publication: article
  Year: 2019
- Abstract: In today's digital world, healthcare is one of the core areas in the medical
    domain. A healthcare system is required to analyze a large amount of patient data,
    which helps to derive insights and predictions of disease. This system should
    be intelligent and able to predict the patient's health condition by analyzing
    the patient's lifestyle, physical health records, and social activities. The health
    recommendation system (HRS) is becoming an important platform for healthcare services.
    In this context, health intelligent systems have become indispensable tools in
    decision-making processes in the healthcare sector. The main objective is to ensure
    the availability of valuable information at the right time by ensuring information
    quality, trustworthiness, authentication, and privacy. As people use social networks
    to learn about their health condition, so the HRS is very important to derive
    outcomes such as recommending diagnosis, health insurance, clinical pathway-based
    treatment methods, and alternative medicines based on the patient's health profile.
    In this chapter, we discuss recent research that targeted utilization of large
    volumes of medical data while combining multimodal data from disparate sources,
    which reduces the workload and cost in healthcare. In the healthcare sector, big
    data analytics using a recommendation system has an important role in terms of
    decision-making processes regarding the patient's health. This chapter presents
    a proposed intelligent HRS that provides an insight into how to use big data analytics
    for implementing an effective health recommendation engine and shows how to transform
    the healthcare industry from the traditional scenario to more personalized paradigm
    in a tele-health environment. Our proposed intelligent HRS resulted in lower MAE
    value when compared to existing approaches.
  Author: Abhaya Kumar Sahoo and Sitikantha Mallik and Chittaranjan Pradhan and Bhabani
    Shankar Prasad Mishra and Rabindra Kumar Barik and Himansu Das
  Book Title/Journal: Big Data Analytics for Intelligent Healthcare Management
  DOI: https://doi.org/10.1016/B978-0-12-818146-1.00009-X
  JCS_FACTOR: 0.0
  Keywords: Big data analytics, Classification, Healthcare, Privacy preservation,
    Recommendation system
  SCI_FACTOR: 0.0
  TITLE_UPPER: BIG DATA ANALYTICS FOR INTELLIGENT HEALTHCARE MANAGEMENT
  Title: Chapter 9 - Intelligence-Based Health Recommendation System Using Big Data
    Analytics
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: Patients face difficulties identifying appropriate doctors owing to the
    sizeable quantity and uneven quality of information in online healthcare communities.
    In studying physician searches, researchers often focus on expertise similarity
    matches and sentiment analyses of reviews. However, the quality is often ignored.
    To address patients' information needs holistically, we propose a four-dimensional
    IT framework based on signaling theory. The model takes expertise knowledge, online
    reviews, profile descriptions (e.g., hospital reputation, number of patients,
    city) and service quality (e.g., response speed, interaction frequency, cost)
    as signals that distinguish high-quality physicians. It uses machine learning
    approaches to derive similarity matches and sentiment analysis. It also measures
    the relative importance of the signals by multi-criterion analysis and derives
    the physician rankings through the aggregated scores. Our study revealed that
    the proposed approach performs better compared with the other two recommend techniques.
    This research expands the boundary of signaling theory to healthcare management
    and enriches the literature on IT use and inter-organizational systems. The proposed
    IT model may improve patient care, alleviate the physician-patient relationship
    and reduce lawsuits against hospitals; it also has practical implications for
    healthcare management.
  Author: Yan Ye and Yang Zhao and Jennifer Shang and Liyi Zhang
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2019.01.005
  JCS_FACTOR: 14.098
  Keywords: Online healthcare communities, Physician identifying, Signaling theory,
    Machine learning, Topic modeling, Multi-criterion analysis
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: A hybrid IT framework for identifying high-quality physicians using big data
    analytics
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: Nearly all studies that analyze the term structure of interest rates take
    a two-step approach. First, actual bond prices are summarized by interpolated
    synthetic zero-coupon yields, and second, some of these yields are used as the
    source data for further empirical examination. In contrast, we consider the advantages
    of a one-step approach that directly analyzes the universe of bond prices. To
    illustrate the feasibility and desirability of the one-step approach, we compare
    arbitrage-free dynamic term structure models estimated using both approaches.
    We also provide a simulation study showing that a one-step approach can extract
    the information in large panels of bond prices and avoid any arbitrary noise introduced
    from a first-stage interpolation of yields.
  Author: Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch
  Book Title/Journal: Journal of Econometrics
  DOI: https://doi.org/10.1016/j.jeconom.2019.04.019
  JCS_FACTOR: 2.388
  Keywords: "Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson\xE2\
    \u20AC\u201CSiegel model"
  SCI_FACTOR: 3.769
  TITLE_UPPER: JOURNAL OF ECONOMETRICS
  Title: 'Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices'
  Title_JCS: JOURNAL OF ECONOMETRICS
  Title_SCI: Journal of Econometrics
  Type Publication: article
  Year: 2019
- Abstract: Nearly all studies that analyze the term structure of interest rates take
    a two-step approach. First, actual bond prices are summarized by interpolated
    synthetic zero-coupon yields, and second, some of these yields are used as the
    source data for further empirical examination. In contrast, we consider the advantages
    of a one-step approach that directly analyzes the universe of bond prices. To
    illustrate the feasibility and desirability of the one-step approach, we compare
    arbitrage-free dynamic term structure models estimated using both approaches.
    We also provide a simulation study showing that a one-step approach can extract
    the information in large panels of bond prices and avoid any arbitrary noise introduced
    from a first-stage interpolation of yields.
  Author: Martin M. Andreasen and Jens H.E. Christensen and Glenn D. Rudebusch
  Book Title/Journal: Journal of Econometrics
  DOI: https://doi.org/10.1016/j.jeconom.2019.04.019
  JCS_FACTOR: 2.388
  Keywords: "Extended Kalman filter, Fixed-coupon bond prices, Arbitrage-free Nelson\xE2\
    \u20AC\u201CSiegel model"
  SCI_FACTOR: 3.769
  TITLE_UPPER: JOURNAL OF ECONOMETRICS
  Title: 'Term Structure Analysis with Big Data: One-Step Estimation Using Bond Prices'
  Title_JCS: JOURNAL OF ECONOMETRICS
  Title_SCI: Journal of Econometrics
  Type Publication: article
  Year: 2019
- Abstract: The accuracy and relevance of Business Intelligence & Analytics (BI&A)
    rely on the ability to bring high data quality to the data warehouse from both
    internal and external sources using the ETL process. The latter is complex and
    time-consuming as it manages data with heterogeneous content and diverse quality
    problems. Ensuring data quality requires tracking quality defects along the ETL
    process. In this paper, we present the main ETL quality characteristics. We provide
    an overview of the existing ETL process data quality approaches. We also present
    a comparative study of some commercial ETL tools to show how much these tools
    consider data quality dimensions. To illustrate our study, we carry out experiments
    using an ETL dedicated solution (Talend Data Integration) and a data quality dedicated
    solution (Talend Data Quality). Based on our study, we identify and discuss quality
    challenges to be addressed in our future research.
  Author: Manel Souibgui and Faten Atigui and Saloua Zammali and Samira Cherfi and
    Sadok Ben Yahia
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.09.223
  JCS_FACTOR: 0.0
  Keywords: Business Intelligence & Analytics, ETL quality, Data, process quality,
    Talend Data Integration, Talend Data Quality
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: 'Data quality in ETL process: A preliminary study'
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: 'Background

    Since the beginning of the 21st century, the amount of data obtained from public
    health surveillance has increased dramatically due to the advancement of information
    and communications technology and the data collection systems now in place.

    Methods

    This paper aims to highlight the opportunities gained through the use of Artificial
    Intelligence (AI) methods to enable reliable disease-oriented monitoring and projection
    in this information age.

    Results and Conclusion

    It is foreseeable that together with reliable data management platforms AI methods
    will enable analysis of massive infectious disease and surveillance data effectively
    to support government agencies, healthcare service providers, and medical professionals
    to response to disease in the future.'
  Author: Zoie S.Y. Wong and Jiaqi Zhou and Qingpeng Zhang
  Book Title/Journal: Infection, Disease & Health
  DOI: https://doi.org/10.1016/j.idh.2018.10.002
  JCS_FACTOR: 0.0
  Keywords: Infectious diseases modelling, Emergency response, Artificial Intelligence,
    Machine learning
  SCI_FACTOR: 0.0
  TITLE_UPPER: INFECTION, DISEASE & HEALTH
  Title: Artificial Intelligence for infectious disease Big Data Analytics
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Faced with internal and external pressure to adapt and implement environmental
    friendly business activities, it is becoming crucial for firms to identify practices
    that enhance their competitive advantage, economic, and environmental performance.
    Green innovation, green technologies, and the implementation of green supply chain
    management are examples of such practices. Green innovation and the adoption of
    the combination of green product innovation and green process innovation involve
    reduction in consumption of energy and pollution emission, recycling of wastes,
    sustainable utilization of resources, and green product designs. Although the
    extent research in this area is substantial, research on the importance of considering
    corporate environmental ethics, stakeholders view of green product, and demand
    for green products as drivers of green innovation must be conducted. Moreover,
    the role of large scale data, management commitment, and human resource practices
    play to overcome the technological challenges, achieve competitive advantage,
    and enhance the economic and environmental performance have yet to be addressed.
    This paper develops and tests a holistic model that depicts and examines the relationships
    among green innovation, its drivers, as well as factors that help overcome the
    technological challenges and influence the performance and competitive advantage
    of the firm. This paper is among the first works to deal with such a complex framework
    which considers the interrelationships among numerous constructs and their effects
    on competitive advantage as well as overall organizational performance. A questionnaire
    was designed to measure the influence of green innovation adoption/implementation
    and its drivers on performance and competitive advantage while taking into consideration
    the impact of management commitment and HR practices, as well as the use of large
    data on these relationships. Data collected from a sample of 215 respondents working
    in Middle East and North Africa (MENA) region and Golf-Cooperation Countries (GCC)
    were used to test the proposed relationships. The proposed model proved to be
    fit. The hypotheses were supported, and implications were discussed.
  Author: Abdul-Nasser El-Kassar and Sanjay Kumar Singh
  Book Title/Journal: Technological Forecasting and Social Change
  DOI: https://doi.org/10.1016/j.techfore.2017.12.016
  JCS_FACTOR: 8.593
  Keywords: Green innovation, Corporate environmental ethics, Large scale data, Human
    resource practices, Management commitment, Environmental and economic performance
  SCI_FACTOR: 2.226
  TITLE_UPPER: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title: 'Green innovation and organizational performance: The influence of big data
    and the moderating role of management commitment and HR practices'
  Title_JCS: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title_SCI: Technological Forecasting and Social Change
  Type Publication: article
  Year: 2019
- Abstract: 'Nowadays, companies are facing challenges due to increasingly dynamic
    market environments, a growing internal and external complexity, as well as globally
    intensifying competition. To keep pace, companies need to establish extensive
    knowledge about their business and its surroundings based on insights generated
    through the analysis of data. The digital shadow is a novel information system
    concept that integrates data of heterogeneous sources to provide product-related
    information to stakeholders across the company. The concept aims at improving
    the results of decision making, enabling advanced data analyses, and increasing
    information handling efficiency. As insufficient information quality has immediate
    effects on the utility of the information and induces significant costs, managing
    the quality of the digital shadow data basis is crucial. However, there are currently
    no comprehensive methodologies for the assessment and improvement of the data
    quality of digital shadows. Therefore, this paper introduces a methodology that
    supports the derivation of data quality projects aimed at optimizing the digital
    shadow data basis. The proposed methodology comprises four steps: First, digital
    shadow use cases along the product lifecycle are described. Next, the use cases
    are prioritized with regard to the expected benefits of applying the digital shadow.
    Third, quality deficiencies in the digital shadow data basis are assessed with
    respect to use case specific requirements. Finally, the prioritized use cases
    in relation with the identified quality deficits allow deriving needs for action,
    which are addressed by data quality projects. Together, the data quality projects
    constitute a data quality program. The methodology is applied in an industry case
    to prove the practical effectivity and efficiency.'
  Author: "G\xC3\xBCnther Schuh and Eric Rebentisch and Michael Riesener and Thorben\
    \ Ipers and Christian T\xC3\xB6nnes and Merle-Hendrikje Jank"
  Book Title/Journal: Procedia CIRP
  DOI: https://doi.org/10.1016/j.procir.2020.01.027
  JCS_FACTOR: 0.0
  Keywords: data quality program, digital shadow, data quality management
  SCI_FACTOR: 0.683
  TITLE_UPPER: PROCEDIA CIRP
  Title: Data quality program management for digital shadows of products
  Title_JCS: N/A
  Title_SCI: Procedia CIRP
  Type Publication: article
  Year: 2019
- Abstract: Staggering statistics regarding the global burden of disease due to lack
    of surgical care worldwide has been gaining attention in the global health literature
    over the last 10 years. The Lancet Commission on Global Surgery reported that
    16.9 million lives were lost due to an absence of surgical care in 2010, equivalent
    to 33% of all deaths worldwide. Although data from low- and middle-income countries
    (LMICs) are limited, recent investigations, such as the African Surgical Outcomes
    Study, highlight that despite operating on low risk patients, there is increased
    postoperative mortality in LMICs versus higher-resource settings, a majority of
    which occur secondary to seemingly preventable complications like surgical site
    infections. We propose that implementing creative, low-cost surgical outcomes
    monitoring and select quality improvement systems proven effective in high-income
    countries, such as surgical infection prevention programs and safety checklists,
    can enhance the delivery of safe surgical care in existing LMIC surgical systems.
    While efforts to initiate and expand surgical access and capacity continues to
    deserve attention in the global health community, here we advocate for creative
    modifications to current service structures, such as promoting a culture of safety,
    employing technology and mobile health (mHealth) for patient data collection and
    follow-up, and harnessing partnerships for information sharing, to create a framework
    for improving morbidity and mortality in responsible, scalable, and sustainable
    ways.
  Author: Belain Eyob and Marissa A. Boeck and Patrick FaSiOen and Shamir Cawich and
    Michael D. Kluger
  Book Title/Journal: International Journal of Surgery
  DOI: https://doi.org/10.1016/j.ijsu.2019.07.036
  JCS_FACTOR: 6.071
  Keywords: Surgical outcomes, Developing countries, Caribbean, Safe surgery, Quality
    improvement, Big data
  SCI_FACTOR: 1.315
  TITLE_UPPER: INTERNATIONAL JOURNAL OF SURGERY
  Title: Ensuring safe surgical care across resource settings via surgical outcomes
    data & quality improvement initiatives
  Title_JCS: International Journal of Surgery
  Title_SCI: International Journal of Surgery
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: James R. Marsden and David E. Pingry and Jason B. Thatcher
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.113172
  JCS_FACTOR: 5.795
  Keywords: empty
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: Perspectives on numerical data quality in IS research
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: Valentina Bellini and Alberto Petroni and Giuseppina Palumbo and Elena Bignami
  Book Title/Journal: Anaesthesia Critical Care & Pain Medicine
  DOI: https://doi.org/10.1016/j.accpm.2018.12.015
  JCS_FACTOR: 4.132
  Keywords: Machine learning, Artificial intelligence, Blockchain technology
  SCI_FACTOR: 0.0
  TITLE_UPPER: ANAESTHESIA CRITICAL CARE & PAIN MEDICINE
  Title: Data quality and blockchain technology
  Title_JCS: Anaesthesia Critical Care & Pain Medicine
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Regional healthcare platforms collect clinical data from hospitals in
    specific areas for the purpose of healthcare management. It is a common requirement
    to reuse the data for clinical research. However, we have to face challenges like
    the inconsistence of terminology in electronic health records (EHR) and the complexities
    in data quality and data formats in regional healthcare platform. In this paper,
    we propose methodology and process on constructing large scale cohorts which forms
    the basis of causality and comparative effectiveness relationship in epidemiology.
    We firstly constructed a Chinese terminology knowledge graph to deal with the
    diversity of vocabularies on regional platform. Secondly, we built special disease
    case repositories (i.e., heart failure repository) that utilize the graph to search
    the related patients and to normalize the data. Based on the requirements of the
    clinical research which aimed to explore the effectiveness of taking statin on
    180-days readmission in patients with heart failure, we built a large-scale retrospective
    cohort with 29647 cases of heart failure patients from the heart failure repository.
    After the propensity score matching, the study group (n=6346) and the control
    group (n=6346) with parallel clinical characteristics were acquired. Logistic
    regression analysis showed that taking statins had a negative correlation with
    180-days readmission in heart failure patients. This paper presents the workflow
    and application example of big data mining based on regional EHR data.
  Author: Daowen Liu and Liqi Lei and Tong Ruan and Ping He
  Book Title/Journal: Chinese Medical Sciences Journal
  DOI: https://doi.org/10.24920/003579
  JCS_FACTOR: 0.0
  Keywords: electronic health records, clinical terminology knowledge graph, clinical
    special disease case repository, evaluation of data quality, large scale cohort
    study
  SCI_FACTOR: 0.215
  TITLE_UPPER: CHINESE MEDICAL SCIENCES JOURNAL
  Title: 'Constructing Large Scale Cohort for Clinical Study on Heart Failure with
    Electronic Health Record in Regional Healthcare Platform: Challenges and Strategies
    in Data Reuse'
  Title_JCS: N/A
  Title_SCI: Chinese Medical Sciences Journal
  Type Publication: article
  Year: 2019
- Abstract: "Objective\nTo identify common temporal evolution profiles in biological\
    \ data and propose a semi-automated method to these patterns in a clinical data\
    \ warehouse (CDW).\nMaterials and Methods\nWe leveraged the CDW of the European\
    \ Hospital Georges Pompidou and tracked the evolution of 192 biological parameters\
    \ over a period of 17 years (for 445,000\xE2\u20AC\xAF+\xE2\u20AC\xAFpatients,\
    \ and 131 million laboratory test results).\nResults\nWe identified three common\
    \ profiles of evolution: discretization, breakpoints, and trends. We developed\
    \ computational and statistical methods to identify these profiles in the CDW.\
    \ Overall, of the 192 observed biological parameters (87,814,136 values), 135\
    \ presented at least one evolution. We identified breakpoints in 30 distinct parameters,\
    \ discretizations in 32, and trends in 79.\nDiscussion and conclusion\nour method\
    \ allowed the identification of several temporal events in the data. Considering\
    \ the distribution over time of these events, we identified probable causes for\
    \ the observed profiles: instruments or software upgrades and changes in computation\
    \ formulas. We evaluated the potential impact for data reuse. Finally, we formulated\
    \ recommendations to enable safe use and sharing of biological data collection\
    \ to limit the impact of data evolution in retrospective and federated studies\
    \ (e.g. the annotation of laboratory parameters presenting breakpoints or trends)."
  Author: "Vincent Looten and Liliane {Kong Win Chang} and Antoine Neuraz and Marie-Anne\
    \ Landau-Loriot and Benoit Vedie and Jean-Louis Paul and La\xC3\xABtitia Mauge\
    \ and Nadia Rivet and Angela Bonifati and Gilles Chatellier and Anita Burgun and\
    \ Bastien Rance"
  Book Title/Journal: Computer Methods and Programs in Biomedicine
  DOI: https://doi.org/10.1016/j.cmpb.2018.12.030
  JCS_FACTOR: 5.428
  Keywords: Quality control, Computational biology/methods*, Information storage and
    retrieval, Humans, Clinical laboratory information systems
  SCI_FACTOR: 0.924
  TITLE_UPPER: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title: What can millions of laboratory test results tell us about the temporal aspect
    of data quality? Study of data spanning 17 years in a clinical data warehouse
  Title_JCS: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title_SCI: Computer Methods and Programs in Biomedicine
  Type Publication: article
  Year: 2019
- Abstract: Although big data, big data analytics (BDA) and business intelligence
    have attracted growing attention of both academics and practitioners, a lack of
    clarity persists about how BDA has been applied in business and management domains.
    In reflecting on Professor Ayre's contributions, we want to extend his ideas on
    technological change by incorporating the discourses around big data, BDA and
    business intelligence. With this in mind, we integrate the burgeoning but disjointed
    streams of research on big data, BDA and business intelligence to develop unified
    frameworks. Our review takes on both technical and managerial perspectives to
    explore the complex nature of big data, techniques in big data analytics and utilisation
    of big data in business and management community. The advanced analytics techniques
    appear pivotal in bridging big data and business intelligence. The study of advanced
    analytics techniques and their applications in big data analytics led to identification
    of promising avenues for future research.
  Author: Jie Sheng and Joseph Amankwah-Amoah and Xiaojun Wang
  Book Title/Journal: Technological Forecasting and Social Change
  DOI: https://doi.org/10.1016/j.techfore.2018.06.009
  JCS_FACTOR: 8.593
  Keywords: Business intelligence, Big data, Big data analytics, Advanced techniques,
    Decision-making
  SCI_FACTOR: 2.226
  TITLE_UPPER: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title: 'Technology in the 21st century: New challenges and opportunities'
  Title_JCS: TECHNOLOGICAL FORECASTING AND SOCIAL CHANGE
  Title_SCI: Technological Forecasting and Social Change
  Type Publication: article
  Year: 2019
- Abstract: The proliferation of Internet of Things (IoT) has led to the emergence
    of enabling many interesting applications within the realm of several domains
    including smart cities. However, the accumulation of data from smart IoT devices
    poses significant challenges for data storage while there are needs to deliver
    relevant and high quality services to consumers. In this paper, we propose QDaS,
    a novel domain agnostic framework as a solution for effective data storage and
    management of IoT applications. The framework incorporates a novel data summarisation
    mechanism that uses an innovative data quality estimation technique. This proposed
    data quality estimation technique computes the quality of data (based on their
    utility) without requiring any feedback from users of this IoT data or domain
    awareness of the data. We evaluate the effectiveness of the proposed QDaS framework
    using real world datasets.
  Author: Jonathan Liono and Prem Prakash Jayaraman and A.K. Qin and Thuong Nguyen
    and Flora D. Salim
  Book Title/Journal: Journal of Parallel and Distributed Computing
  DOI: https://doi.org/10.1016/j.jpdc.2018.03.013
  JCS_FACTOR: 3.734
  Keywords: Quality of data, Storage management, Internet of Things (IoT), Cloud computing,
    Quality of service, Data summarisation
  SCI_FACTOR: 0.638
  TITLE_UPPER: JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING
  Title: 'QDaS: Quality driven data summarisation for effective storage management
    in Internet of Things'
  Title_JCS: JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING
  Title_SCI: Journal of Parallel and Distributed Computing
  Type Publication: article
  Year: 2019
- Abstract: "Where a technology is its life cycle can make a difference in the data\
    \ generated by or about adopting, using or implementing that technology. As a\
    \ result, it is arguable that adoption, usage or implementation data generated\
    \ early in a technology's life cycle is directly comparable to data generated\
    \ later in the life cycle. In particular, comparisons of early and late data can\
    \ result in a number of disparate results and limit replicability, because of\
    \ biases in the data and non-stationary data. This paper suggests that it can\
    \ be important for information systems researchers to disclose an estimate of\
    \ the location in the technology's life cycle, as part of their analysis. The\
    \ data life cycle discussion is then extended to the notion of \xE2\u20AC\u0153\
    data phase triangulation,\xE2\u20AC\x9D which is contrasted with \xE2\u20AC\u0153\
    methodology triangulation\xE2\u20AC\x9D and \xE2\u20AC\u0153data (collection)\
    \ triangulation.\xE2\u20AC\x9D In addition, we discuss the importance of being\
    \ able to use the findings from life cycle-based research to \xE2\u20AC\u0153\
    push\xE2\u20AC\x9D a technology from one phase to another phase."
  Author: Daniel E. O'Leary
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.113139
  JCS_FACTOR: 5.795
  Keywords: Technology life cycle, Data quality, Non-stationary data, Hype cycle,
    Data biases, Data phase triangulation
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: 'Technology life cycle and data quality: Action and triangulation'
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: empty
  Author: "Carlos S\xC3\xA1ez and Siaw-Teng Liaw and Eizen Kimura and Pascal Coorevits\
    \ and Juan M Garcia-Gomez"
  Book Title/Journal: Computer Methods and Programs in Biomedicine
  DOI: https://doi.org/10.1016/j.cmpb.2019.06.013
  JCS_FACTOR: 5.428
  Keywords: empty
  SCI_FACTOR: 0.924
  TITLE_UPPER: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title: 'Guest editorial: Special issue in biomedical data quality assessment methods'
  Title_JCS: COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE
  Title_SCI: Computer Methods and Programs in Biomedicine
  Type Publication: article
  Year: 2019
- Abstract: Urban space monitoring and surveillance systems are present almost everywhere
    in various forms of sensing devices such as closed-circuit television, smartphone,
    and camera. This requires a robust and easy-to-manage information and communication
    technology (ICT) infrastructure that is generally comprises sensors, protocols,
    networks, and steps. Smart adoption of such systems could influence, manage, direct,
    and protect human beings and property. Nevertheless, it may create problems of
    government support, data quality, privacy, and security. Today's computational
    world allows implementation of artificial intelligence models for big data analytics
    to bring cities smart (with intelligence and optimal improvement). This chapter
    will discuss the applications of urban space monitoring and surveillance systems
    via ICT. The typical limitations of the current research are discussed in detail.
  Author: Kwok Tai Chui and Pandian Vasant and Ryan Wen Liu
  Book Title/Journal: 'Smart Cities: Issues and Challenges'
  DOI: https://doi.org/10.1016/B978-0-12-816639-0.00007-7
  JCS_FACTOR: 0.0
  Keywords: Cyber security, Ethics, Policy-making, Security, Surveillance
  SCI_FACTOR: 0.0
  TITLE_UPPER: 'SMART CITIES: ISSUES AND CHALLENGES'
  Title: "Chapter 7 - Smart city is a safe city: information and communication technology\xE2\
    \u20AC\u201Cenhanced urban space monitoring and surveillance systems: the promise\
    \ and limitations"
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: The emergence of the industrial Internet of Things (IoT) and ever advancing
    computing and communication technologies have fueled a new industrial revolution
    which is happening worldwide to make current manufacturing systems smarter, safer,
    and more efficient. Although many general frameworks have been proposed for IoT
    enabled systems for industrial application, there is limited literature on demonstrations
    or testbeds of such systems. In addition, there is a lack of systematic study
    on the characteristics of IoT sensors and data analytics challenges associated
    with IoT sensor data. This study is an attempt to help fill this gap by exploring
    the characteristics of IoT vibration sensors and show how IoT sensors and big
    data analytics can be used to develop real time monitoring frameworks.
  Author: Devarshi Shah and Jin Wang and Q. Peter He
  Book Title/Journal: IFAC-PapersOnLine
  DOI: https://doi.org/10.1016/j.ifacol.2019.06.122
  JCS_FACTOR: 0.0
  Keywords: Internet-of-things, smart manufacturing, big data, data analytics, statistical
    analysis, vibration, soft sensor, process monitoring
  SCI_FACTOR: 0.308
  TITLE_UPPER: IFAC-PAPERSONLINE
  Title: An Internet-of-things Enabled Smart Manufacturing Testbed
  Title_JCS: N/A
  Title_SCI: IFAC-PapersOnLine
  Type Publication: article
  Year: 2019
- Abstract: "This paper aims to estimate the causal effect of accidents on traffic\
    \ congestion and vice versa. In order to identify both effects of this two-way\
    \ relationship, I use dynamic panel data techniques and open access \xE2\u20AC\
    \u02DCbig data\xE2\u20AC\u2122 of highway traffic and accidents in England for\
    \ the period 2012\xE2\u20AC\u201C2014. The research design is based on the daily-and-hourly\
    \ specific mean reversion pattern of highway traffic, which can be used to define\
    \ a recurrent congestion benchmark. Using this benchmark, I am able to identify\
    \ the causal effect of accidents on non-recurrent traffic congestion. A positive\
    \ relationship between traffic congestion and road accidents would yield multiplicative\
    \ benefits for policies that aim at reducing either of these issues. Additionally,\
    \ I explore the duration of the effect of an accident on congestion, the \xE2\u20AC\
    \u02DCrubbernecking\xE2\u20AC\u2122 effect, as well as heterogeneous effects in\
    \ the most congested highway segments. Then, I test the use of methods which employ\
    \ the bulk of information in big data and other methods using a very reduced sample.\
    \ In my application, both approaches produce similar results. Finally, I find\
    \ a non-linear negative effect of traffic congestion on the probability of an\
    \ accident."
  Author: Ilias Pasidis
  Book Title/Journal: Journal of Transport Geography
  DOI: https://doi.org/10.1016/j.jtrangeo.2017.10.006
  JCS_FACTOR: 4.986
  Keywords: Accidents, Traffic congestion, Big data, Highways, England
  SCI_FACTOR: 1.809
  TITLE_UPPER: JOURNAL OF TRANSPORT GEOGRAPHY
  Title: Congestion by accident? A two-way relationship for highways in England
  Title_JCS: Journal of Transport Geography
  Title_SCI: Journal of Transport Geography
  Type Publication: article
  Year: 2019
- Abstract: "Wildfires, whether natural or caused by humans, are considered among\
    \ the most dangerous and devastating disasters around the world. Their complexity\
    \ comes from the fact that they are hard to predict, hard to extinguish and cause\
    \ enormous financial losses. To address this issue, many research efforts have\
    \ been conducted in order to monitor, predict and prevent wildfires using several\
    \ Artificial Intelligence techniques and strategies such as Big Data, Machine\
    \ Learning, and Remote Sensing. The latter offers a rich source of satellite images,\
    \ from which we can retrieve a huge amount of data that can be used to monitor\
    \ wildfires. The method used in this paper combines Big Data, Remote Sensing and\
    \ Data Mining algorithms (Artificial Neural Network and SVM) to process data collected\
    \ from satellite images over large areas and extract insights from them to predict\
    \ the occurrence of wildfires and avoid such disasters. For this reason, we implemented\
    \ a methodology that serves this purpose by building a dataset based on Remote\
    \ Sensing data related to the state of the crops (NDVI), meteorological conditions\
    \ (LST), as well as the fire indicator \xE2\u20AC\u0153Thermal Anomalies\xE2\u20AC\
    \x9D, these data, were acquired from \xE2\u20AC\u0153MODIS\xE2\u20AC\x9D (Moderate\
    \ Resolution Imaging Spectroradiometer), a key instrument aboard the Terra and\
    \ Aqua satellites. This dataset is available on GitHub via this link (https://github.com/ouladsayadyounes/Wildfires).\
    \ Experiments were made using the big data platform \xE2\u20AC\u0153Databricks\xE2\
    \u20AC\x9D. Experimental results gave high prediction accuracy (98.32%). These\
    \ results were assessed using several validation strategies (e.g., classification\
    \ metrics, cross-validation, and regularization) as well as a comparison with\
    \ some wildfire early warning systems."
  Author: Younes Oulad Sayad and Hajar Mousannif and Hassan {Al Moatassime}
  Book Title/Journal: Fire Safety Journal
  DOI: https://doi.org/10.1016/j.firesaf.2019.01.006
  JCS_FACTOR: 2.764
  Keywords: Big data, Remote sensing, Machine learning, Wildfire prediction, Data
    mining, Artificial intelligence
  SCI_FACTOR: 0.958
  TITLE_UPPER: FIRE SAFETY JOURNAL
  Title: 'Predictive modeling of wildfires: A new dataset and machine learning approach'
  Title_JCS: FIRE SAFETY JOURNAL
  Title_SCI: Fire Safety Journal
  Type Publication: article
  Year: 2019
- Abstract: Urine from first trimester pregnancies has been found to be rich in information
    related to aneuploidies and other clinical conditions. Mass spectral analysis
    derived from matrix assisted laser desorption ionization (MALDI) time of flight
    (ToF) data has been proven to be a cost effective method for clinical diagnostics.
    However, urine mass spectra are complex and require data modelling frameworks.
    Therefore, computational approaches that systematically analyse big data generated
    from MALDI-ToF mass spectra are essential. To address this issue, we developed
    an automated workflow that successfully processed large data sets from MALDI-ToF
    which is 100-fold faster than using a common software tool. Our method performs
    accurate data quality control decisions, and generates a comparative analysis
    to extract peak intensity patterns from a data set. We successfully applied our
    framework to the identification of peak intensity patterns for Trisomy 21 and
    Trisomy 18 gestations on data sets from maternal pregnancy urines obtained in
    the UK and China. The results from our automated comparative analysis have shown
    characteristic patterns associated with aneuploidies in the first trimester pregnancy.
    Moreover, we have shown that the intensity patterns depended on the population
    origin, gestational age, and MALDI-ToF instrument.
  Author: Ricardo J. Pais and R. Zmuidinaite and S.A. Butler and R.K. Iles
  Book Title/Journal: Informatics in Medicine Unlocked
  DOI: https://doi.org/10.1016/j.imu.2019.100194
  JCS_FACTOR: 0.0
  Keywords: MALDI-ToF, Pattern recognition, Quality control, Comparative intensity
    data, Automated processing
  SCI_FACTOR: 0.44
  TITLE_UPPER: INFORMATICS IN MEDICINE UNLOCKED
  Title: 'An automated workflow for MALDI-ToF mass spectra pattern identification
    on large data sets: An application to detect aneuploidies from pregnancy urine'
  Title_JCS: N/A
  Title_SCI: Informatics in Medicine Unlocked
  Type Publication: article
  Year: 2019
- Abstract: 'The Korean CHildren''s ENvironmental health Study (Ko-CHENS) is a nationwide
    prospective birth cohort showing the correlation between the environmental exposures
    and the health effects to prevent the environmental diseases in children, and
    it provides the guidelines for the environmental hazardous factors, applying the
    life-course approach to the environmental-health management system. The Ko-CHENS
    consists of 5000 Core and 65,000 Main Cohorts. The children in the Core Cohort
    are followed up at 6 months, every year before their admission into the elementary
    school, and every 3 years from the first year after this admission. The children
    in the Cohort will be followed up through the data links (Statistics Korea, National
    Health Insurance Service [NHIS], and Ministry of Education). The individual biospecimens
    will be analyzed for 19 substances. The long-term-storage biological samples will
    be used for the further substance analysis. The Ko-CHENS will investigate whether
    the environmental variables including the perinatal outdoor and indoor factors
    and the greenness contribute causally to the health outcomes in the children and
    adolescents. In addition to the individual surveys, the assessments of the outdoor
    exposures and health outcomes will use the national air-quality monitoring data
    and claim data of the NHIS, respectively. The two big-data forms of the Ko-CHENS
    are as follows: The Ko-CHENS data that can be linked with the nationally registered
    NHIS health-related database, including the medical utilization and the periodic
    health screening, and the birth/mortality database in the Statistics; the other
    is the Big-CHENS dataset that is based on the NHIS mother delivery code, for which
    the follow-up of almost 97% of the total birth population is expected. The Ko-CHENS
    is a very cost-effective study that fully exploits the existing national big-data
    systems with the data linkage.'
  Author: "Kyoung Sook Jeong and Suejin Kim and Woo\xC2\_Jin Kim and Hwan-Cheol Kim\
    \ and Jisuk Bae and Yun-Chul Hong and Mina Ha and Kangmo Ahn and Ji-Young Lee\
    \ and Yangho Kim and Eunhee Ha"
  Book Title/Journal: Environmental Research
  DOI: https://doi.org/10.1016/j.envres.2018.12.009
  JCS_FACTOR: 6.498
  Keywords: Ko-CHENS, Children, Environment, Cohort profile, Birth cohort
  SCI_FACTOR: 1.46
  TITLE_UPPER: ENVIRONMENTAL RESEARCH
  Title: "Cohort profile: Beyond birth cohort study \xE2\u20AC\u201C The Korean CHildren's\
    \ ENvironmental health Study (Ko-CHENS)"
  Title_JCS: ENVIRONMENTAL RESEARCH
  Title_SCI: Environmental Research
  Type Publication: article
  Year: 2019
- Abstract: "The paper at hand motivates, proposes, demonstrates, and evaluates a\
    \ novel systematic approach to discovering causal dependencies between events\
    \ encoded in large arrays of data, called causality mining. The approach has emerged\
    \ in the discussions with our project partner, an Australian public energy company.\
    \ It was successfully evaluated in a case study with the project partner to extract\
    \ valuable, and otherwise unknown, information on the causal dependencies between\
    \ observations reported by the company\xE2\u20AC\u2122s employees as part of the\
    \ organizational health and safety management practices and incidents that had\
    \ occurred at the organization\xE2\u20AC\u2122s sites. The dependencies were derived\
    \ based on the notion of proximity of the observations and incidents. The setup\
    \ and results of the evaluation are reported in this paper. The new approach and\
    \ the delivered insights aim at improving the overall health and safety culture\
    \ of the project partner practices, as they can be applied to caution and, thus,\
    \ prevent future incidents."
  Author: Artem Polyvyanyy and Anastasiia Pika and Moe T. Wynn and Arthur H.M. {ter
    Hofstede}
  Book Title/Journal: Safety Science
  DOI: https://doi.org/10.1016/j.ssci.2019.04.045
  JCS_FACTOR: 4.877
  Keywords: Big data, Data mining, Process mining, Proximity of events, Causality,
    Health and safety, Cause of incidents
  SCI_FACTOR: 1.178
  TITLE_UPPER: SAFETY SCIENCE
  Title: A systematic approach for discovering causal dependencies between observations
    and incidents in the health and safety domain
  Title_JCS: SAFETY SCIENCE
  Title_SCI: Safety Science
  Type Publication: article
  Year: 2019
- Abstract: In state-of-the-art big-data applications, the process of building machine
    learning models can be very challenging due to continuous changes in data structures
    and the need for human interaction to tune the variables and models over time.
    Hence, expedited learning in rapidly changing environments is required. In this
    work, we address this challenge by implementing concepts from the field of intrinsically
    motivated computational learning, also known as artificial curiosity (AC). In
    AC, an autonomous agent acts to optimize its learning about itself and its environment
    by receiving internal rewards based on prediction errors. We present a novel method
    of intrinsically motivated learning, based on the curiosity loop, to learn the
    data structures in large and varied datasets. An autonomous agent learns to select
    a subset of relevant features in the data, i.e., feature selection, to be used
    later for model construction. The agent optimizes its learning about the data
    structure over time without requiring external supervision. We show that our method,
    called the Curious Feature Selection (CFS) algorithm, positively impacts the accuracy
    of learning models on three public datasets.
  Author: Michal Moran and Goren Gordon
  Book Title/Journal: Information Sciences
  DOI: https://doi.org/10.1016/j.ins.2019.02.009
  JCS_FACTOR: 6.795
  Keywords: Intrinsic motivation learning, Curiosity loop, Reinforcement learning,
    Big data, Data science, Feature selection
  SCI_FACTOR: 1.524
  TITLE_UPPER: INFORMATION SCIENCES
  Title: Curious Feature Selection
  Title_JCS: INFORMATION SCIENCES
  Title_SCI: Information Sciences
  Type Publication: article
  Year: 2019
- Abstract: "Forecasting tourism demand has important implications for both policy\
    \ makers and companies operating in the tourism industry. In this research, we\
    \ applied methods and tools of social network and semantic analysis to study user-generated\
    \ content retrieved from online communities which interacted on the TripAdvisor\
    \ travel forum. We analyzed the forums of 7 major European capital cities, over\
    \ a period of 10\xE2\u20AC\xAFyears, collecting more than 2,660,000 posts, written\
    \ by about 147,000 users. We present a new methodology of analysis of tourism-related\
    \ big data and a set of variables which could be integrated into traditional forecasting\
    \ models. We implemented Factor Augmented Autoregressive and Bridge models with\
    \ social network and semantic variables which often led to a better forecasting\
    \ performance than univariate models and models based on Google Trend data. Forum\
    \ language complexity and the centralization of the communication network \xE2\
    \u20AC\u201C i.e. the presence of eminent contributors \xE2\u20AC\u201C were the\
    \ variables that contributed more to the forecasting of international airport\
    \ arrivals."
  Author: Andrea {Fronzetti Colladon} and Barbara Guardabascio and Rosy Innarella
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.113075
  JCS_FACTOR: 5.795
  Keywords: Tourism forecasting, Social network analysis, Semantic analysis, Online
    community, Text mining, Big data
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: Using social network and semantic analysis to analyze online travel forums
    and forecast tourism demand
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: "Recently, with the development of \xE2\u20AC\u0153Industry 4.0\xE2\u20AC\
    \x9D, \xE2\u20AC\u0153Oil and Gas 4.0\xE2\u20AC\x9D has also been put on the agenda\
    \ in the past two years. Some companies and experts believe that \xE2\u20AC\u0153\
    Oil and Gas 4.0\xE2\u20AC\x9D can completely change the status quo of the oil\
    \ and gas industry, which can bring huge benefits because it accelerates the digitization\
    \ and intelligentization of the oil and gas industry. However, the \xE2\u20AC\u0153\
    Oil and Gas 4.0\xE2\u20AC\x9D is still in its infancy. Therefore, this paper systematically\
    \ introduces the concept and core technologies of \xE2\u20AC\u0153Oil and Gas\
    \ 4.0\xE2\u20AC\x9D, such as big data and the industrial Internet of Things (IIoT).\
    \ Moreover, this paper analyzes typical application scenarios of the oil and gas\
    \ industry chain (upstream, midstream and downstream) through examples, such as\
    \ intelligent oilfield, intelligent pipeline, and intelligent refinery. It is\
    \ concluded that the essence of \xE2\u20AC\u0153Oil and Gas 4.0\xE2\u20AC\x9D\
    \ is a data-driven intelligence system based on the highly digitization. To the\
    \ best of our knowledge, this is the first academic peer-reviewed paper on the\
    \ \xE2\u20AC\u0153Oil and Gas 4.0\xE2\u20AC\x9D era, aiming to let more oil and\
    \ gas industry personnel understand its benefits and application scenarios, so\
    \ as to better apply it to practical engineering in the future. In the discussion\
    \ section, this paper also analyzes the opportunities and difficulties that may\
    \ be brought about by the \xE2\u20AC\u0153Oil and Gas 4.0\xE2\u20AC\x9D era. Finally,\
    \ relevant policy recommendations are proposed."
  Author: Hongfang Lu and Lijun Guo and Mohammadamin Azimi and Kun Huang
  Book Title/Journal: Computers in Industry
  DOI: https://doi.org/10.1016/j.compind.2019.06.007
  JCS_FACTOR: 7.635
  Keywords: Oil and Gas 4.0, Big data, Digitization, IIoT, Intelligentization
  SCI_FACTOR: 1.432
  TITLE_UPPER: COMPUTERS IN INDUSTRY
  Title: 'Oil and Gas 4.0 era: A systematic review and outlook'
  Title_JCS: COMPUTERS IN INDUSTRY
  Title_SCI: Computers in Industry
  Type Publication: article
  Year: 2019
- Abstract: This paper aims to contribute to a better understanding of the literature
    on open data in three ways. The first is to develop a descriptive analysis of
    journals and authors to identify the knowledge areas in which open data are applied.
    The second is to analyse the conceptual structure of the field using a bibliometric
    technique. The co-word analysis enabled us to create a map of the main themes
    that have been studied, identifying their importance and relevance. These themes
    were analysed and grouped. The third is to propose future research trends. According
    to our results, the main knowledge areas are Engineering, Health, Public Administration,
    Management and Education. The main themes are big data, open-linked data and data
    reuse. Finally, several research questions are proposed according to knowledge
    area and theme.
  Author: "Diego Corrales-Garay and Marta Ortiz-de-Urbina-Criado and Eva-Mar\xC3\xAD\
    a Mora-Valent\xC3\xADn"
  Book Title/Journal: Government Information Quarterly
  DOI: https://doi.org/10.1016/j.giq.2018.10.008
  JCS_FACTOR: 7.279
  Keywords: Open data, Bibliometric analysis, Co-word analysis, Science map, Knowledge
    areas, Most-studied themes, Future trends
  SCI_FACTOR: 2.121
  TITLE_UPPER: GOVERNMENT INFORMATION QUARTERLY
  Title: 'Knowledge areas, themes and future research on open data: A co-word analysis'
  Title_JCS: GOVERNMENT INFORMATION QUARTERLY
  Title_SCI: Government Information Quarterly
  Type Publication: article
  Year: 2019
- Abstract: "This review discusses practical benefits and limitations of novel data-driven\
    \ research for social scientists in general and criminologists in particular by\
    \ providing a comprehensive examination of the matter. Specifically, this study\
    \ is an attempt to critically evaluate \xE2\u20AC\u02DCbig data\xE2\u20AC\u2122\
    , data-driven perspectives, and their epistemological value for both scholars\
    \ and practitioners, particularly those working on crime. It serves as guidance\
    \ for those who are interested in data-driven research by pointing out new research\
    \ avenues. In addition to the benefits, the drawbacks associated with data-driven\
    \ approaches are also discussed. Finally, critical problems that are emerging\
    \ in this era, such as privacy and ethical concerns are highlighted."
  Author: Turgut Ozkan
  Book Title/Journal: The Social Science Journal
  DOI: https://doi.org/10.1016/j.soscij.2018.10.010
  JCS_FACTOR: 0.0
  Keywords: Social science, Big data, Crime, Social media, Data-driven social science
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE SOCIAL SCIENCE JOURNAL
  Title: 'Criminology in the age of data explosion: New directions'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: While the use of big data tends to add value for business throughout the
    entire value chain, the integration of big data analytics (BDA) to the decision-making
    process remains a challenge. This study, based on a systematic literature review,
    thematic analysis and qualitative interview findings, proposes a set of six-steps
    to establish both rigor and relevance in the process of analytics-driven decision-making.
    Our findings illuminate the key steps in this decision process including problem
    definition, review of past findings, model development, data collection, data
    analysis as well as actions on insights in the context of service systems. Although
    findings have been discussed in a sequence of steps, the study identifies them
    as interdependent and iterative. The proposed six-step analytics-driven decision-making
    process, practical evidence from service systems, and future research agenda,
    provide altogether the foundation for future scholarly research and can serve
    as a step-wise guide for industry practitioners.
  Author: Shahriar Akter and Ruwan Bandara and Umme Hani and Samuel {Fosso Wamba}
    and Cyril Foropon and Thanos Papadopoulos
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2019.01.020
  JCS_FACTOR: 14.098
  Keywords: Big data analytics, Decision-making, Service systems
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: 'Analytics-based decision-making for service systems: A qualitative study
    and agenda for future research'
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: 'In this paper we present a collection of ontologies specifically designed
    to model the information exchange needs of combined software and data engineering.
    Effective, collaborative integration of software and big data engineering for
    Web-scale systems, is now a crucial technical and economic challenge. This requires
    new combined data and software engineering processes and tools. Our proposed models
    have been deployed to enable: tool-chain integration, such as the exchange of
    data quality reports; cross-domain communication, such as interlinked data and
    software unit testing; mediation of the system design process through the capture
    of design intents and as a source of context for model-driven software engineering
    processes. These ontologies are deployed in web-scale, data-intensive, system
    development environments in both the commercial and academic domains. We exemplify
    the usage of the suite on case-studies emerging from two complex collaborative
    software and data engineering scenarios: one from the legal sector and the other
    from the Social sciences and Humanities domain.'
  Author: "Monika Solanki and Bojan Bo\xC5\xBEi\xC4\u2021 and Christian Dirschl and\
    \ Rob Brennan"
  Book Title/Journal: Journal of Systems and Software
  DOI: https://doi.org/10.1016/j.jss.2018.12.017
  JCS_FACTOR: 2.829
  Keywords: Ontologies, Data engineering, Software engineering, Alignment, Integration
  SCI_FACTOR: 0.642
  TITLE_UPPER: JOURNAL OF SYSTEMS AND SOFTWARE
  Title: Towards a knowledge driven framework for bridging the gap between software
    and data engineering
  Title_JCS: JOURNAL OF SYSTEMS AND SOFTWARE
  Title_SCI: Journal of Systems and Software
  Type Publication: article
  Year: 2019
- Abstract: "In the age of \xE2\u20AC\u0153Internet+\xE2\u20AC\x9D, many Internet\
    \ service platforms (ISPs) in China have been widely introduced to the closed-loop\
    \ supply chain (CLSC). To further study the role of the Internet service platform,\
    \ this paper considers a CLSC composed of a manufacturer, a retailer and an Internet\
    \ service platform who invests in research and development (R&D), advertising\
    \ and Big Data marketing, and develops the goodwill dynamic model based on the\
    \ differential game theory. The construction of a goodwill dynamic model has two\
    \ purposes, namely, to increase sales and the return rate. The optimal decisions\
    \ for 3 players under two different cooperative scenarios are obtained, namely,\
    \ the retailer payment scenario (scenario D) and the manufacturer cost-sharing\
    \ scenario (scenario S). The supply chain members gain more profit or achieve\
    \ a higher level of goodwill for products under certain conditions, i.e., a high\
    \ residual value from remanufacturing, a high sharing rate of residual value from\
    \ the retailer's recycled products, and a low recycling cost. Interestingly, the\
    \ wholesale price increases with the residual value of recycled products when\
    \ goodwill effectiveness is low, while the price declines when goodwill effectiveness\
    \ is high. After comparing two cooperative scenarios, the result shows that an\
    \ Internet service platform will invest more in Big Data marketing under the manufacturer\
    \ cost-sharing scenario, and cooperation between the manufacturer and the Internet\
    \ service platform can help improve the goodwill of enterprises or products. Moreover,\
    \ the manufacturer cost-sharing scenario is payoff-Pareto-improving in most cases\
    \ through the coordination of a cost-sharing rate, and the effectiveness of Big\
    \ Data marketing exerts a positive effect on goodwill and the development of the\
    \ industry. In addition, the retailer has \xE2\u20AC\u0153free rider\xE2\u20AC\
    \x9D tendencies in the manufacturer cost-sharing scenario. The results encourage\
    \ more enterprises to enhance the value of goodwill through cooperation with Internet\
    \ service platforms because Internet service platforms conveniently utilize Big\
    \ Data marketing to increase the sales of products and the collecting rate of\
    \ used products, which in turn helps environmental sustainability."
  Author: Zehua Xiang and Minli Xu
  Book Title/Journal: Journal of Cleaner Production
  DOI: https://doi.org/10.1016/j.jclepro.2019.01.310
  JCS_FACTOR: 9.297
  Keywords: Big data marketing, Differential game, Closed-loop supply chain, Internet
    service platform
  SCI_FACTOR: 1.937
  TITLE_UPPER: JOURNAL OF CLEANER PRODUCTION
  Title: Dynamic cooperation strategies of the closed-loop supply chain involving
    the internet service platform
  Title_JCS: Journal of Cleaner Production
  Title_SCI: Journal of Cleaner Production
  Type Publication: article
  Year: 2019
- Abstract: Digital health or eHealth technologies, notably pervasive computing, robotics,
    big-data, wearable devices, machine learning, and artificial intelligence (AI),
    have opened unprecedented opportunities as to how the diseases are diagnosed and
    managed with active patient engagement. Patient-related data have provided insights
    (real world data) into understanding the disease processes. Advanced analytics
    have refined these insights further to draw dynamic algorithms aiding clinicians
    in making more accurate diagnosis with the help of machine learning. AI is another
    tool, which, although is still in the evolution stage, has the potential to help
    identify early signs even before the clinical features are apparent. The evolving
    digital developments pose challenges on allowing access to health-related data
    for further research but, at the same time, protecting each patient's privacy.
    This review focuses on the recent technological advances and their applications
    and highlights the immense potential to enable early diagnosis of rheumatological
    diseases.
  Author: Suchitra Kataria and Vinod Ravindran
  Book Title/Journal: Best Practice & Research Clinical Rheumatology
  DOI: https://doi.org/10.1016/j.berh.2019.101429
  JCS_FACTOR: 0.0
  Keywords: Artificial intelligence, Big data, Machine learning, Data analytics, Wearable
    devices, Robotics, Digital health
  SCI_FACTOR: 0.0
  TITLE_UPPER: BEST PRACTICE & RESEARCH CLINICAL RHEUMATOLOGY
  Title: Emerging role of eHealth in the identification of very early inflammatory
    rheumatic diseases
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: This paper reviews the accounting literature that focuses on four Internet-related
    technologies that have the potential to dramatically change and disrupt the work
    of accountants and accounting researchers in the near future. These include cloud,
    big data, blockchain, and artificial intelligence (AI). For instance, access to
    distributed ledgers (blockchain) and big data supported by cloud-based analytics
    tools and AI will automate decision making to a large extent. These technologies
    may significantly improve financial visibility and allow more timely intervention
    due to the perpetual nature of accounting. However, given the number of tasks
    technology has relieved of accountants, these technologies may also lead to concerns
    about the profession's legitimacy. The findings suggest that scholars have not
    given sufficient attention to these technologies and how these technologies affect
    the everyday work of accountants. Research is urgently needed to understand the
    new kinds of accounting required to manage firms in the changing digital economy
    and to determine the new skills and competencies accountants may need to master
    to remain relevant and add value. The paper outlines a set of questions to guide
    future research.
  Author: Jodie Moll and Ogan Yigitbasioglu
  Book Title/Journal: The British Accounting Review
  DOI: https://doi.org/10.1016/j.bar.2019.04.002
  JCS_FACTOR: 0.0
  Keywords: Accounting profession, Cloud, Big data, Blockchain, Artificial intelligence
  SCI_FACTOR: 0.0
  TITLE_UPPER: THE BRITISH ACCOUNTING REVIEW
  Title: 'The role of internet-related technologies in shaping the work of accountants:
    New directions for accounting research'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: "The amount of data, which is created in companies is increasing due to\
    \ modern communication technologies and decreasing costs for storing data. This\
    \ leads to an advancement of methods for data analyses as well as to an increasing\
    \ awareness of benefits resulting from data-based knowledge. In the context of\
    \ product service systems and product development, there are two major concepts\
    \ for providing product information. The digital twin collects every information\
    \ possible, while the digital shadow provides a sufficient and content-related\
    \ picture of the product. Since these concepts merge data from different sources,\
    \ comprehension about information quality and its relation to the data quality\
    \ becomes immanently important. This paper introduces a framework to determine\
    \ information quality with respect to data-related and system-related attributes.\
    \ An extensive literature review with focus on \xE2\u20AC\u0153information quality\xE2\
    \u20AC\x9D and \xE2\u20AC\u0153data quality\xE2\u20AC\x9D identifies the important\
    \ approaches for describing information and data quality. A latent dirichlet allocation\
    \ (LDA) algorithm is applied on 371 definitions and identify 12 data-related and\
    \ system-related attributes for information quality. Those attributes are assigned\
    \ to six dimensions for information quality. So the proposed framework depicts\
    \ the relationships between data attributes and the influence on information quality."
  Author: "Michael Riesener and Christian D\xC3\xB6lle and G\xC3\xBCnther Schuh and\
    \ Christian T\xC3\xB6nnes"
  Book Title/Journal: Procedia CIRP
  DOI: https://doi.org/10.1016/j.procir.2019.03.131
  JCS_FACTOR: 0.0
  Keywords: Digital Shadow, Information quality, Data quality, Latent dirichlet allocation
    (LDA)
  SCI_FACTOR: 0.683
  TITLE_UPPER: PROCEDIA CIRP
  Title: Framework for defining information quality based on data attributes within
    the digital shadow using LDA
  Title_JCS: N/A
  Title_SCI: Procedia CIRP
  Type Publication: article
  Year: 2019
- Abstract: Recent advancements in data-driven process control and performance analysis
    could provide the wastewater treatment industry with an opportunity to reduce
    costs and improve operations. However, big data in wastewater treatment plants
    (WWTP) is widely underutilized, due in part to a workforce that lacks background
    knowledge of data science required to fully analyze the unique characteristics
    of WWTP. Wastewater treatment processes exhibit nonlinear, nonstationary, autocorrelated,
    and co-correlated behavior that (i) is very difficult to model using first principals
    and (ii) must be considered when implementing data-driven methods. This review
    provides an overview of data-driven methods of achieving fault detection, variable
    prediction, and advanced control of WWTP. We present how big data has been used
    in the context of WWTP, and much of the discussion can also be applied to water
    treatment. Due to the assumptions inherent in different data-driven modeling approaches
    (e.g., control charts, statistical process control, model predictive control,
    neural networks, transfer functions, fuzzy logic), not all methods are appropriate
    for every goal or every dataset. Practical guidance is given for matching a desired
    goal with a particular methodology along with considerations regarding the assumed
    data structure. References for further reading are provided, and an overall analysis
    framework is presented.
  Author: Kathryn B. Newhart and Ryan W. Holloway and Amanda S. Hering and Tzahi Y.
    Cath
  Book Title/Journal: Water Research
  DOI: https://doi.org/10.1016/j.watres.2019.03.030
  JCS_FACTOR: 11.236
  Keywords: Wastewater treatment, Big data, Statistical process control, Process optimization,
    Monitoring
  SCI_FACTOR: 3.099
  TITLE_UPPER: WATER RESEARCH
  Title: 'Data-driven performance analyses of wastewater treatment plants: A review'
  Title_JCS: WATER RESEARCH
  Title_SCI: Water Research
  Type Publication: article
  Year: 2019
- Abstract: "Locating the bottlenecks in cities where traffic congestion usually occurs\
    \ is essential prior to solving congestion problems. Therefore, this paper proposes\
    \ a low-frequency probe vehicle data (PVD)-based method to identify turn-level\
    \ intersection traffic congestion in an urban road network. This method initially\
    \ divides an urban area into meter-scale square cells and maps PVD into those\
    \ cells and then identifies the cells that correspond to road intersections by\
    \ taking advantage of the fixed-location stop-and-go characteristics of traffic\
    \ passing through intersections. With those rasterized road intersections, the\
    \ proposed method recognizes probe vehicles\xE2\u20AC\u2122 turning directions\
    \ and provides preliminary analysis of traffic conditions at all turning directions.\
    \ The proposed method is map-independent (i.e., no digital map is needed) and\
    \ computationally efficient and is able to rapidly screen most of the intersections\
    \ for turn-level congestion in a road network. Thereby, this method is expected\
    \ to greatly decrease traffic engineers\xE2\u20AC\u2122 workloads by providing\
    \ information regarding where and when to investigate and solve traffic congestion\
    \ problems."
  Author: Zhengbing He and Geqi Qi and Lili Lu and Yanyan Chen
  Book Title/Journal: 'Transportation Research Part C: Emerging Technologies'
  DOI: https://doi.org/10.1016/j.trc.2019.10.001
  JCS_FACTOR: 0.0
  Keywords: Big data, Floating car data, Urban road network, Traffic congestion, Road
    intersection
  SCI_FACTOR: 3.185
  TITLE_UPPER: 'TRANSPORTATION RESEARCH PART C: EMERGING TECHNOLOGIES'
  Title: Network-wide identification of turn-level intersection congestion using only
    low-frequency probe vehicle data
  Title_JCS: N/A
  Title_SCI: 'Transportation Research Part C: Emerging Technologies'
  Type Publication: article
  Year: 2019
- Abstract: "This study sets out to assess whether there is a knowledge gap between\
    \ the research frontier and the consultation business in how transport data are\
    \ collected, managed and analysed. The consulting business plays an important\
    \ role in applying data and methods as they typically carry out public tasks in\
    \ various parts of the transport system, which are becoming more and more specialised.\
    \ At the same time, big data has emerged with the promise to provide new, more\
    \ and better information to help understand society and execute policies more\
    \ efficiently \xE2\u20AC\u201C what we refer to as the data driven transition.\
    \ We conduct a literature review to identify the state of the art within international\
    \ research and compare this with results from interviews and with a survey sent\
    \ to representatives from the Norwegian consultation business. We find that there\
    \ is a considerable gap between international researchers and the consulting business\
    \ within the entire process of collection, management and analysis of traffic\
    \ data, and that this gap is increasing with the emergence of the data driven\
    \ transition. Finally, we argue that the results are applicable to other countries\
    \ as well. Action should be taken to keep the consultants up to speed, which will\
    \ require efforts from several actors, including governmental agencies, the education\
    \ institutions, the consulting business and researchers."
  Author: "Hanne Seter and Petter Arnesen and Odd Andr\xC3\xA9 Hjelkrem"
  Book Title/Journal: Transport Policy
  DOI: https://doi.org/10.1016/j.tranpol.2019.05.016
  JCS_FACTOR: 4.674
  Keywords: empty
  SCI_FACTOR: 1.687
  TITLE_UPPER: TRANSPORT POLICY
  Title: The data driven transport research train is leaving the station. Consultants
    all aboard?
  Title_JCS: Transport Policy
  Title_SCI: Transport Policy
  Type Publication: article
  Year: 2019
- Abstract: 'As companies become increasingly digital, growth hacking emerged as a
    new way of scaling businesses. While the term is fashionable in business, many
    executives remain confused about the concept. Even if firms have an idea of what
    growth hacking is, they may still be puzzled as to how to do it, creating a strategy-execution
    gap. Our article assists firms by bridging the growth hacking strategy-execution
    gap. First, we provide a growth hacking framework and deconstruct its building
    blocks: marketing, data analysis, coding, and the lean startup philosophy. We
    then present a taxonomy of 34 growth hacking patterns along the customer lifecycle
    of acquisition, activation, revenue, retention, and referral; categorize them
    on the two dimensions of resource intensity and time lag; and provide an example
    of how to apply the taxonomy in the case of a fitness application. Finally, we
    discuss seven opportunities and challenges of growth hacking that firms should
    keep in mind.'
  Author: "Ren\xC3\xA9 Bohnsack and Meike Malena Liesner"
  Book Title/Journal: Business Horizons
  DOI: https://doi.org/10.1016/j.bushor.2019.09.001
  JCS_FACTOR: 6.361
  Keywords: Growth hacking, Digital transformation, Lean startup, Digital marketing,
    Big data
  SCI_FACTOR: 2.174
  TITLE_UPPER: BUSINESS HORIZONS
  Title: What the hack? A growth hacking taxonomy and practical applications for firms
  Title_JCS: BUSINESS HORIZONS
  Title_SCI: Business Horizons
  Type Publication: article
  Year: 2019
- Abstract: Data governance refers to the exercise of authority and control over the
    management of data. The purpose of data governance is to increase the value of
    data and minimize data-related cost and risk. Despite data governance gaining
    in importance in recent years, a holistic view on data governance, which could
    guide both practitioners and researchers, is missing. In this review paper, we
    aim to close this gap and develop a conceptual framework for data governance,
    synthesize the literature, and provide a research agenda. We base our work on
    a structured literature review including 145 research papers and practitioner
    publications published during 2001-2019. We identify the major building blocks
    of data governance and decompose them along six dimensions. The paper supports
    future research on data governance by identifying five research areas and displaying
    a total of 15 research questions. Furthermore, the conceptual framework provides
    an overview of antecedents, scoping parameters, and governance mechanisms to assist
    practitioners in approaching data governance in a structured manner.
  Author: Rene Abraham and Johannes Schneider and Jan {vom Brocke}
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2019.07.008
  JCS_FACTOR: 14.098
  Keywords: Data governance, Information governance, Conceptual framework, Literature
    review, Research agenda
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: 'Data governance: A conceptual framework, structured review, and research
    agenda'
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: "Data science is likely to lead to major changes in cardiovascular imaging.\
    \ Problems with timing, efficiency, and missed diagnoses occur at all stages of\
    \ the imaging chain. The application of artificial intelligence (AI) is dependent\
    \ on robust data; the application of appropriate computational approaches and\
    \ tools; and validation of its clinical application to image segmentation, automated\
    \ measurements, and eventually, automated diagnosis. AI may reduce cost and improve\
    \ value at the stages of image acquisition, interpretation, and decision-making.\
    \ Moreover, the precision now possible with cardiovascular imaging, combined with\
    \ \xE2\u20AC\u0153big data\xE2\u20AC\x9D from the electronic health record and\
    \ pathology, is likely to better characterize disease and personalize therapy.\
    \ This review summarizes recent promising applications of AI in cardiology and\
    \ cardiac imaging, which potentially add value to patient care."
  Author: Damini Dey and Piotr J. Slomka and Paul Leeson and Dorin Comaniciu and Sirish
    Shrestha and Partho P. Sengupta and Thomas H. Marwick
  Book Title/Journal: Journal of the American College of Cardiology
  DOI: https://doi.org/10.1016/j.jacc.2018.12.054
  JCS_FACTOR: 24.094
  Keywords: artificial intelligence, cardiovascular imaging, deep learning, machine
    learning
  SCI_FACTOR: 10.315
  TITLE_UPPER: JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY
  Title: 'Artificial Intelligence in Cardiovascular Imaging: JACC State-of-the-Art
    Review'
  Title_JCS: JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY
  Title_SCI: Journal of the American College of Cardiology
  Type Publication: article
  Year: 2019
- Abstract: The advancement of various research sectors such as Internet of Things
    (IoT), Machine Learning, Data Mining, Big Data, and Communication Technology has
    shed some light in transforming an urban city integrating the aforementioned techniques
    to a commonly known term - Smart City. With the emergence of smart city, plethora
    of data sources have been made available for wide variety of applications. The
    common technique for handling multiple data sources is data fusion, where it improves
    data output quality or extracts knowledge from the raw data. In order to cater
    evergrowing highly complicated applications, studies in smart city have to utilize
    data from various sources and evaluate their performance based on multiple aspects.
    To this end, we introduce a multi-perspectives classification of the data fusion
    to evaluate the smart city applications. Moreover, we applied the proposed multi-perspectives
    classification to evaluate selected applications in each domain of the smart city.
    We conclude the paper by discussing potential future direction and challenges
    of data fusion integration.
  Author: Billy Pik Lik Lau and Sumudu Hasala Marakkalage and Yuren Zhou and Naveed
    Ul Hassan and Chau Yuen and Meng Zhang and U-Xuan Tan
  Book Title/Journal: Information Fusion
  DOI: https://doi.org/10.1016/j.inffus.2019.05.004
  JCS_FACTOR: 12.975
  Keywords: Data fusion, Sensor fusion, Smart city, Big data, Internet of things,
    Multi-perspectives classification
  SCI_FACTOR: 2.776
  TITLE_UPPER: INFORMATION FUSION
  Title: A survey of data fusion in smart city applications
  Title_JCS: Information Fusion
  Title_SCI: Information Fusion
  Type Publication: article
  Year: 2019
- Abstract: "The critical factors in the big data era are collection, analysis, and\
    \ dissemination of information to improve an organization\xE2\u20AC\u2122s competitive\
    \ position and enhance its products and services. In this scenario, it is imperative\
    \ that organizations use Intelligence, which is understood as a process of gathering,\
    \ analyzing, interpreting, and disseminating high-value data and information at\
    \ the right time for use in the decision-making process. Earlier, the concept\
    \ of Intelligence was associated with the military and national security sector;\
    \ however, in present times, and as organizations evolve, Intelligence has been\
    \ defined in several ways for the purposes of different applications. Given that\
    \ the purpose of Intelligence is to obtain real value from data, information,\
    \ and the dynamism of the organizations, the study of this discipline provides\
    \ an opportunity to analyze the core trends related to data collection and processing,\
    \ information management, decision-making process, and organizational capabilities.\
    \ Therefore, the present study makes a conceptual analysis of the existing definitions\
    \ of intelligence in the literature by quantifying the main bibliometric performance\
    \ indicators, identifying the main authors and research areas, and evaluating\
    \ the development of the field using SciMAT as a bibliometric analysis software."
  Author: "J.R. L\xC3\xB3pez-Robles and J.R. Otegi-Olaso and I. {Porto G\xC3\xB3mez}\
    \ and M.J. Cobo"
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2019.01.013
  JCS_FACTOR: 14.098
  Keywords: Business Intelligence, Competitive Intelligence, Strategic Intelligence,
    Science information management, Mapping analysis
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: '30 years of intelligence models in management and business: A bibliometric
    review'
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: "The effect of height on pollen concentration is not well documented and\
    \ little is known about the near-ground vertical profile of airborne pollen. This\
    \ is important as most measuring stations are on roofs, but patient exposure is\
    \ at ground level. Our study used a big data approach to estimate the near-ground\
    \ vertical profile of pollen concentrations based on a global study of paired\
    \ stations located at different heights. We analyzed paired sampling stations\
    \ located at different heights between 1.5 and 50\xE2\u20AC\xAFm above ground\
    \ level (AGL). This provided pollen data from 59 Hirst-type volumetric traps from\
    \ 25 different areas, mainly in Europe, but also covering North America and Australia,\
    \ resulting in about 2,000,000 daily pollen concentrations analyzed. The daily\
    \ ratio of the amounts of pollen from different heights per location was used,\
    \ and the values of the lower station were divided by the higher station. The\
    \ lower station of paired traps recorded more pollen than the higher trap. However,\
    \ while the effect of height on pollen concentration was clear, it was also limited\
    \ (average ratio 1.3, range 0.7\xE2\u20AC\u201C2.2). The standard deviation of\
    \ the pollen ratio was highly variable when the lower station was located close\
    \ to the ground level (below 10\xE2\u20AC\xAFm AGL). We show that pollen concentrations\
    \ measured at >10\xE2\u20AC\xAFm are representative for background near-ground\
    \ levels."
  Author: "Jes\xC3\xBAs Rojo and Jose Oteros and Rosa P\xC3\xA9rez-Badia and Patricia\
    \ Cervig\xC3\xB3n and Zuzana Ferencova and A. Monserrat Guti\xC3\xA9rrez-Bustillo\
    \ and Karl-Christian Bergmann and Gilles Oliver and Michel Thibaudon and Roberto\
    \ Albertini and David {Rodr\xC3\xADguez-De la Cruz} and Estefan\xC3\xADa S\xC3\
    \xA1nchez-Reyes and Jos\xC3\xA9 S\xC3\xA1nchez-S\xC3\xA1nchez and Anna-Mari Pessi\
    \ and Jukka Reiniharju and Annika Saarto and M. Carmen Calder\xC3\xB3n and C\xC3\
    \xA9sar Guerrero and Daniele Berra and Maira Bonini and Elena Chiodini and Delia\
    \ Fern\xC3\xA1ndez-Gonz\xC3\xA1lez and Jos\xC3\xA9 Garc\xC3\xADa and M. Mar Trigo\
    \ and Dorota Myszkowska and Santiago Fern\xC3\xA1ndez-Rodr\xC3\xADguez and Rafael\
    \ Tormo-Molina and Athanasios Damialis and Franziska Kolek and Claudia Traidl-Hoffmann\
    \ and Elena Severova and Elsa Caeiro and Helena Ribeiro and Don\xC3\xA1t Magyar\
    \ and L\xC3\xA1szl\xC3\xB3 Makra and Orsolya Udvardy and Purificaci\xC3\xB3n Alc\xC3\
    \xA1zar and Carmen Gal\xC3\xA1n and Katarzyna Borycka and Idalia Kasprzyk and\
    \ Ed Newbigin and Beverley Adams-Groom and Godfrey P. Apangu and Carl A. Frisk\
    \ and Carsten A. Skj\xC3\xB8th and Predrag Radi\xC5\xA1i\xC4\u2021 and Branko\
    \ \xC5\_ikoparija and Sevcan Celenk and Carsten B. Schmidt-Weber and Jeroen Buters"
  Book Title/Journal: Environmental Research
  DOI: https://doi.org/10.1016/j.envres.2019.04.027
  JCS_FACTOR: 6.498
  Keywords: Height, Pollen, Aerobiology, Monitoring network, Big data
  SCI_FACTOR: 1.46
  TITLE_UPPER: ENVIRONMENTAL RESEARCH
  Title: Near-ground effect of height on pollen exposure
  Title_JCS: ENVIRONMENTAL RESEARCH
  Title_SCI: Environmental Research
  Type Publication: article
  Year: 2019
- Abstract: "Smart, real-time, low-cost, and distributed ecosystem monitoring is essential\
    \ for understanding and managing rapidly changing ecosystems. However, new techniques\
    \ in the big data era have rarely been introduced into operational ecosystem monitoring,\
    \ particularly for fragile ecosystems in remote areas. We introduce the Internet\
    \ of Things (IoT) techniques to establish a prototype ecosystem monitoring system\
    \ by developing innovative smart devices and using IoT technologies for ecosystem\
    \ monitoring in isolated environments. The developed smart devices include four\
    \ categories: large-scale and nonintrusive instruments to measure evapotranspiration\
    \ and soil moisture, in situ observing systems for CO2 and \xCE\xB413C associated\
    \ with soil respiration, portable and distributed devices for monitoring vegetation\
    \ variables, and Bi-CMOS cameras and pressure trigger sensors for terrestrial\
    \ vertebrate monitoring. These new devices outperform conventional devices and\
    \ are connected to each other via wireless communication networks. The breakthroughs\
    \ in the ecosystem monitoring IoT include new data loggers and long-distance wireless\
    \ sensor network technology that supports the rapid transmission of data from\
    \ devices to wireless networks. The applicability of this ecosystem monitoring\
    \ IoT is verified in three fragile ecosystems, including a karst rocky desertification\
    \ area, the National Park for Amur Tigers, and the oasis-desert ecotone in China.\
    \ By integrating these devices and technologies with an ecosystem monitoring information\
    \ system, a seamless data acquisition, transmission, processing, and application\
    \ IoT is created. The establishment of this ecosystem monitoring IoT will serve\
    \ as a new paradigm for ecosystem monitoring and therefore provide a platform\
    \ for ecosystem management and decision making in the era of big data."
  Author: Xin Li and Ning Zhao and Rui Jin and Shaomin Liu and Xiaomin Sun and Xuefa
    Wen and Dongxiu Wu and Yan Zhou and Jianwen Guo and Shiping Chen and Ziwei Xu
    and Mingguo Ma and Tianming Wang and Yonghua Qu and Xinwei Wang and Fangming Wu
    and Yuke Zhou
  Book Title/Journal: Science Bulletin
  DOI: https://doi.org/10.1016/j.scib.2019.07.004
  JCS_FACTOR: 11.78
  Keywords: Ecosystem monitoring, Fragile ecosystem, Internet of Things, Wireless
    sensor network, Smart device
  SCI_FACTOR: 1.983
  TITLE_UPPER: SCIENCE BULLETIN
  Title: Internet of Things to network smart devices for ecosystem monitoring
  Title_JCS: Science Bulletin
  Title_SCI: Science Bulletin
  Type Publication: article
  Year: 2019
- Abstract: In recent years an increasing number of academic disciplines, including
    IS, have sourced digital trace data for their research. Notwithstanding the potential
    of such data in (re)investigations of various phenomena of interest that would
    otherwise be difficult or impossible to study using other sources of data, we
    view the quality of digital trace data as an underappreciated issue in IS research.
    To initiate a discussion of how to evaluate and report on the quality of digital
    trace data in IS research, we couch our arguments within the broader tradition
    of research on data quality. We explain how the uncontrolled nature of digital
    trace data creates unique challenges for IS researchers, who need to collect,
    store, retrieve, and transform those data for the purpose of numerical analysis.
    We then draw parallels with concepts and patterns commonly used in data analysis
    projects and argue that, although IS researchers probably apply such concepts
    and patterns, this is not reported in publications, undermining the reader's ability
    to assess the reliability, statistical power and replicability of the findings.
    Using the case of GitHub to illustrate such challenges, we develop a preliminary
    set of guidelines to help researchers consider and report on the quality of the
    digital trace data they use in their research. Our work contributes to the debate
    on data quality and provides relevant recommendations for scholars and IS journals
    at a time when a growing number of publications are relying on digital trace data.
  Author: Gregory Vial
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.113133
  JCS_FACTOR: 5.795
  Keywords: Digital trace data, Data quality, GitHub
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: Reflections on quality requirements for digital trace data in IS research
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: The significance of distributed data warehouses is to initiate the proliferation
    of various analytical applications. However, with the increase of ubiquitous devices,
    it is likely that massive volumes of data will be generated, which poses further
    problems based on the degradation of data quality. The practical reasons for the
    degradation of data quality in distributed warehouses are identified as heterogeneous
    data, uncertain inferior data which further affect predictions. The proposed system
    presents an integrated optimization model to address all the quality degradation
    problems and to provide a better computational model which effectively incorporates
    a higher degree of quality assurance. An analytical methodology is adopted in
    order to develop the proposed quality assurance model for distributed data warehouses.
  Author: P. Amuthabala and R. Santhosh
  Book Title/Journal: Computers & Electrical Engineering
  DOI: https://doi.org/10.1016/j.compeleceng.2019.02.003
  JCS_FACTOR: 3.818
  Keywords: Data warehouse, Distributed, Data complexity, Data quality, Quality assurance,
    Optimization, Machine learning
  SCI_FACTOR: 0.0
  TITLE_UPPER: COMPUTERS & ELECTRICAL ENGINEERING
  Title: Robust analysis and optimization of a novel efficient quality assurance model
    in data warehousing
  Title_JCS: COMPUTERS & ELECTRICAL ENGINEERING
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Nowadays, IoT, cloud computing, mobile and social networks are generating
    a transformation in social processes. Nevertheless, this technological change
    rise to new threats and security attacks that produce new and complex cybersecurity
    scenarios with large volumes of data and different attack vectors that can exceeded
    the cognitive skills of security analysts. In this context, cognitive sciences
    can enhance the cognitive processes, which can help to security analysts to establish
    actions in less time and more efficiently within cybersecurity operations. This
    works presents a cognitive security model that integrates technological solutions
    such as Big Data, Machine Learning, and Support Decision Systems with the cognitive
    processes of security analysts used to generate knowledge, understanding and execution
    of security response actions. The model considers alternatives to establish the
    automation process in the execution of cognitive tasks defined in the cyber operations
    processes and includes the analyst as the central axis in the processes of validation
    and decision making through the use of MAPE-K, OODA and Human in the Loop.
  Author: Roberto O Andrade and Sang Guun Yoo
  Book Title/Journal: Journal of Information Security and Applications
  DOI: https://doi.org/10.1016/j.jisa.2019.06.008
  JCS_FACTOR: 3.872
  Keywords: Cognitive security, Cognitive science, Situation awareness, Cyber operations
  SCI_FACTOR: 0.61
  TITLE_UPPER: JOURNAL OF INFORMATION SECURITY AND APPLICATIONS
  Title: 'Cognitive security: A comprehensive study of cognitive science in cybersecurity'
  Title_JCS: Journal of Information Security and Applications
  Title_SCI: Journal of Information Security and Applications
  Type Publication: article
  Year: 2019
- Abstract: In the past, data in which science and engineering is based, was scarce
    and frequently obtained by experiments proposed to verify a given hypothesis.
    Each experiment was able to yield only very limited data. Today, data is abundant
    and abundantly collected in each single experiment at a very small cost. Data-driven
    modeling and scientific discovery is a change of paradigm on how many problems,
    both in science and engineering, are addressed. Some scientific fields have been
    using artificial intelligence for some time due to the inherent difficulty in
    obtaining laws and equations to describe some phenomena. However, today data-driven
    approaches are also flooding fields like mechanics and materials science, where
    the traditional approach seemed to be highly satisfactory. In this paper we review
    the application of data-driven modeling and model learning procedures to different
    fields in science and engineering.
  Author: "Francisco J. Mont\xC3\xA1ns and Francisco Chinesta and Rafael G\xC3\xB3\
    mez-Bombarelli and J. Nathan Kutz"
  Book Title/Journal: "Comptes Rendus M\xC3\xA9canique"
  DOI: https://doi.org/10.1016/j.crme.2019.11.009
  JCS_FACTOR: 0.0
  Keywords: Data-driven science, Data-driven modeling, Artificial intelligence, Machine
    learning, Data-science, Big data
  SCI_FACTOR: 0.0
  TITLE_UPPER: "COMPTES RENDUS M\xC3\xA9CANIQUE"
  Title: Data-driven modeling and learning in science and engineering
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: Insurance plays a crucial role in human efforts to adapt to environmental
    hazards. Effective insurance can serve as both a measure to distribute, and a
    method to communicate risk. In order for insurance to fulfil these roles successfully,
    policy pricing and cover choices must be risk-based and founded on accurate information.
    This is reliant on a robust evidence base forming the foundation of policy choices.
    This paper focuses on the evidence available to insurers and emergent innovation
    in the use of data. The main risk considered is coastal flooding, for which the
    insurance sector offers an option for potential adaptation, capable of increasing
    resilience. However, inadequate supply and analysis of data have been highlighted
    as factors preventing insurance from fulfilling this role. Research was undertaken
    to evaluate how data are currently, and could potentially, be used within risk
    evaluations for the insurance industry. This comprised of 50 interviews with those
    working and associated with the London insurance market. The research reveals
    new opportunities, which could facilitate improvements in risk-reflective pricing
    of policies. These relate to a new generation of data collection techniques and
    analytics, such as those associated with satellite-derived data, IoT (Internet
    of Things) sensors, cloud computing, and Big Data solutions. Such technologies
    present opportunities to reduce moral hazard through basing predictions and pricing
    of risk on large empirical datasets. The value of insurers' claims data is also
    revealed, and is shown to have the potential to refine, calibrate, and validate
    models and methods. The adoption of such data-driven techniques could enable insurers
    to re-evaluate risk ratings, and in some instances, extend coverage to locations
    and developments, previously rated as too high a risk to insure. Conversely, other
    areas may be revealed more vulnerable, which could generate negative impacts for
    residents in these regions, such as increased premiums. However, the enhanced
    risk awareness generated, by new technology, data and data analytics, could positively
    alter future planning, development and investment decisions.
  Author: Alexander G. Rumson and Stephen H. Hallett
  Book Title/Journal: Science of The Total Environment
  DOI: https://doi.org/10.1016/j.scitotenv.2019.01.114
  JCS_FACTOR: 7.963
  Keywords: Risk analytics, Adaptation, Remote sensing, Big Data
  SCI_FACTOR: 1.795
  TITLE_UPPER: SCIENCE OF THE TOTAL ENVIRONMENT
  Title: Innovations in the use of data facilitating insurance as a resilience mechanism
    for coastal flood risk
  Title_JCS: SCIENCE OF THE TOTAL ENVIRONMENT
  Title_SCI: Science of the Total Environment
  Type Publication: article
  Year: 2019
- Abstract: The increased of cybercrime incidents taking place in the world is at
    its perilous magnitude causing losses in term of money and trust. Even though
    there are various cybersecurity solutions in place; the threat of cybercrime is
    still a hard problem. Exploration of cybercrime challenges, especially the preventions
    and detections of the cybercrime should be investigated by composing all the stakeholders
    and players of a cybercrime issue. In this paper; an exploration of several cybercrime
    stakeholders is done. It is argued that cybercrime is a systemic threat and cannot
    be tackled with cybersecurity and legal systems. The architectural model proposed
    is significant and should become one of the considered milestones in designing
    security control in tackling cybercrime globally.
  Author: Manmeet Mahinderjit Singh and Anizah Abu Bakar
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.11.227
  JCS_FACTOR: 0.0
  Keywords: Big data, Internet of Things (IoT), Cyberspace, Routine Activity Theory,
    Systemic
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: A Systemic Cybercrime Stakeholders Architectural Model
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: The fourth industrial revolution considers data as a business asset and
    therefore this is placed as a central element of the software architecture (data
    as a service) that will support the horizontal and vertical digitalization of
    industrial processes. The large volume of data that the environment generates,
    its heterogeneity and complexity, as well as its reuse for later processes (e.g.
    analytics, IA) requires the adoption of policies, directives and standards for
    its right governance. Furthermore, the issues related to the use of resources
    in the cloud computing must be taken into account with the aim of meeting the
    requirements of performance and security of the different processes. This article,
    in the absence of frameworks adapted to this new architecture, proposes an initial
    schema for developing an effective data governance programme for third generation
    platforms, that means, a conceptual tool which guides organizations to define,
    design, develop and deploy services aligned with its vision and business goals
    in I4.0 era.
  Author: Juan Yebenes and Marta Zorrilla
  Book Title/Journal: Procedia Computer Science
  DOI: https://doi.org/10.1016/j.procs.2019.04.082
  JCS_FACTOR: 0.0
  Keywords: Data governance, Industry 4.0, data bus architecture, cloud computing,
    IoT, big data
  SCI_FACTOR: 0.334
  TITLE_UPPER: PROCEDIA COMPUTER SCIENCE
  Title: Towards a Data Governance Framework for Third Generation Platforms
  Title_JCS: N/A
  Title_SCI: Procedia Computer Science
  Type Publication: article
  Year: 2019
- Abstract: Recent rapid technological advances are producing exposure data sets for
    which there are no available data quality assessment tools. At the same time,
    regulatory agencies are moving in the direction of data quality assessment for
    environmental risk assessment and decision-making. A transparent and systematic
    approach to evaluating exposure data will aid in those efforts. Any approach to
    assessing data quality must consider the level of quality needed for the ultimate
    use of the data. While various fields have developed approaches to assess data
    quality, there is as yet no general, user-friendly approach to assess both measured
    and modeled data in the context of a fit-for-purpose risk assessment. Here we
    describe ExpoQual, an instrument developed for this purpose which applies recognized
    parameters and exposure data quality elements from existing approaches for assessing
    exposure data quality. Broad data streams such as quantitative measured and modeled
    human exposure data as well as newer and developing approaches can be evaluated.
    The key strength of ExpoQual is that it facilitates a structured, reproducible
    and transparent approach to exposure data quality evaluation and provides for
    an explicit fit-for-purpose determination. ExpoQual was designed to minimize subjectivity
    and to include transparency in aspects based on professional judgment. ExpoQual
    is freely available on-line for testing and user feedback (exposurequality.com).
  Author: "Judy S. LaKind and Cian O\xE2\u20AC\u2122Mahony and Thomas Armstrong and\
    \ Rosalie Tibaldi and Benjamin C. Blount and Daniel Q. Naiman"
  Book Title/Journal: Environmental Research
  DOI: https://doi.org/10.1016/j.envres.2019.01.039
  JCS_FACTOR: 6.498
  Keywords: ExpoQual, BEES-C, Exposure, Human, Quality, Fit-for-purpose, Instrument,
    Biomonitoring, Model uncertainty
  SCI_FACTOR: 1.46
  TITLE_UPPER: ENVIRONMENTAL RESEARCH
  Title: 'ExpoQual: Evaluating measured and modeled human exposure data'
  Title_JCS: ENVIRONMENTAL RESEARCH
  Title_SCI: Environmental Research
  Type Publication: article
  Year: 2019
- Abstract: "Remote sensing image products (e.g. brightness of nighttime lights and\
    \ land cover/land use types) have been widely used to disaggregate census data\
    \ to produce gridded population maps for large geographic areas. The advent of\
    \ the geospatial big data revolution has created additional opportunities to map\
    \ population distributions at fine resolutions with high accuracy. A considerable\
    \ proportion of the geospatial data contains semantic information that indicates\
    \ different categories of human activities occurring at exact geographic locations.\
    \ Such information is often lacking in remote sensing data. In addition, the remarkable\
    \ progress in machine learning provides toolkits for demographers to model complex\
    \ nonlinear correlations between population and heterogeneous geographic covariates.\
    \ In this study, a typical type of geospatial big data, points-of-interest (POIs),\
    \ was combined with multi-source remote sensing data in a random forests model\
    \ to disaggregate the 2010 county-level census population data to 100\xE2\u20AC\
    \xAF\xC3\u2014\xE2\u20AC\xAF100\xE2\u20AC\xAFm grids. Compared with the WorldPop\
    \ population dataset, our population map showed higher accuracy. The root mean\
    \ square error for population estimates in Beijing, Shanghai, Guangzhou, and Chongqing\
    \ for this method and WorldPop were 27,829 and 34,193, respectively. The large\
    \ under-allocation of the population in urban areas and over-allocation in rural\
    \ areas in the WorldPop dataset was greatly reduced in this new population map.\
    \ Apart from revealing the effectiveness of POIs in improving population mapping,\
    \ this study promises the potential of geospatial big data for mapping other socioeconomic\
    \ parameters in the future."
  Author: Tingting Ye and Naizhuo Zhao and Xuchao Yang and Zutao Ouyang and Xiaoping
    Liu and Qian Chen and Kejia Hu and Wenze Yue and Jiaguo Qi and Zhansheng Li and
    Peng Jia
  Book Title/Journal: Science of The Total Environment
  DOI: https://doi.org/10.1016/j.scitotenv.2018.12.276
  JCS_FACTOR: 7.963
  Keywords: Points of interest, Population, Random forests, Nighttime light, China
  SCI_FACTOR: 1.795
  TITLE_UPPER: SCIENCE OF THE TOTAL ENVIRONMENT
  Title: Improved population mapping for China using remotely sensed and points-of-interest
    data within a random forests model
  Title_JCS: SCIENCE OF THE TOTAL ENVIRONMENT
  Title_SCI: Science of the Total Environment
  Type Publication: article
  Year: 2019
- Abstract: "Abstract\nIn the past ten years, the application of artificial intelligence\
    \ (AI) in biomedicine has increased rapidly, which roots in the rapid growth of\
    \ biomedicine data, the improvement of computing performance, and the development\
    \ of deep learning methods. At present, there are great difficulties in front\
    \ of AI for solving complex and comprehensive medical problems. Ontology can play\
    \ an important role in how to make machines have stronger intelligence and has\
    \ wider applications in the medical field. By using ontologies, (meta) data can\
    \ be standardized so that data quality is improved and more data analysis methods\
    \ can be introduced, data integration can be supported by the semantics relationships\
    \ which are specified in ontologies, and effective logic expression in nature\
    \ language can be better understood by machine. This can be a pathway to stronger\
    \ AI. Under this circumstance, the Chinese Conference on Biomedical Ontology and\
    \ Terminology was held in Beijing in autumn 2019, with the theme \xE2\u20AC\u0153\
    Making Machine Understand Data\xE2\u20AC\x9D. The success of this conference further\
    \ improves the development of ontology in the field of biomedical information\
    \ in China, and will promote the integration of Chinese ontology research and\
    \ application with the international standards and the findability, accessibility,\
    \ interoperability, and reusability(FAIR) Data Principle."
  Author: Xiaolin Yang and Zhe Wang and Hongjie Pan and Yan Zhu
  Book Title/Journal: Chinese Medical Sciences Journal
  DOI: https://doi.org/10.24920/003701
  JCS_FACTOR: 0.0
  Keywords: ontology, artificial intelligence, biomedicine, big data
  SCI_FACTOR: 0.215
  TITLE_UPPER: CHINESE MEDICAL SCIENCES JOURNAL
  Title: 'Ontology: Footstone for Strong Artificial Intelligence'
  Title_JCS: N/A
  Title_SCI: Chinese Medical Sciences Journal
  Type Publication: article
  Year: 2019
- Abstract: "Data quality tags are a means of informing decision makers about the\
    \ quality of the data they use from information systems. Unfortunately, data quality\
    \ tags have not been successfully adopted despite their potential to assist decision\
    \ makers. One reason for the non-adoption is that maintaining the tags is expensive\
    \ and time-consuming: having a tag that represents accuracy, for example, would\
    \ be massively time-consuming to measure because it requires some physical observation\
    \ of reality to check the true value. We argue that a useful surrogate tag for\
    \ accuracy can be created\xE2\u20AC\u201Dwithout having to physically measure\
    \ it\xE2\u20AC\u201Dby counting the number of times the data has been exposed\
    \ to an event that could cause it to become inaccurate. Experimental results show\
    \ that the tags can help to avoid problems caused by inaccuracies, and also to\
    \ help find the inaccuracies themselves."
  Author: Philip Woodall and Vaggelis Giannikas and Wenrong Lu and Duncan McFarlane
  Book Title/Journal: Decision Support Systems
  DOI: https://doi.org/10.1016/j.dss.2019.04.007
  JCS_FACTOR: 5.795
  Keywords: Data quality, Information quality, Accuracy, Metadata, Data analytics,
    Data tags
  SCI_FACTOR: 1.564
  TITLE_UPPER: DECISION SUPPORT SYSTEMS
  Title: 'Potential Problem Data Tagging: Augmenting information systems with the
    capability to deal with inaccuracies'
  Title_JCS: DECISION SUPPORT SYSTEMS
  Title_SCI: Decision Support Systems
  Type Publication: article
  Year: 2019
- Abstract: Energy performance certificates (EPC) were introduced in European Union
    to support reaching energy efficiency targets by informing actors in the building
    sector about energy efficiency in buildings. While EPC have become a core source
    of information about building energy, the domains of its applications have not
    been studied systematically. This partly explains the limitation of conventional
    EPC data quality studies that fail to expose the essential problems and secure
    effective use of the data. This study reviews existing applications of EPC data
    and proposes a new method for assessing the quality of EPCs using data analytics.
    Thirteen application domains were identified from systematic mapping of 79 papers,
    revealing increases in the number and complexity of studies and advances in applied
    data analysis techniques. The proposed data quality assurance method based on
    six validation levels was tested using four samples of EPC dataset for the case
    of Sweden. The analysis showed that EPC data can be improved through adding or
    revising the EPC features and assuring interoperability of EPC datasets. In conclusion,
    EPC data have wider applications than initially intended by the EPC policy instrument,
    placing stronger requirements on the quality and content of the data.
  Author: "Oleksii Pasichnyi and J\xC3\xB6rgen Wallin and Fabian Levihn and Hossein\
    \ Shahrokni and Olga Kordas"
  Book Title/Journal: Energy Policy
  DOI: https://doi.org/10.1016/j.enpol.2018.11.051
  JCS_FACTOR: 6.142
  Keywords: Energy performance certificate (EPC), Building energy efficiency, Data
    applications, Data quality, Sweden
  SCI_FACTOR: 2.093
  TITLE_UPPER: ENERGY POLICY
  Title: "Energy performance certificates \xE2\u20AC\u201D New opportunities for data-enabled\
    \ urban energy policy instruments?"
  Title_JCS: ENERGY POLICY
  Title_SCI: Energy Policy
  Type Publication: article
  Year: 2019
- Abstract: Energy performance certificates (EPC) were introduced in European Union
    to support reaching energy efficiency targets by informing actors in the building
    sector about energy efficiency in buildings. While EPC have become a core source
    of information about building energy, the domains of its applications have not
    been studied systematically. This partly explains the limitation of conventional
    EPC data quality studies that fail to expose the essential problems and secure
    effective use of the data. This study reviews existing applications of EPC data
    and proposes a new method for assessing the quality of EPCs using data analytics.
    Thirteen application domains were identified from systematic mapping of 79 papers,
    revealing increases in the number and complexity of studies and advances in applied
    data analysis techniques. The proposed data quality assurance method based on
    six validation levels was tested using four samples of EPC dataset for the case
    of Sweden. The analysis showed that EPC data can be improved through adding or
    revising the EPC features and assuring interoperability of EPC datasets. In conclusion,
    EPC data have wider applications than initially intended by the EPC policy instrument,
    placing stronger requirements on the quality and content of the data.
  Author: "Oleksii Pasichnyi and J\xC3\xB6rgen Wallin and Fabian Levihn and Hossein\
    \ Shahrokni and Olga Kordas"
  Book Title/Journal: Energy Policy
  DOI: https://doi.org/10.1016/j.enpol.2018.11.051
  JCS_FACTOR: 6.142
  Keywords: Energy performance certificate (EPC), Building energy efficiency, Data
    applications, Data quality, Sweden
  SCI_FACTOR: 2.093
  TITLE_UPPER: ENERGY POLICY
  Title: "Energy performance certificates \xE2\u20AC\u201D New opportunities for data-enabled\
    \ urban energy policy instruments?"
  Title_JCS: ENERGY POLICY
  Title_SCI: Energy Policy
  Type Publication: article
  Year: 2019
- Abstract: 'Big and open linked data are often mentioned together because storing,
    processing, and publishing large amounts of these data play an increasingly important
    role in today''s society. However, although this topic is described from the political,
    economic, and social points of view, a technical dimension, which is represented
    by big data analytics, is insufficient. The aim of this review article was to
    provide a theoretical background of big and open linked data analytics ecosystem
    and its essential elements. First, the key terms were introduced including related
    dimensions. Then, the key lifecycle phases were defined and involved stakeholders
    were identified. Finally, a conceptual framework was proposed. In contrast to
    previous research, the new ecosystem is formed by interactions of stakeholders
    in the following dimensions and their sub-dimensions: transparency, engagement,
    legal, technical, social, and economic. These relationships are characterized
    by the most important requisites and public policy choices affecting the data
    analytics ecosystem together with the key phases and activities of the data analytics
    lifecycle. The findings should contribute to relevant initiatives, strategies,
    and policies and their effective implementation.'
  Author: Martin Lnenicka and Jitka Komarkova
  Book Title/Journal: Government Information Quarterly
  DOI: https://doi.org/10.1016/j.giq.2018.11.004
  JCS_FACTOR: 7.279
  Keywords: Big and open linked data, Ecosystem approach, Dimensions, Data analytics
    lifecycle, Stakeholders, Conceptual framework
  SCI_FACTOR: 2.121
  TITLE_UPPER: GOVERNMENT INFORMATION QUARTERLY
  Title: 'Big and open linked data analytics ecosystem: Theoretical background and
    essential elements'
  Title_JCS: GOVERNMENT INFORMATION QUARTERLY
  Title_SCI: Government Information Quarterly
  Type Publication: article
  Year: 2019
- Abstract: The main challenge in distributing electronic health records (EHRs) for
    patient-centered research, market analysis, medicine investigation, healthcare
    data mining etc., is data privacy. Handling the large-scale data and preserving
    the privacy of patients has been a challenge to researchers for a long period
    of time. On the contrary, blockchain technology has alleviated some of the problems
    by providing a protected and distributed platform. Sadly, existing electronic
    health record (EHR) management systems suffer from data manipulation, delayed
    communication, and trustless cooperation in data collection, storage, and distribution.
    This chapter discusses the current issues of healthcare data privacy and existing
    and upcoming regulations on this sector. This chapter also includes an overview
    of the architecture, existing issues, and future scope of blockchain technology
    for successfully handling privacy and management of current and future medical
    records. This chapter also presents few blockchain solutions that advocate the
    future research scopes in healthcare, big data, and blockchain.
  Author: Md. Mehedi Hassan Onik and Satyabrata Aich and Jinhong Yang and Chul-Soo
    Kim and Hee-Cheol Kim
  Book Title/Journal: Big Data Analytics for Intelligent Healthcare Management
  DOI: https://doi.org/10.1016/B978-0-12-818146-1.00008-8
  JCS_FACTOR: 0.0
  Keywords: Blockchain, Big data, Healthcare, Data privacy, EHR security
  SCI_FACTOR: 0.0
  TITLE_UPPER: BIG DATA ANALYTICS FOR INTELLIGENT HEALTHCARE MANAGEMENT
  Title: 'Chapter 8 - Blockchain in Healthcare: Challenges and Solutions'
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: 'Context

    The need for business intelligence has led to advances in machine learning in
    the business domain, especially with the rise of big data analytics. However,
    the resulting predictive systems often fail to maintain a satisfactory level of
    performance in production. Besides, for predictive systems used in business-to-business
    scenarios, user trust is subject to the model performance. Therefore, the processes
    of creating, evaluating, and deploying machine learning systems in the business
    domain need innovative solutions to solve the critical challenges of assuring
    the quality of the resulting systems.

    Objective

    Applying machine learning in business-to-business situations imposes specific
    requirements. This paper aims at providing an integrated solution to businesses
    to help them transform their data into actions.

    Method

    The paper presents MLean, an end-to-end framework, that aims at guiding businesses
    in designing, developing, evaluating, and deploying business-to-business predictive
    systems. The framework employs the Lean Startup methodology and aims at maximizing
    the business value while eliminating wasteful development practices.

    Results

    To evaluate the proposed framework, with the help of our industrial partner, we
    applied the framework to a case study to build a predictive product. The case
    study resulted in a predictive system to predict the risks of software license
    cancellations. The system was iteratively developed and evaluated while adopting
    the management and end-user perspectives.

    Conclusion

    It is concluded that, in industry, it is important to be aware of the businesses
    requirements before considering the application of machine learning. The framework
    accommodates business perspective from the beginning to produce a holistic product.
    From the results of the case study, we think that this framework can help businesses
    define the right opportunities for applying machine learning, developing solutions,
    evaluating the effectiveness of these solutions, and maintaining their performance
    in production.'
  Author: Mona Nashaat and Aindrila Ghosh and James Miller and Shaikh Quader and Chad
    Marston
  Book Title/Journal: Information and Software Technology
  DOI: https://doi.org/10.1016/j.infsof.2019.05.009
  JCS_FACTOR: 2.73
  Keywords: Big data, Machine learning, Business-to-business, User trust, Case study
  SCI_FACTOR: 0.606
  TITLE_UPPER: INFORMATION AND SOFTWARE TECHNOLOGY
  Title: 'M-Lean: An end-to-end development framework for predictive models in B2B
    scenarios'
  Title_JCS: INFORMATION AND SOFTWARE TECHNOLOGY
  Title_SCI: Information and Software Technology
  Type Publication: article
  Year: 2019
- Abstract: 'Purpose

    Status epilepticus is an often apparently randomly occurring, life-threatening
    medical emergency which affects the quality of life in patients with epilepsy
    and their families. The purpose of this review is to summarize information on
    ambulatory seizure detection, seizure prediction, and status epilepticus prevention.

    Method

    Narrative review.

    Results

    Seizure detection devices are currently under investigation with regards to utility
    and feasibility in the detection of isolated seizures, mainly in adult patients
    with generalized tonic-clonic seizures, in long-term epilepsy monitoring units,
    and occasionally in the outpatient setting. Detection modalities include accelerometry,
    electrocardiogram, electrodermal activity, electroencephalogram, mattress sensors,
    surface electromyography, video detection systems, gyroscope, peripheral temperature,
    photoplethysmography, and respiratory sensors, among others. Initial detection
    results are promising, and improve even further, when several modalities are combined.
    Some portable devices have already been U.S. FDA approved to detect specific seizures.
    Improved seizure prediction may be attainable in the future given that epileptic
    seizure occurrence follows complex patient-specific non-random patterns. The combination
    of multimodal monitoring devices, big data sets, and machine learning may enhance
    patient-specific detection and predictive algorithms. The integration of these
    technological advances and novel approaches into closed-loop warning and treatment
    systems in the ambulatory setting may help detect seizures sooner, and tentatively
    prevent status epilepticus in the future.

    Conclusions

    Ambulatory monitoring systems are being developed to improve seizure detection
    and the quality of life in patients with epilepsy and their families.'
  Author: Marta Amengual-Gual and Adriana Ulate-Campos and Tobias Loddenkemper
  Book Title/Journal: Seizure
  DOI: https://doi.org/10.1016/j.seizure.2018.09.013
  JCS_FACTOR: 0.0
  Keywords: Epilepsy, Status epilepticus, Closed-loop systems, Machine learning, Seizure
    detection sensors, Automated seizure detection
  SCI_FACTOR: 0.0
  TITLE_UPPER: SEIZURE
  Title: Status epilepticus prevention, ambulatory monitoring, early seizure detection
    and prediction in at-risk patients
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: article
  Year: 2019
- Abstract: 'Although business analytics (BA) have been increasingly adopted into
    businesses, there is limited empirical research examining the drivers of each
    stage of BA adoption in organizations. Drawing upon technological-organizational-environmental
    framework and innovation diffusion process, we developed an integrative model
    to examine BA adoption processes and tested with 170 Korean firms. The analysis
    shows data-related technological characteristics derive all stages of BA adoption:
    initiation, adoption and assimilation. While organizational characteristics are
    associated with adoption and assimilation stage, only competition intensity in
    environmental characteristics is associated with initiation stage. Our findings
    help practitioners and researchers to understand what factors can enable companies
    to adopt BA in each stage.'
  Author: Dalwoo Nam and Junyeong Lee and Heeseok Lee
  Book Title/Journal: International Journal of Information Management
  DOI: https://doi.org/10.1016/j.ijinfomgt.2019.07.017
  JCS_FACTOR: 14.098
  Keywords: Business analytics, Innovation diffusion, Adoption process, Data infrastructure,
    Data quality management, Analytics centralization
  SCI_FACTOR: 2.77
  TITLE_UPPER: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title: 'Business analytics adoption process: An innovation diffusion perspective'
  Title_JCS: INTERNATIONAL JOURNAL OF INFORMATION MANAGEMENT
  Title_SCI: International Journal of Information Management
  Type Publication: article
  Year: 2019
- Abstract: In order to make Internet of things applications easily available and
    cost-effective, we aim at using low-cost hardware for typical measurement tasks,
    and in return putting more effort into the signal processing and data analysis.
    By the example of beverage recognition with a low-cost temperature-modulated gas
    sensor, we demonstrate the benefits of processing techniques in big data such
    as feature selection and dimensionality reduction. Specifically, we determine
    a subset of temperatures that yields good support vector machine classification
    results and thereby shortens the measurement process.
  Author: Matthias Dziubany and Marcel Garling and Anke Schmeink and Guido Burger
    and Guido Dartmann and Stefan Naumann and Klaus-Uwe Gollmer
  Book Title/Journal: Big Data Analytics for Cyber-Physical Systems
  DOI: https://doi.org/10.1016/B978-0-12-816637-6.00011-7
  JCS_FACTOR: 0.0
  Keywords: Artificial Nose, PCA, SVM, Feature selection, low cost
  SCI_FACTOR: 0.0
  TITLE_UPPER: BIG DATA ANALYTICS FOR CYBER-PHYSICAL SYSTEMS
  Title: Chapter 11 - Machine learning-based artificial nose on a low-cost IoT-hardware
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: The use of high-throughput, genome-wide assays in toxicoepigenetics is
    rapidly developing and expanding. With recent advances in experimental technologies,
    a great amount of multiomics epigenomic data has been generated requiring the
    development of correspondingly advanced bioinformatics approaches to analyze and
    interpret such big data. This chapter discusses analysis methods for current epigenomic
    assays available for use in toxicoepigenetic and novel bioinformatics approaches
    to interpret, visualize, and integrate a variety of epigenomic data and data resources.
    The epigenomic features covered include DNA methylation, DNA hydroxymethylation,
    histone modification, chromatin accessibility, and chromatin interaction. For
    each type of assay used to interrogate those features, bioinformatics tools for
    data quality control, epigenetic mark detection, comparative analysis, data visualization,
    functional analysis, and integrative analysis are suggested. Looking forward,
    it is anticipated that researchers in toxicoepigenomics will adopt newer techniques
    such as single-cell assays and the bioinformatics methods will continue to evolve.
  Author: Raymond G. Cavalcante and Tingting Qin and Maureen A. Sartor
  Book Title/Journal: Toxicoepigenetics
  DOI: https://doi.org/10.1016/B978-0-12-812433-8.00012-5
  JCS_FACTOR: 0.0
  Keywords: Epigenomic analysis, Toxicoepigenomics, Bisulfite sequencing, Chromatin
    accessibility, ChIP-seq, Integrative analysis, Chromosomal interactions
  SCI_FACTOR: 0.0
  TITLE_UPPER: TOXICOEPIGENETICS
  Title: Chapter 4-2 - Novel Bioinformatics Methods for Toxicoepigenetics
  Title_JCS: N/A
  Title_SCI: N/A
  Type Publication: incollection
  Year: 2019
- Abstract: "Internet of Things (IoT) is leading to a paradigm shift within the logistics\
    \ industry. Logistics services providers use sensor technologies such as GPS or\
    \ telemetry to track and manage their shipment processes. Additionally, they use\
    \ external data that contain critical information about events such as traffic,\
    \ accidents, and natural disasters. Correlating data from different sensors and\
    \ social media and performing analysis in real-time provide opportunities to predict\
    \ events and prevent unexpected delivery delay at run-time. However, collecting\
    \ and processing data from heterogeneous sources foster problems due to the variety\
    \ and velocity of data. In addition, processing data in real-time is heavily challenging\
    \ that it cannot be dealt with using conventional logistics information systems.\
    \ In this paper, we present a hybrid framework for processing massive volume of\
    \ data in batch style and real-time. Our framework is built upon Johnson\xE2\u20AC\
    \u2122s hierarchical clustering (HCL) algorithm which produces a dendrogram that\
    \ represents different clusters of data objects."
  Author: "Mohammed AlShaer and Yehia Taher and Rafiqul Haque and Mohand-Sa\xC3\xAF\
    d Hacid and Mohamed Dbouk"
  Book Title/Journal: Future Generation Computer Systems
  DOI: https://doi.org/10.1016/j.future.2019.02.044
  JCS_FACTOR: 0.0
  Keywords: Realtime processing, Clustering, Big data, Internet of Things, Logistics,
    Hierarchical clustering algorithm
  SCI_FACTOR: 1.262
  TITLE_UPPER: FUTURE GENERATION COMPUTER SYSTEMS
  Title: 'IBRIDIA: A hybrid solution for processing big logistics data'
  Title_JCS: N/A
  Title_SCI: Future Generation Computer Systems
  Type Publication: article
  Year: 2019
- Abstract: "Discrete event simulation is becoming increasingly important in the planning\
    \ and operation of complex manufacturing systems. A major problem with today\xE2\
    \u20AC\u2122s approach to manufacturing simulation studies is the collection and\
    \ processing of data from heterogeneous sources, because the data is often of\
    \ poor quality and does not contain all the necessary information for a simulation.\
    \ This work introduces a framework that uses a real-time indoor localization systems\
    \ (RTILS) as a central main data harmonizer, that is designed to feed production\
    \ data into a manufacturing simulation from a single source of truth. It is shown,\
    \ based on different data quality dimensions, how this contributes to a better\
    \ overall data quality in manufacturing simulation. Furthermore, a detailed overview\
    \ on which simulation inputs can be derived from the RTILS data is given."
  Author: Carina Mieth and Anne Meyer and Michael Henke
  Book Title/Journal: Procedia CIRP
  DOI: https://doi.org/10.1016/j.procir.2019.03.216
  JCS_FACTOR: 0.0
  Keywords: real-time indoor localization system, input data management, cyber-physical
    system, manufacturing simulation, digital twin
  SCI_FACTOR: 0.683
  TITLE_UPPER: PROCEDIA CIRP
  Title: Framework for the usage of data from real-time indoor localization systems
    to derive inputs for manufacturing simulation
  Title_JCS: N/A
  Title_SCI: Procedia CIRP
  Type Publication: article
  Year: 2019
