[{"Author": "Luo, Xin and Chen, Minzhi and Wu, Hao and Liu, Zhigang and Yuan, Huaqiang and Zhou, MengChu", "Title": "Adjusting Learning Depth in Nonnegative Latent Factorization of Tensors for Accurately Modeling Temporal Patterns in Dynamic QoS Data", "Keywords": "Tensors;Big Data;Quality of service;Computational efficiency;Machine learning;Web services;Algorithm;big data;dynamics;high-dimensional and incomplete (HDI) data;machine learning;missing data estimation;multichannel data;nonnegative latent factorization of tensors (NLFT);temporal pattern;quality of service (QoS);web service", "Abstract": "A nonnegative latent factorization of tensors (NLFT) model precisely represents the temporal patterns hidden in multichannel data emerging from various applications. It often adopts a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. However, learning depth in this algorithm is not adjustable, resulting in frequent training fluctuation or poor model convergence caused by overshooting. To address this issue, this study carefully investigates the connections between the performance of an NLFT model and its learning depth via SLF-NMUT to present a joint learning-depth-adjusting scheme for it. Based on this scheme, a Depth-adjusted Multiplicative Update on tensor algorithm is innovatively proposed, thereby achieving a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Empirical studies on two industrial data sets demonstrate that compared with the state-of-the-art NLFT models, a DNL model achieves significant accuracy gain when performing missing data estimation on a high-dimensional and incomplete tensor with high efficiency. Note to Practitioners\u00e2\u20ac\u201dMultichannel data are often encountered in various big-data-related applications. It is vital for a data analyzer to correctly capture the temporal patterns hidden in them for efficient knowledge acquisition and representation. This article focuses on analyzing temporal QoS data, which is a representative kind of multichannel data. To correctly extract their temporal patterns, an analyzer should correctly describe their nonnegativity. Such a purpose can be achieved by building a nonnegative latent factorization of tensors (NLFT) model relying on a single latent factor-dependent, nonnegative and multiplicative update on tensor (SLF-NMUT) algorithm. But its learning depth is not adjustable, making an NLFT model frequently suffer from severe fluctuations in its training error or even fail to converge. To address this issue, this study carefully investigates the learning rules for an NLFT model\u00e2\u20ac\u2122s decision parameters using an SLF-NMUT and proposes a joint learning-depth-adjusting scheme. This scheme manipulates the multiplicative terms in SLF-NMUT-based learning rules linearly and exponentially, thereby making the learning depth adjustable. Based on it, this study builds a novel depth-adjusted nonnegative latent-factorization-of-tensors (DNL) model. Compared with the existing NLFT models, a DNL model better represents multichannel data. It meets industrial needs well and can be used to achieve high performance in data analysis tasks like temporal-aware missing data estimation", "Year": 2021, "Type Publication": "article", "DOI": "10.1109/TASE.2020.3040400", "Book Title/Journal": "IEEE Transactions on Automation Science and Engineering", "TITLE_UPPER": "IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING", "Title_SCI": "IEEE Transactions on Automation Science and Engineering", "Title_JCS": "IEEE Transactions on Automation Science and Engineering", "SCI_FACTOR": 1.314, "JCS_FACTOR": 5.083}, {"Author": "Victor O.K. Li and Jacqueline C.K. Lam and Yang Han and Kenyon Chow", "Title": "A Big Data and Artificial Intelligence Framework for Smart and Personalized Air Pollution Monitoring and Health Management in Hong Kong", "Keywords": "Air Pollution Monitoring, Health Management, Artificial Intelligence, Big Data, PM, Personalization, Smart Behavioural Intervention, Health and Well-being Improvement", "Abstract": "All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.envsci.2021.06.011", "Book Title/Journal": "Environmental Science & Policy", "TITLE_UPPER": "ENVIRONMENTAL SCIENCE & POLICY", "Title_SCI": "N/A", "Title_JCS": "ENVIRONMENTAL SCIENCE & POLICY", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.581}, {"Author": "Donghui Hu and Yifan Li and Lixuan Pan and Meng Li and Shuli Zheng", "Title": "A blockchain-based trading system for big data", "Keywords": "Big data trading, Blockchain, Smart contract, Proxy re-encryption, Price negotiation, Value reward", "Abstract": "Data are an extremely important asset. Governments around the world encourage big data sharing and trading to promote the big data economy. However, existing data trading platforms are not fully trusted. Such platforms face the problems of a single point of failure (SPOF), opaque transactions, uncontrollability, untraceability, and issues of data privacy. Several blockchain-based big data trading methods have been proposed; however, they do not adequately address the security issues introduced by dishonesty in the data provider and data agent or the fairness of data revenue distribution and price bargaining. In this paper, we propose a blockchain-based decentralized data trading system in which data trading is completed by smart contract-based data matching, price negotiation, and reward assigning. Moreover, the proposed data trading system evaluates the data quality on the basis of three metrics, records the evaluation results in a side-chain, and distributes the data users\u00e2\u20ac\u2122 application revenue to the data provider according to the evaluated data quality. We verify the security, usability, and efficiency of the proposed big data trading system.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.comnet.2021.107994", "Book Title/Journal": "Computer Networks", "TITLE_UPPER": "COMPUTER NETWORKS", "Title_SCI": "Computer Networks", "Title_JCS": "Computer Networks", "SCI_FACTOR": 0.798, "JCS_FACTOR": 4.474}, {"Author": "Victor O.K. Li and Jacqueline C.K. Lam and Jiahuan Cui", "Title": "AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making", "Keywords": "empty", "Abstract": "AI and big data technologies have been increasingly deployed to process complex, heterogeneous, high-resolution environmental data, and generate results at greater speeds and higher accuracies to facilitate environmental decision-making. However, current attempts to develop reliable AI and big data technologies for environmental decision-making are still inadequate. In this special issue, AI for Social Good: AI and Big Data Approaches for Environmental Decision-Making, we attempt to address the following important questions: What are the conditions for AI and big data technologies to facilitate environmental decision-making? How can AI and big data be used to facilitate environmental decision-making? Do AI and big data serve those most at risk of environmental pollution? Who should own and govern AI and big data? This special issue brings together researchers in relevant fields of AI and environmental science to address these pertinent questions. First, we will review the existing works which attempt to address these four questions. Second, we summarize the significance and novelty of six articles included in our special issue in addressing these four questions. Finally, we highlight the important principles of AI for Social Good, which can help distinguish good from bad environmental decisions based on AI and big data technologies.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.envsci.2021.09.001", "Book Title/Journal": "Environmental Science & Policy", "TITLE_UPPER": "ENVIRONMENTAL SCIENCE & POLICY", "Title_SCI": "N/A", "Title_JCS": "ENVIRONMENTAL SCIENCE & POLICY", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.581}, {"Author": "Faheem Ullah and M. Ali Babar", "Title": "On the scalability of Big Data Cyber Security Analytics systems", "Keywords": "Big data, Cyber security, Adaptation, Scalability, Configuration parameter, Spark", "Abstract": "Big Data Cyber Security Analytics (BDCA) systems use big data technologies (e.g., Apache Spark) to collect, store, and analyse a large volume of security event data for detecting cyber-attacks. The volume of digital data in general and security event data in specific is increasing exponentially. The velocity with which security event data is generated and fed into a BDCA system is unpredictable. Therefore, a BDCA system should be highly scalable to deal with the unpredictable increase/decrease in the velocity of security event data. However, there has been little effort to investigate the scalability of BDCA systems to identify and exploit the sources of scalability improvement. In this paper, we first investigate the scalability of a Spark-based BDCA system with default Spark settings. We then identify Spark configuration parameters (e.g., execution memory) that can significantly impact the scalability of a BDCA system. Based on the identified parameters, we finally propose a parameter-driven adaptation approach, SCALER, for optimizing a system's scalability. We have conducted a set of experiments by implementing a Spark-based BDCA system on a large-scale OpenStack cluster. We ran our experiments with four security datasets. We have found that (i) a BDCA system with default settings of Spark configuration parameters deviates from ideal scalability by 59.5% (ii) 9 out of 11 studied Spark configuration parameters significantly impact scalability and (iii) SCALER improves the BDCA system's scalability by 20.8% compared to the scalability with default Spark parameter setting. The findings of our study highlight the importance of exploring the parameter space of the underlying big data framework (e.g., Apache Spark) for scalable cyber security analytics.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jnca.2021.103294", "Book Title/Journal": "Journal of Network and Computer Applications", "TITLE_UPPER": "JOURNAL OF NETWORK AND COMPUTER APPLICATIONS", "Title_SCI": "Journal of Network and Computer Applications", "Title_JCS": "JOURNAL OF NETWORK AND COMPUTER APPLICATIONS", "SCI_FACTOR": 1.145, "JCS_FACTOR": 6.281}, {"Author": "Sonia Cisneros-Cabrera and Anna-Valentini Michailidou and Sandra Sampaio and Pedro Sampaio and Anastasios Gounaris", "Title": "Experimenting with big data computing for scaling data quality-aware query processing", "Keywords": "Data quality-aware queries, Big data computing, Empirical evaluation", "Abstract": "Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.eswa.2021.114858", "Book Title/Journal": "Expert Systems with Applications", "TITLE_UPPER": "EXPERT SYSTEMS WITH APPLICATIONS", "Title_SCI": "Expert Systems with Applications", "Title_JCS": "EXPERT SYSTEMS WITH APPLICATIONS", "SCI_FACTOR": 1.368, "JCS_FACTOR": 6.954}, {"Author": "Jian Wang", "Title": "A novel oscillation identification method for grid-connected renewable energy based on big data technology", "Keywords": "Oscillation identification, Big data, Evidence theory, Support vector machine", "Abstract": "With the development of big data technology, power system has entered the era of data analysis. With the help of the massive data provided by the wide area measurement system, the power system can be easily evaluated, and the abnormal operation status can be detected and positioned. As the increase of renewable energy permeability, more new abnormal operating status have appeared in the system. Aimed at the abnormal operation state in the development of new energy, this paper proposes an oscillation location scheme based on evidence theory and support vector machine, which makes up for the limitation of single oscillation location method. The result of location analysis of oscillation energy method, oscillation phase difference method and forced oscillation phase difference location method is fused by evidence theory.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.egyr.2022.02.022", "Book Title/Journal": "Energy Reports", "TITLE_UPPER": "ENERGY REPORTS", "Title_SCI": "Energy Reports", "Title_JCS": "Energy Reports", "SCI_FACTOR": 1.199, "JCS_FACTOR": 6.87}, {"Author": "Jiadi Yin and Jinwei Dong and Nicholas A.S. Hamm and Zhichao Li and Jianghao Wang and Hanfa Xing and Ping Fu", "Title": "Integrating remote sensing and geospatial big data for urban land use mapping: A review", "Keywords": "Integration methods, Urban functional zone classification, Urban management, Land use", "Abstract": "Remote Sensing (RS) has been used in urban mapping for a long time; however, the complexity and diversity of urban functional patterns are difficult to be captured by RS only. Emerging Geospatial Big Data (GBD) are considered as the supplement to RS data, and help to contribute to our understanding of urban lands from physical aspects (i.e., urban land cover) to socioeconomic aspects (i.e., urban land use). Integrating RS and GBD could be an effective way to combine physical and socioeconomic aspects with great potential for high-quality urban land use classification. In this study, we reviewed the existing literature and focused on the state-of-the-art and perspective of the urban land use categorization by integrating RS and GBD. Specifically, the commonly used RS features (e.g., spectral, textural, temporal, and spatial features) and GBD features (e.g., spatial, temporal, semantic, and sequence features) were identified and analyzed in urban land use classification. The integration strategies for RS and GBD features were categorized into feature-level integration (FI) and decision-level integration (DI). To be more specific, the FI method integrates the RS and GBD features and classifies urban land use types using the integrated feature sets; the DI method processes RS and GBD independently and then merges the classification results based on decision rules. We also discussed other critical issues, including analysis unit setting, parcel segmentation, parcel labeling of land use types, and data integration. Our findings provide a retrospect of different features from RS and GBD, strategies of RS and GBD integration, and their pros and cons, which could help to define the framework for future urban land use mapping and better support urban planning, urban environment assessment, urban disaster monitoring and urban traffic analysis.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jag.2021.102514", "Book Title/Journal": "International Journal of Applied Earth Observation and Geoinformation", "TITLE_UPPER": "INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION", "Title_SCI": "International Journal of Applied Earth Observation and Geoinformation", "Title_JCS": "International Journal of Applied Earth Observation and Geoinformation", "SCI_FACTOR": 1.623, "JCS_FACTOR": 5.933}, {"Author": "David G. Rosado and Julio Moreno and Luis E. S\u00c3\u00a1nchez and Antonio Santos-Olmo and Manuel A. Serrano and Eduardo Fern\u00c3\u00a1ndez-Medina", "Title": "MARISMA-BiDa pattern: Integrated risk analysis for big data", "Keywords": "Big data, Risk assessment, Risk analysis, Information security, Security standards", "Abstract": "Data is one of the most important assets for all types of companies, which have undoubtedly grown their quantity and the ways of exploiting them. Big Data appears in this context as a set of technologies that manage data to obtain information that supports decision-making. These systems were not conceived to be secure, resulting in significant risks that must be controlled. Security risks in Big Data must be analyzed and managed in an appropriate manner to protect the system and secure the information and the data being handled. This paper proposes a risk analysis approach for Big Data environments, which is based on a security analysis methodology called MARISMA (Methodology for the Analysis of Risks on Information System), supported by a technological environment in the cloud (eMARISMA tool) already used by numerous clients. Both MARISMA and eMARISMA are specifically designed to be easily adapted to particular contexts, such as Big Data. Our proposal, called MARISMA-BiDa, is based on the main related standards, such as ISO/IEC 27,000 and 31,000, or the NIST Big Data reference architecture or ENISA and CSA recommendations for Big Data.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cose.2020.102155", "Book Title/Journal": "Computers & Security", "TITLE_UPPER": "COMPUTERS & SECURITY", "Title_SCI": "N/A", "Title_JCS": "COMPUTERS & SECURITY", "SCI_FACTOR": 0.0, "JCS_FACTOR": 4.438}, {"Author": "Jeou-Shyan Horng and Chih-Hsing Liu and Sheng-Fang Chou and Tai-Yi Yu and Da-Chian Hu", "Title": "Role of big data capabilities in enhancing competitive advantage and performance in the hospitality sector: Knowledge-based dynamic capabilities view", "Keywords": "Knowledge-based dynamic capabilities view, Big data capabilities, Knowledge management, Sustainability marketing, Social media, Big data strategy", "Abstract": "To address the unsolved problem of the mechanism underlying the effect of big data analytics capabilities on competitive advantage and performance, this study combined quantitative and qualitative methods to test the examined framework. The results of 257 questionnaires from hotel marketing managers and 19 semistructured interviews, confirm that big data analytics capabilities develop from big data strategies and knowledge management and enhance competitive advantage and performance through sustainability marketing. Moreover, social media enhance sustainability marketing and competitive advantage and performance. The original findings of the current research contribute to the development of big data, sustainability marketing, and social media.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jhtm.2022.02.026", "Book Title/Journal": "Journal of Hospitality and Tourism Management", "TITLE_UPPER": "JOURNAL OF HOSPITALITY AND TOURISM MANAGEMENT", "Title_SCI": "Journal of Hospitality and Tourism Management", "Title_JCS": "Journal of Hospitality and Tourism Management", "SCI_FACTOR": 1.31, "JCS_FACTOR": 5.959}, {"Author": "Sepideh {Bazzaz Abkenar} and Mostafa {Haghi Kashani} and Ebrahim Mahdipour and Seyed Mahdi Jameii", "Title": "Big data analytics meets social media: A systematic review of techniques, open issues, and future directions", "Keywords": "Social networks, Big data, Content analysis, Sentiment analysis, Systematic literature review", "Abstract": "Social Networking Services (SNSs) connect people worldwide, where they communicate through sharing contents, photos, videos, posting their first-hand opinions, comments, and following their friends. Social networks are characterized by velocity, volume, value, variety, and veracity, the 5\u00c2\u00a0V\u00e2\u20ac\u2122s of big data. Hence, big data analytic techniques and frameworks are commonly exploited in Social Network Analysis (SNA). By the ever-increasing growth of social networks, the analysis of social data, to describe and find communication patterns among users and understand their behaviors, has attracted much attention. In this paper, we demonstrate how big data analytics meets social media, and a comprehensive review is provided on big data analytic approaches in social networks to search published studies between 2013 and August 2020, with 74 identified papers. The findings of this paper are presented in terms of main journals/conferences, yearly distributions, and the distribution of studies among publishers. Furthermore, the big data analytic approaches are classified into two main categories: Content-oriented approaches and network-oriented approaches. The main ideas, evaluation parameters, tools, evaluation methods, advantages, and disadvantages are also discussed in detail. Finally, the open challenges and future directions that are worth further investigating are discussed.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.tele.2020.101517", "Book Title/Journal": "Telematics and Informatics", "TITLE_UPPER": "TELEMATICS AND INFORMATICS", "Title_SCI": "Telematics and Informatics", "Title_JCS": "TELEMATICS AND INFORMATICS", "SCI_FACTOR": 1.567, "JCS_FACTOR": 6.182}, {"Author": "El Mehdi Ouafiq and Rachid Saadane and Abdellah Chehri and Seunggil Jeon", "Title": "AI-based modeling and data-driven evaluation for smart farming-oriented big data architecture using IoT with energy harvesting capabilities", "Keywords": "Smart Farming, Energy Harvesting Capabilities, IoT, Big Data, Agriculture 4.0, Water Management, Sustainability", "Abstract": "The use of Internet of Things (IoT) networks offers great advantages over wired networks, especially due to their simple installation, low maintenance costs, and automatic configuration. IoT facilitates the integration of sensing and communication for various industries, including smart farming and precision agriculture. For several years, many researchers have strived to find new sources of energy that are always \u00e2\u20ac\u0153cleaner\u00e2\u20ac\u009d and more environmentally friendly. Energy harvesting technology is one of the most promising environment-friendly solutions that extend the lifetime of these IoT devices. In this paper, the state-of-art of IoT energy harvesting capabilities and communication technologies in smart agriculture is presented. In addition, this work proposes a comprehensive architecture that includes big data technologies, IoT components, and knowledge-based systems for innovative farm architecture. The solution answers some of the biggest challenges the agriculture industry faces, especially when handling small files in a big data environment without impacting the computation performance. The solution is built on top of a pre-defined big data architecture that includes an abstraction layer of the data lake that handles data quality following a data migration strategy to ensure the data's insights. Furthermore, in this paper, we compared several machine learning algorithms to find the most suitable smart farming analytics tools in terms of forecasting and predictions.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.seta.2022.102093", "Book Title/Journal": "Sustainable Energy Technologies and Assessments", "TITLE_UPPER": "SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS", "Title_SCI": "Sustainable Energy Technologies and Assessments", "Title_JCS": "Sustainable Energy Technologies and Assessments", "SCI_FACTOR": 1.04, "JCS_FACTOR": 5.353}, {"Author": "Antonio Ben\u00c3\u00adtez-Hidalgo and Crist\u00c3\u00b3bal Barba-Gonz\u00c3\u00a1lez and Jos\u00c3\u00a9 Garc\u00c3\u00ada-Nieto and Pedro Guti\u00c3\u00a9rrez-Moncayo and Manuel Paneque and Antonio J. Nebro and Mar\u00c3\u00ada del Mar Rold\u00c3\u00a1n-Garc\u00c3\u00ada and Jos\u00c3\u00a9 F. Aldana-Montes and Ismael Navas-Delgado", "Title": "TITAN: A knowledge-based platform for Big Data workflow management", "Keywords": "Big Data analytics, Semantics, Knowledge extraction", "Abstract": "Modern applications of Big Data are transcending from being scalable solutions of data processing and analysis, to now provide advanced functionalities with the ability to exploit and understand the underpinning knowledge. This change is promoting the development of tools in the intersection of data processing, data analysis, knowledge extraction and management. In this paper, we propose TITAN, a software platform for managing all the life cycle of science workflows from deployment to execution in the context of Big Data applications. This platform is characterised by a design and operation mode driven by semantics at different levels: data sources, problem domain and workflow components. The proposed platform is developed upon an ontological framework of meta-data consistently managing processes and models and taking advantage of domain knowledge. TITAN comprises a well-grounded stack of Big Data technologies including Apache Kafka for inter-component communication, Apache Avro for data serialisation and Apache Spark for data analytics. A series of use cases are conducted for validation, which comprises workflow composition and semantic meta-data management in academic and real-world fields of human activity recognition and land use monitoring from satellite images.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.knosys.2021.107489", "Book Title/Journal": "Knowledge-Based Systems", "TITLE_UPPER": "KNOWLEDGE-BASED SYSTEMS", "Title_SCI": "Knowledge-Based Systems", "Title_JCS": "KNOWLEDGE-BASED SYSTEMS", "SCI_FACTOR": 1.587, "JCS_FACTOR": 8.038}, {"Author": "Lei Yang and Anqian Jiang and Jiahua Zhang", "Title": "Optimal timing of big data application in a two-period decision model with new product sales", "Keywords": "Supply chain management, Optimal strategy, Big data application, Two-period model, Social welfare", "Abstract": "We study a firm's strategy in adopting big data technology to motivate consumer demand over two periods. In the first period, the firm designs a product to sell to the market and determines whether to apply big data to attract more consumers. In the second period, the firm designs a new product and determines whether to sell the old product and the new product simultaneously, where big data can also be applied in this period to stimulate more demands. We formulate this problem into four models considering whether the firm adopts big data in the first period and/or the second period, and whether the firm only sells the new product or sells both the old and new products in the second period. We find that the firm prefers to apply big data over both periods when the cost is low, only over the second period when the cost is median and will not apply big data when the cost is high. Interestingly, only applying big data over the first period also may bring the most profits with heterogeneous big data coefficients. Furthermore, applying big data in the second period is the better choice for the social welfare.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cie.2021.107550", "Book Title/Journal": "Computers & Industrial Engineering", "TITLE_UPPER": "COMPUTERS & INDUSTRIAL ENGINEERING", "Title_SCI": "N/A", "Title_JCS": "COMPUTERS & INDUSTRIAL ENGINEERING", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.431}, {"Author": "Rakesh D. Raut and Vinay Surendra Yadav and Naoufel Cheikhrouhou and Vaibhav S. Narwane and Balkrishna E. Narkhede", "Title": "Big data analytics: Implementation challenges in Indian manufacturing supply chains", "Keywords": "Big data analytics, DEMATEL, Indian manufacturing supply chains, Interpretive structural modeling, MICMAC analysis", "Abstract": "Big Data Analytics (BDA) has attracted significant attention from both academicians and practitioners alike as it provides several ways to improve strategic, tactical and operational capabilities to eventually create a positive impact on the economic performance of organizations. In the present study, twelve significant barriers against BDA implementation are identified and assessed in the context of Indian manufacturing Supply Chains (SC). These barriers are modeled using an integrated two-stage approach, consisting of Interpretive Structural Modeling (ISM) in the first stage and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in the second stage. The approach developed provides the interrelationships between the identified constructs and their intensities. Moreover, Fuzzy MICMAC technique is applied to analyze the high impact (i.e., high driving power) barriers. Results show that four constructs, namely lack of top management support, lack of financial support, lack of skills, and lack of techniques or procedures, are the most significant barriers. This study aids policy-makers in conceptualizing the mutual interaction of the barriers for developing policies and strategies to improve the penetration of BDA in manufacturing SC.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compind.2020.103368", "Book Title/Journal": "Computers in Industry", "TITLE_UPPER": "COMPUTERS IN INDUSTRY", "Title_SCI": "Computers in Industry", "Title_JCS": "COMPUTERS IN INDUSTRY", "SCI_FACTOR": 1.432, "JCS_FACTOR": 7.635}, {"Author": "Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU", "Title": "scenario modeling for government big data governance decision-making: Chinese experience with public safety services", "Keywords": "Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services", "Abstract": "In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.im.2022.103622", "Book Title/Journal": "Information & Management", "TITLE_UPPER": "INFORMATION & MANAGEMENT", "Title_SCI": "N/A", "Title_JCS": "INFORMATION & MANAGEMENT", "SCI_FACTOR": 0.0, "JCS_FACTOR": 7.555}, {"Author": "Zhao-ge LIU and Xiang-yang LI and Xiao-han ZHU", "Title": "scenario modeling for government big data governance decision-making: Chinese experience with public safety services", "Keywords": "Government big data governance, Scenario-based decision-making, Scenario modeling, Model-driven, Data link network, Public safety services", "Abstract": "In the public safety service context, government big data governance (GBDG) is a challenging decision-making problem that encompasses uncertainties in the arenas of big data and its complex links. Modeling and collaborating the key scenario information required for GBDG decision-making can minimize system uncertainties. However, existing scenario-building methods are limited by their rigidity as they are employed in various application contexts and the associated high costs of modeling. In this paper, using a design science paradigm, a model-driven scenario modeling approach is proposed to achieve flexible scenario modeling for various applications through the transfer of generic domain knowledge. The key component of the proposed approach is a scenario meta-model that is built from existing literatures and practices by integrating qualitative, quantitative, and meta-modeling analysis. An instantiation mechanism of the scenario meta-model is also proposed to generate customized scenarios under Antecedent-Behavior-Consequence (ABC) theory. Two real-world safety service cases in Wuhan, China were evaluated to find that the proposed approach reduces GBDG decision-making uncertainties significantly by providing key information for GBDG problem identification, solution design, and solution value perception. This scenario-building approach can be further used to develop other GBDG systems for public safety services with reduced uncertainties and complete decision-making functions.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.im.2022.103622", "Book Title/Journal": "Information & Management", "TITLE_UPPER": "INFORMATION & MANAGEMENT", "Title_SCI": "N/A", "Title_JCS": "INFORMATION & MANAGEMENT", "SCI_FACTOR": 0.0, "JCS_FACTOR": 7.555}, {"Author": "Zhimei Lei and Yandan Chen and Ming K. Lim", "Title": "Modelling and analysis of big data platform group adoption behaviour based on social network analysis", "Keywords": "Big data, Platforms, Technology adoption, Corporate group behaviour, Social network analysis", "Abstract": "Due to the importance of big data technology in decision-making, production and service provision, enterprises have adopted various big data technologies and platforms to improve their operational efficiency. However, the number of enterprises that have adopted big data is not promising. The purpose of this study is to explore the current status of big data adoption by Chinese enterprises and to reveal the possible factors that hinder big data adoption from the group behaviour network perspective. Based on a real case survey of 54 big data platforms (BDPs), four types of networks\u00e2\u20ac\u201di.e., the enterprise-platform network, enterprise network, platform network and industry similarity and difference (ISD) network\u00e2\u20ac\u201dare constructed and analysed on the basis of social network analysis (SNA). This study finds that among Chinese enterprises, the level and scope of big data adoption are generally low and are imbalanced among industries; the cognitive level and adoption behaviour of enterprises on BDPs are inconsistent, the compatibility of BDPs is different, and the density and distance-based cohesion of networks are weak; although the current big data adoption behaviours of Chinese enterprises have formed some structural features, core-periphery structures and maximal complete cliques are found, and the current network structure has little impact on individual enterprises and platforms; enterprises in the same industry prefer to adopt the same kind of big data technology or platform. Based on these findings, several strategies and suggestions to improve big data adoption are provided.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.techsoc.2021.101570", "Book Title/Journal": "Technology in Society", "TITLE_UPPER": "TECHNOLOGY IN SOCIETY", "Title_SCI": "Technology in Society", "Title_JCS": "TECHNOLOGY IN SOCIETY", "SCI_FACTOR": 0.819, "JCS_FACTOR": 4.192}, {"Author": "Mayada Abd El-Aziz Youssef and Riyad Eid and Gomaa Agag", "Title": "Cross-national differences in big data analytics adoption in the retail industry", "Keywords": "Big data analytics, Technology adoption, Diffusion of innovations model, Cross-national differences, Retail industry", "Abstract": "Big data analytics (BDA) has emerged as a significant area of research for both researchers and practitioners in the retail industry, indicating the importance and influence of solving data-related problems in contemporary business organization. The present study utilised a quantitative-methods approach to investigate factors affecting retailers' adoption of BDA across three countries. A survey questionnaire was used to collect data from managers and decision-makers in the retail industry. Data of 2278 respondents were analysed through structural equation modelling. The findings revealed that security concerns, external support, top management support, and rational decision making culture have a greater effect on BDA adoption in developed countries UK than in UAE and Egypt. However, competition intensity and firm size have a greater effect on BDA adoption in UAE and Egypt than in UK. Finally, human variables (competence of information system's staff and staff's information system knowledge) have a greater effect on BDA adoption in Egypt than UK and UAE. The findings indicate that a \u00e2\u20ac\u0153one-size-fits-all\u00e2\u20ac\u009d approach is insufficient in capturing the heterogeneity of managers across countries. Implications for practice and theory were demonstrated.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jretconser.2021.102827", "Book Title/Journal": "Journal of Retailing and Consumer Services", "TITLE_UPPER": "JOURNAL OF RETAILING AND CONSUMER SERVICES", "Title_SCI": "Journal of Retailing and Consumer Services", "Title_JCS": "Journal of Retailing and Consumer Services", "SCI_FACTOR": 1.568, "JCS_FACTOR": 7.135}, {"Author": "Shuangqi Li and Pengfei Zhao", "Title": "Big data driven vehicle battery management method: A novel cyber-physical system perspective", "Keywords": "Electric vehicles, Battery energy storage, Cyber-physical battery management system, Big data, Deep learning", "Abstract": "The establishment of an accurate battery model is of great significance to improve the reliability of electric vehicles (EVs). However, the battery is a complex electrochemical system with hardly observable and simulatable internal chemical reactions, and it is challenging to estimate the state of battery accurately. This paper proposes a novel flexible and reliable battery management method based on the battery big data platform and Cyber-Physical System (CPS) technology. First of all, to integrate the battery big data resources in the cloud, a Cyber-physical battery management framework is defined and served as the basic data platform for battery modeling issues. And to improve the quality of the collected battery data in the database, this work reports the first attempt to develop an adaptive data cleaning method for the cloud battery management issue. Furthermore, a deep learning algorithm-based feature extraction model, as well as a feature-oriented battery modeling method, is developed to mitigate the under-fitting problem and improve the accuracy of the cloud-based battery model. The actual operation data of electric buses is used to validate the proposed methodologies. The maximum data restoring error can be limited within 1.3% in the experiments, which indicates that the proposed data cleaning method is able to improve the cloud battery data quality effectively. Meanwhile, the maximum SoC estimation error in the proposed feature-oriented battery modeling method is within 2.47%, which highlights the effectiveness of the proposed method.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.est.2020.102064", "Book Title/Journal": "Journal of Energy Storage", "TITLE_UPPER": "JOURNAL OF ENERGY STORAGE", "Title_SCI": "Journal of Energy Storage", "Title_JCS": "Journal of Energy Storage", "SCI_FACTOR": 1.088, "JCS_FACTOR": 6.583}, {"Author": "Yuguang Ye and Jianshe Shi and Daxin Zhu and Lianta Su and Jianlong Huang and Yifeng Huang", "Title": "Management of medical and health big data based on integrated learning-based health care system: A review and comparative analysis", "Keywords": "Integrated learning, Health care system, Elaboration Likelihood Machine, System design, Medical big data, Internet of Medical Things", "Abstract": "Purpose\nWe present a Health Care System (HCS) based on integrated learning to achieve high-efficiency and high-precision integration of medical and health big data, and compared it with an internet-based integrated system.\nMethod\nThe method proposed in this paper adopts the Bagging integrated learning method and the Extreme Learning Machine (ELM) prediction model to obtain a high-precision strong learning model. In order to verify the integration efficiency of the system, we compare it with the Internet-based health big data integration system in terms of integration volume, integration efficiency, and storage space capacity.\nResults\nThe HCS based on integrated learning relies on the Internet in terms of integration volume, integration efficiency, and storage space capacity. The amount of integration is proportional to the time and the integration time is between 170-450\u00c2\u00a0ms, which is only half of the comparison system; whereby the storage space capacity reaches 8.3\u00c3\u201428TB.\nConclusion\nThe experimental results show that the integrated learning-based HCS integrates medical and health big data with high integration volume and integration efficiency, and has high space storage capacity and concurrent data processing performance.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cmpb.2021.106293", "Book Title/Journal": "Computer Methods and Programs in Biomedicine", "TITLE_UPPER": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "Title_SCI": "Computer Methods and Programs in Biomedicine", "Title_JCS": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "SCI_FACTOR": 0.924, "JCS_FACTOR": 5.428}, {"Author": "Angelo Corallo and Anna Maria Crespino and Mariangela Lazoi and Marianna Lezzi", "Title": "Model-based Big Data Analytics-as-a-Service framework in smart manufacturing: A case study", "Keywords": "Big Data Analytics, BDA, MBDAaaS framework, Smart manufacturing, Industry 4.0, Anomaly detection", "Abstract": "Today, in a smart manufacturing environment based on the Industry 4.0 paradigm, people, technological infrastructure and machinery equipped with sensors can constantly generate and communicate a huge volume of data, also known as Big Data. The manufacturing industry takes advantage of Big Data and analytics evolution by improving its capability to bring out valuable information and knowledge from industrial processes, production systems and sensors. The adoption of model-based frameworks in the Big Data Analytics pipeline can better address user configuration requirements (e.g. type of analysis to perform, type of algorithm to be applied) and also provide more transparency and clearness on the execution of workflows and data processing. In the current state of art, an application of a model-based framework in a manufacturing scenario is missing. Therefore, in this study, by means of a case study research focused on data from sensors associated with Computer Numerical Control machines, the configuration and execution of a Big Data Analytics pipeline with a Model-based Big Data Analytics-as-a-Service framework is described. The case study provides to theoreticians and managerial experts useful evidence for managing real-time data analytics and deploying a workflow that addresses specific analytical goals, driven by user requirements and developer models, in a complex manufacturing domain.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.rcim.2022.102331", "Book Title/Journal": "Robotics and Computer-Integrated Manufacturing", "TITLE_UPPER": "ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING", "Title_SCI": "Robotics and Computer-Integrated Manufacturing", "Title_JCS": "ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING", "SCI_FACTOR": 1.561, "JCS_FACTOR": 5.666}, {"Author": "Yuncheng Shen and Bing Guo and Yan Shen and Xuliang Duan and Xiangqian Dong and Hong Zhang and Chuanwu Zhang and Yuming Jiang", "Title": "Personal big data pricing method based on differential privacy", "Keywords": "Personal big data, Data privacy, Privacy protection, Differential privacy, Positive pricing, Reverse pricing, Privacy budget, Privacy compensation", "Abstract": "Personal big data can greatly promote social management, business applications, and personal services, and bring certain economic benefits to users. The difficulty with personal big data security and privacy protection lies in realizing the maximization of the value of personal big data and in striking a balance between data privacy protection and sharing on the premise of satisfying personal big data security and privacy protection. Thus, in this paper, we propose a personal big data pricing method based on differential privacy (PMDP). We design two different mechanisms of positive and reverse pricing to reasonbly price personal big data. We perform aggregate statistics on an open dataset and extensively evaluated its performance. The experimental results show that PMDP can provide reasonable pricing for personal big data and fair compensation to data owners, ensuring an arbitrage-free condition and finding a balance between privacy protection and data utility.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cose.2021.102529", "Book Title/Journal": "Computers & Security", "TITLE_UPPER": "COMPUTERS & SECURITY", "Title_SCI": "N/A", "Title_JCS": "COMPUTERS & SECURITY", "SCI_FACTOR": 0.0, "JCS_FACTOR": 4.438}, {"Author": "Lihong Zhao", "Title": "Prediction model of ecological environmental water demand based on big data analysis", "Keywords": "Big data analysis, Ecological environment, Water demand, Prediction", "Abstract": "The existing prediction model of eco-environmental water demand has the problem of large prediction error. In order to solve the above problems, the prediction model of eco-environmental water demand is constructed based on big data analysis. In order to reduce the prediction error of the ecological environment water demand prediction model, the framework of the ecological environment water demand prediction model is built. On this basis, the principal component analysis method is used to select the auxiliary variables of the model. Based on the selected auxiliary variables, the minimum monthly average flow method is used to analyze the basic water demand of the ecological environment, the leakage water demand and the water surface evaporation ecological environment water demand, so as to analyze based on the results, the water demand of ecological environment is predicted by big data analysis technology, and the prediction of water demand of ecological environment is realized. The experimental results show that compared with the existing ecological environment water demand prediction model, the prediction error of the model is within 19.3, which fully shows that the constructed ecological environment water demand prediction model has better prediction effect and can provide a certain reference value for the actual use of water resources.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.eti.2020.101196", "Book Title/Journal": "Environmental Technology & Innovation", "TITLE_UPPER": "ENVIRONMENTAL TECHNOLOGY & INNOVATION", "Title_SCI": "N/A", "Title_JCS": "Environmental Technology & Innovation", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.263}, {"Author": "A Miracolo and M Mills and P Kanavos", "Title": "POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review", "Keywords": "empty", "Abstract": "empty", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jval.2021.11.1002", "Book Title/Journal": "Value in Health", "TITLE_UPPER": "VALUE IN HEALTH", "Title_SCI": "Value in Health", "Title_JCS": "VALUE IN HEALTH", "SCI_FACTOR": 1.859, "JCS_FACTOR": 5.725}, {"Author": "A Miracolo and M Mills and P Kanavos", "Title": "POSB319 Predictive Analytic Techniques and Big Data for Improved Health Outcomes in the Context of Value Based Health Care and Coverage Decisions: A Scoping Review", "Keywords": "empty", "Abstract": "empty", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jval.2021.11.1002", "Book Title/Journal": "Value in Health", "TITLE_UPPER": "VALUE IN HEALTH", "Title_SCI": "Value in Health", "Title_JCS": "VALUE IN HEALTH", "SCI_FACTOR": 1.859, "JCS_FACTOR": 5.725}, {"Author": "Yongqiang Lv and Lin Zhou and Guobiao Yao and Xinqi Zheng", "Title": "Detecting the true urban polycentric pattern of Chinese cities in morphological dimensions: A multiscale analysis based on geospatial big data", "Keywords": "Polycentricity, Urban centers, Multi-scale, Street blocks, Geospatial big data, Chinese cities", "Abstract": "With current decentralization trends and polycentric planning efforts, the urban spatial structures of Chinese cities have been changing tremendously. To detect the true urban polycentric pattern of Chinese cities, this article analyzed the urban polycentricity characteristics of 294 cities. The natural cities were delineated by points of interest (POIs), and road networks constituted street blocks. Based on check-in data and new spatial units, centers within both metropolitan areas and central cities were identified and examined. We discovered that all Chinese cities have at least one natural city in their metropolitan areas because of rapid urban sprawl. Although a monocentric structure is still the most common urban spatial structure, 110 Chinese cities displayed different degrees of polycentricity at the metropolitan level. Many natural cities beyond central cities contribute to polycentric development at the metropolitan level. Central cities have maintained their original vitality and importance, most Chinese cities have dispersed urban structures in central cities, and 45 central cities are polycentric. The spatial structures in metropolitan areas are more polycentric than those in central cities. The only 36 cities with polycentric urban structures at both the metropolitan and central city levels are all national or regional central cities in eastern China.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cities.2021.103298", "Book Title/Journal": "Cities", "TITLE_UPPER": "CITIES", "Title_SCI": "Cities", "Title_JCS": "CITIES", "SCI_FACTOR": 1.771, "JCS_FACTOR": 5.835}, {"Author": "Qiuping Ma and Hongyan Li and Anders Thorstenson", "Title": "A big data-driven root cause analysis system: Application of Machine Learning in quality problem solving", "Keywords": "Quality management, Data mining, Machine Learning, Multi-class classification, Neural Network", "Abstract": "Root cause analysis for quality problem solving is critical to improve product quality performance and reduce the quality risk for manufacturers. Subjective conventional methods have been applied frequently in past decades. However, due to increasingly complex product and supply chain structures, diverse working conditions, and massive amounts of components, accuracy and efficiency of root cause analysis are progressively challenged in practice. Therefore, data-driven root cause analysis methods have attracted attention lately. In this paper, taking advantage of the availability of big operations data and the rapid development of data science, we design a big data-driven root cause analysis system utilizing Machine Learning techniques to improve the performance of root cause analysis. More specifically, we first propose a conceptual framework of the big data-driven root cause analysis system including three modules of Problem Identification, Root Cause Identification, and Permanent Corrective Action. Furthermore, in the Problem Identification Module, we construct a unified feature-based approach to describe multiple and different types of quality problems by applying a data mining method. In the Root Cause Identification Module, we use supervised Machine Learning (classification) methods to automatically predict the root causes of multiple quality problems. Finally, we illustrate the accuracy and efficiency of the proposed system and algorithms based on actual quality data from a case company. This study contributes to the literature from the following aspects: (i) the integrated system and algorithms can be used directly to develop a computer application to manage and solve quality problems with high concurrences and complexities in any manufacturing process; (ii) a general procedure and method are provided to formulate and describe a large quantity and different types of quality problems; (iii) compared with traditional methods, it is demonstrated using real case data that manufacturing companies can save significant time and cost with our proposed data-driven root cause analysis system; (iv) this study not only aims at improving the quality problem solving practices for a complex manufacturing process but also bridges a gap between the theoretical development of Machining Learning methods and their application in the operations management domain.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cie.2021.107580", "Book Title/Journal": "Computers & Industrial Engineering", "TITLE_UPPER": "COMPUTERS & INDUSTRIAL ENGINEERING", "Title_SCI": "N/A", "Title_JCS": "COMPUTERS & INDUSTRIAL ENGINEERING", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.431}, {"Author": "Pietro Cozzini and Francesca Cavaliere and Giulia Spaggiari and Gianluca Morelli and Marco Riani", "Title": "Computational methods on food contact chemicals: Big data and in silico screening on nuclear receptors family", "Keywords": "Computational chemistry, Consensus prediction, Database, Nuclear receptors, Toxicology", "Abstract": "According to Eurostat, the EU production of chemicals hazardous to health reached 211 million tonnes in 2019. Thus, the possibility that some of these chemical compounds interact negatively with the human endocrine system has received, especially in the last decade, considerable attention from the scientific community. It is obvious that given the large number of chemical compounds it is impossible to use in vitro/in vivo tests for identifying all the possible toxic interactions of these chemicals and their metabolites. In addition, the poor availability of highly curated databases from which to retrieve and download the chemical, structure, and regulative information about all food contact chemicals has delayed the application of in silico methods. To overcome these problems, in this study we use robust computational approaches, based on a combination of highly curated databases and molecular docking, in order to screen all food contact chemicals against the nuclear receptor family in a cost and time-effective manner.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.chemosphere.2021.133422", "Book Title/Journal": "Chemosphere", "TITLE_UPPER": "CHEMOSPHERE", "Title_SCI": "Chemosphere", "Title_JCS": "CHEMOSPHERE", "SCI_FACTOR": 1.632, "JCS_FACTOR": 7.086}, {"Author": "Zhijiong Huang and Zhuangmin Zhong and Qinge Sha and Yuanqian Xu and Zhiwei Zhang and Lili Wu and Yuzheng Wang and Lihang Zhang and Xiaozhen Cui and MingShuang Tang and Bowen Shi and Chuanzeng Zheng and Zhen Li and Mingming Hu and Linlin Bi and Junyu Zheng and Min Yan", "Title": "An updated model-ready emission inventory for Guangdong Province by incorporating big data and mapping onto multiple chemical mechanisms", "Keywords": "Emission inventory, Guangdong Province, Ship emissions, Big data, VOCs speciation", "Abstract": "An accurate characterization of spatial-temporal emission patterns and speciation of volatile organic compounds (VOCs) for multiple chemical mechanisms is important to improving the air quality ensemble modeling. In this study, we developed a 2017-based high-resolution (3\u00c2\u00a0km\u00c2\u00a0\u00c3\u2014\u00c2\u00a03\u00c2\u00a0km) model-ready emission inventory for Guangdong Province (GD) by updating estimation methods, emission factors, activity data, and allocation profiles. In particular, a full-localized speciation profile dataset mapped to five chemical mechanisms was developed to promote the determination of VOC speciation, and two dynamic approaches based on big data were used to improve the estimation of ship emissions and open fire biomass burning (OFBB). Compared with previous emissions, more VOC emissions were classified as oxygenated volatile organic compound (OVOC) species, and their contributions to the total ozone formation potential (OFP) in the Pearl River Delta (PRD) region increased by 17%. Formaldehyde became the largest OFP species in GD, accounting for 11.6% of the total OFP, indicating that the model-ready emission inventory developed in this study is more reactive. The high spatial-temporal variability of ship sources and OFBB, which were previously underestimated, was also captured by using big data. Ship emissions during typhoon days and holidays decreased by 23\u00e2\u20ac\u201c55%. 95% of OFBB emissions were concentrated in 9% of the GD area and 31% of the days in 2017, demonstrating their strong spatial-temporal variability. In addition, this study revealed that GD emissions have changed rapidly in recent years due to the leap-forward control measures implemented, and thus, they needed to be updated regularly. All of these updates led to a 5\u00e2\u20ac\u201c17% decrease in the emission uncertainty for most pollutants. The results of this study provide a reference for how to reduce uncertainties in developing model-ready emission inventories.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.scitotenv.2020.144535", "Book Title/Journal": "Science of The Total Environment", "TITLE_UPPER": "SCIENCE OF THE TOTAL ENVIRONMENT", "Title_SCI": "Science of the Total Environment", "Title_JCS": "SCIENCE OF THE TOTAL ENVIRONMENT", "SCI_FACTOR": 1.795, "JCS_FACTOR": 7.963}, {"Author": "Mustafa Y\u00c4\u00b1ld\u00c4\u00b1r\u00c4\u00b1m and Feyza Y\u00c4\u00b1ld\u00c4\u00b1r\u00c4\u00b1m Okay and Suat \u00c3\u2013zdemir", "Title": "Big data analytics for default prediction using graph theory", "Keywords": "Big data analytics, Graph theory, Machine learning, Default prediction, SHAP value", "Abstract": "With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010\u00e2\u20ac\u201c2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.eswa.2021.114840", "Book Title/Journal": "Expert Systems with Applications", "TITLE_UPPER": "EXPERT SYSTEMS WITH APPLICATIONS", "Title_SCI": "Expert Systems with Applications", "Title_JCS": "EXPERT SYSTEMS WITH APPLICATIONS", "SCI_FACTOR": 1.368, "JCS_FACTOR": 6.954}, {"Author": "Dongfang Ren and Xiaopeng Guo and Cunbin Li", "Title": "Research on big data analysis model of multi energy power generation considering pollutant emission\u00e2\u20ac\u201dEmpirical analysis from Shanxi Province", "Keywords": "Multi-energy power generation, Pollutant emission, Big data analysis, Renewable energy generation, Thermal power", "Abstract": "With the development of the integrated energy Internet, energy structure optimization and emission reduction have led to higher requirements for developing various energy sources to enable coordinated and sustainable development. However, data-mining methods are rarely used to study the coordination of multi-energy generation in published research results. In this study, from the perspective of power industry emissions, coordinated generation of various energy sources, and balance of power generation and consumption, a data-mining algorithm was used to analyze the development of thermal power, hydropower, wind power, waste heat, gas, and other power sources. The chi-square automatic interaction detection tree (CHAID), logistic regression, and two-step clustering methods were applied. The results show that: a) CO2 and SO2 emissions were mainly affected by thermal power generation, whereas NOx emissions were jointly affected by thermal power, garbage power, and gas-fired power, and the emissions of various pollutants increased with an increase in power consumption. The optimal power-generation scheme under minimum emission can be obtained. b) There was a strong correlation between thermal power generation and residential electricity consumption, and renewable energy (wind energy, photovoltaic, hydropower) exhibited the highest correlation with the electricity consumption of the tertiary industry, which indicates that renewable energy generation can be promoted by managing electricity consumption in the tertiary industry. c) When the electricity demand of all users was small, the proportion of renewable energy power generation increased; in contrast, the thermal power generation was larger. This indicates the importance of improving the sustainable and stable power supply of renewable energy. This study provides a data analysis model for the coordinated development of multiple energies, which will contribute to the decision-making basis for controlling power emissions, improving the utilization rate of renewable energy, and optimizing the energy structure.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jclepro.2021.128154", "Book Title/Journal": "Journal of Cleaner Production", "TITLE_UPPER": "JOURNAL OF CLEANER PRODUCTION", "Title_SCI": "Journal of Cleaner Production", "Title_JCS": "Journal of Cleaner Production", "SCI_FACTOR": 1.937, "JCS_FACTOR": 9.297}, {"Author": "Arfan Majeed and Yingfeng Zhang and Shan Ren and Jingxiang Lv and Tao Peng and Saad Waqar and Enhuai Yin", "Title": "A big data-driven framework for sustainable and smart additive manufacturing", "Keywords": "Big data, Additive manufacturing, Sustainable manufacturing, Smart manufacturing, Optimization", "Abstract": "From the last decade, additive manufacturing (AM) has been evolving speedily and has revealed the great potential for energy-saving and cleaner environmental production due to a reduction in material and resource consumption and other tooling requirements. In this modern era, with the advancements in manufacturing technologies, academia and industry have been given more interest in smart manufacturing for taking benefits for making their production more sustainable and effective. In the present study, the significant techniques of smart manufacturing, sustainable manufacturing, and additive manufacturing are combined to make a unified term of sustainable and smart additive manufacturing (SSAM). The paper aims to develop framework by combining big data analytics, additive manufacturing, and sustainable smart manufacturing technologies which is beneficial to the additive manufacturing enterprises. So, a framework of big data-driven sustainable and smart additive manufacturing (BD-SSAM) is proposed which helped AM industry leaders to make better decisions for the beginning of life (BOL) stage of product life cycle. Finally, an application scenario of the additive manufacturing industry was presented to demonstrate the proposed framework. The proposed framework is implemented on the BOL stage of product lifecycle due to limitation of available resources and for fabrication of AlSi10Mg alloy components by using selective laser melting (SLM) technique of AM. The results indicate that energy consumption and quality of the product are adequately controlled which is helpful for smart sustainable manufacturing, emission reduction, and cleaner production.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.rcim.2020.102026", "Book Title/Journal": "Robotics and Computer-Integrated Manufacturing", "TITLE_UPPER": "ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING", "Title_SCI": "Robotics and Computer-Integrated Manufacturing", "Title_JCS": "ROBOTICS AND COMPUTER-INTEGRATED MANUFACTURING", "SCI_FACTOR": 1.561, "JCS_FACTOR": 5.666}, {"Author": "Sukwon Ji and Bumho Lee and Mun Yong Yi", "Title": "Building life-span prediction for life cycle assessment and life cycle cost using machine learning: A big data approach", "Keywords": "Building life span, Life cycle cost, Life cycle assessment, Big data, Machine learning, Deep neural network", "Abstract": "Life cycle assessment (LCA) and life cycle cost (LCC) are two primary methods used to assess the environmental and economic feasibility of building construction. An estimation of the building's life span is essential to carrying out these methods. However, given the diverse factors that affect the building's life span, it was estimated typically based on its main structural type. However, different buildings have different life spans. Simply assuming that all buildings with the same structural type follow an identical life span can cause serious estimation errors. In this study, we collected 1,812,700 records describing buildings built and demolished in South Korea, analysed the actual life span of each building, and developed a building life-span prediction model using deep-learning and traditional machine learning. The prediction models examined in this study produced root mean square errors of 3.72\u00e2\u20ac\u201c4.6 and the coefficients of determination of 0.932\u00e2\u20ac\u201c0.955. Among those models, a deep-learning based prediction model was found the most powerful. As anticipated, the conventional method of determining a building's life expectancy using a discrete set of specific factors and associated assumptions of life span did not yield realistic results. This study demonstrates that an application of deep learning to the LCA and LCC of a building is a promising direction, effectively guiding business planning and critical decision making throughout the construction process.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.buildenv.2021.108267", "Book Title/Journal": "Building and Environment", "TITLE_UPPER": "BUILDING AND ENVIRONMENT", "Title_SCI": "Building and Environment", "Title_JCS": "BUILDING AND ENVIRONMENT", "SCI_FACTOR": 1.736, "JCS_FACTOR": 6.456}, {"Author": "Jianguo Qian and Bingquan Zhu and Ying Li and Zhengchai Shi", "Title": "Visual recognition processing of power monitoring data based on big data computing", "Keywords": "Power control data, Monitoring, Visual identification, Iterative screening, CARIMA", "Abstract": "The operation control of power units is usually carried out by the control personnel with the help of distributed control system. Although it can ensure the safety of unit operation and meet the requirements of power generation loads, the economy of unit operation and the accuracy of control process still need to be further improved. Therefore, by designing multiple view mapping and association, it provides interactive visualization support for relevant experts in the key links of model establishment and evaluation. In the exploration stage of estimating model parameter, the user can get the delay range by line chart and focus + context technology, while in the model screening stage, the user can provide the combination of screening views, selecting the model by its accuracy on different data sets, and finding the model anomalies by the model structure view. Besides, in the model evaluation stage, the user can get the delay range by predicting line chart and model accuracy radar chart. In addition, the method in this paper keeps between 4.2\u00e2\u20ac\u201c7.2 in most distributions, and the maximum value is 18. The time series trend of the data segment is consistent, and the absolute value of the weight coefficient is basically 0 after being superimposed, which has great advantages compared with other methods, proving the effective results of the research content in this paper.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.egyr.2021.09.205", "Book Title/Journal": "Energy Reports", "TITLE_UPPER": "ENERGY REPORTS", "Title_SCI": "Energy Reports", "Title_JCS": "Energy Reports", "SCI_FACTOR": 1.199, "JCS_FACTOR": 6.87}, {"Author": "Kushan {De Silva} and Noel Mathews and Helena Teede and Andrew Forbes and Daniel J\u00c3\u00b6nsson and Ryan T. Demmer and Joanne Enticott", "Title": "Clinical notes as prognostic markers of mortality associated with diabetes mellitus following critical care: A retrospective cohort analysis using machine learning and unstructured big data", "Keywords": "Critical care, Diabetes, Electronic health records, LASSO, Machine learning, Mortality, Natural language processing, Prognosis, Text mining", "Abstract": "Background\nClinical notes are ubiquitous resources offering potential value in optimizing critical care via data mining technologies.\nObjective\nTo determine the predictive value of clinical notes as prognostic markers of 1-year all-cause mortality among people with diabetes following critical care.\nMaterials and methods\nMortality of diabetes patients were predicted using three cohorts of clinical text in a critical care database, written by physicians (n\u00c2\u00a0=\u00c2\u00a045253), nurses (159027), and both (n\u00c2\u00a0=\u00c2\u00a0204280). Natural language processing was used to pre-process text documents and LASSO-regularized logistic regression models were trained and tested. Confusion matrix metrics of each model were calculated and AUROC estimates between models were compared. All predictive words and corresponding coefficients were extracted. Outcome probability associated with each text document was estimated.\nResults\nModels built on clinical text of physicians, nurses, and the combined cohort predicted mortality with AUROC of 0.996, 0.893, and 0.922, respectively. Predictive performance of the models significantly differed from one another whereas inter-rater reliability ranged from substantial to almost perfect across them. Number of predictive words with non-zero coefficients were 3994, 8159, and 10579, respectively, in the models of physicians, nurses, and the combined cohort. Physicians\u00e2\u20ac\u2122 and nursing notes, both individually and when combined, strongly predicted 1-year all-cause mortality among people with diabetes following critical care.\nConclusion\nClinical notes of physicians and nurses are strong and novel prognostic markers of diabetes-associated mortality in critical care, offering potentially generalizable and scalable applications. Clinical text-derived personalized risk estimates of prognostic outcomes such as mortality could be used to optimize patient care.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compbiomed.2021.104305", "Book Title/Journal": "Computers in Biology and Medicine", "TITLE_UPPER": "COMPUTERS IN BIOLOGY AND MEDICINE", "Title_SCI": "Computers in Biology and Medicine", "Title_JCS": "COMPUTERS IN BIOLOGY AND MEDICINE", "SCI_FACTOR": 0.884, "JCS_FACTOR": 4.589}, {"Author": "\u00c3\u0081lvaro Valencia-Parra and Luisa Parody and \u00c3\u0081ngel Jes\u00c3\u00bas Varela-Vaca and Ismael Caballero and Mar\u00c3\u00ada Teresa G\u00c3\u00b3mez-L\u00c3\u00b3pez", "Title": "DMN4DQ: When data quality meets DMN", "Keywords": "Data usability, Data quality, Decision model and notation, Data quality rule, Data quality assessment, Data quality measurement", "Abstract": "To succeed in their business processes, organizations need data that not only attains suitable levels of quality for the task at hand, but that can also be considered as usable for the business. However, many researchers ground the potential usability of the data on its quality. Organizations would benefit from receiving recommendations on the usability of the data before its use. We propose that the recommendation on the usability of the data be supported by a decision process, which includes a context-dependent data-quality assessment based on business rules. Ideally, this recommendation would be generated automatically. Decision Model and Notation (DMN) enables the assessment of data quality based on the evaluation of business rules, and also, provides stakeholders (e.g., data stewards) with sound support for the automation of the whole process of generation of a recommendation regarding usability based on data quality. The main contribution of the proposal involves designing and enabling both DMN-driven mechanisms and a guiding methodology (DMN4DQ) to support the automatic generation of a decision-based recommendation on the potential usability of a data record in terms of its level of data quality. Furthermore, the validation of the proposal is performed through the application of a real dataset.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.dss.2020.113450", "Book Title/Journal": "Decision Support Systems", "TITLE_UPPER": "DECISION SUPPORT SYSTEMS", "Title_SCI": "Decision Support Systems", "Title_JCS": "DECISION SUPPORT SYSTEMS", "SCI_FACTOR": 1.564, "JCS_FACTOR": 5.795}, {"Author": "Zigeng Fang and Yan Liu and Qiuchen Lu and Michael Pitt and Sean Hanna and Zhichao Tian", "Title": "BIM-integrated portfolio-based strategic asset data quality management", "Keywords": "Strategic asset management (SAM), Building information modeling (BIM), Portfolio management, Data quality management", "Abstract": "A building's strategic asset management (SAM) capability has traditionally been limited by its site-based management. With the emergence of needs from clients about delivering a long-term portfolio-based building asset management plan that minimizes the asset risk and optimizes the value of their asset portfolios, SAM Units have emerged as a new business form to provide various SAM services to their clients. However, the quality of their current data model is still hindered by many issues, such as missing important attributes and the lack of customized information flow guidance. In addition, there is a gap in integrating their existing data collection with various data sources and Building Information Modeling (BIM) to enhance their data quality. By evaluating a SAM Unit's portfolio case study, this paper identifies the factors limiting the quality of SAM Units' data model and develops a guide to integrating various data sources better. We develop a BIM-integrated portfolio-based SAM information flow framework and a detailed hierarchical portfolio-based non-geometric data structure. The proposed framework and data structure will help SAM professionals, building asset owners, and other facilities management professionals embrace the benefits of managing the portfolio-based SAM data.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.autcon.2021.104070", "Book Title/Journal": "Automation in Construction", "TITLE_UPPER": "AUTOMATION IN CONSTRUCTION", "Title_SCI": "Automation in Construction", "Title_JCS": "AUTOMATION IN CONSTRUCTION", "SCI_FACTOR": 1.837, "JCS_FACTOR": 7.7}, {"Author": "Antonella D. Pontoriero and Giovanna Nordio and Rubaida Easmin and Alessio Giacomel and Barbara Santangelo and Sameer Jahuar and Ilaria Bonoldi and Maria Rogdaki and Federico Turkheimer and Oliver Howes and Mattia Veronese", "Title": "Automated Data Quality Control in FDOPA brain PET Imaging using Deep Learning", "Keywords": "FDOPA, PET, quality control, QC, convolutional neural networks", "Abstract": "ABSTRACT\nIntroduction. With biomedical imaging research increasingly using large datasets, it becomes critical to find operator-free methods to quality control the data collected and the associated analysis. Attempts to use artificial intelligence (AI) to perform automated quality control (QC) for both single-site and multi-site datasets have been explored in some neuroimaging techniques (e.g. EEG or MRI), although these methods struggle to find replication in other domains. The aim of this study is to test the feasibility of an automated QC pipeline for brain [18F]-FDOPA PET imaging as a biomarker for the dopamine system. Methods. Two different Convolutional Neural Networks (CNNs) were used and combined to assess spatial misalignment to a standard template and the signal-to-noise ratio (SNR) relative to 200 static [18F]-FDOPA PET images that had been manually quality controlled from three different PET/CT scanners. The scans were combined with an additional 400 scans, in which misalignment (200 scans) and low SNR (200 scans) were simulated. A cross-validation was performed, where 80% of the data were used for training and 20% for validation. Two additional datasets of [18F]-FDOPA PET images (50 and 100 scans respectively with at least 80% of good quality images) were used for out-of-sample validation. Results. The CNN performance was excellent in the training dataset (accuracy for motion: 0.86 \u00c2\u00b1 0.01, accuracy for SNR: 0.69 \u00c2\u00b1 0.01), leading to 100% accurate QC classification when applied to the two out-of-sample datasets. Data dimensionality reduction affected the generalizability of the CNNs, especially when the classifiers were applied to the out-of-sample data from 3D to 1D datasets. Conclusions. This feasibility study shows that it is possible to perform automatic QC of [18F]-FDOPA PET imaging with CNNs. The approach has the potential to be extended to other PET tracers in both brain and non-brain applications, but it is dependent on the availability of large datasets necessary for the algorithm training.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cmpb.2021.106239", "Book Title/Journal": "Computer Methods and Programs in Biomedicine", "TITLE_UPPER": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "Title_SCI": "Computer Methods and Programs in Biomedicine", "Title_JCS": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "SCI_FACTOR": 0.924, "JCS_FACTOR": 5.428}, {"Author": "Laura Mor\u00c3\u00a1n-Fern\u00c3\u00a1ndez and Ver\u00c3\u00b3nica B\u00c3\u00b3lon-Canedo and Amparo Alonso-Betanzos", "Title": "How important is data quality? Best classifiers vs best features", "Keywords": "Feature selection, Filters, Preprocessing, High dimensionality, Classification, Data analysis", "Abstract": "The task of choosing the appropriate classifier for a given scenario is not an easy-to-solve question. First, there is an increasingly high number of algorithms available belonging to different families. And also there is a lack of methodologies that can help on recommending in advance a given family of algorithms for a certain type of datasets. Besides, most of these classification algorithms exhibit a degradation in the performance when faced with datasets containing irrelevant and/or redundant features. In this work we analyze the impact of feature selection in classification over several synthetic and real datasets. The experimental results obtained show that the significance of selecting a classifier decreases after applying an appropriate preprocessing step and, not only this alleviates the choice, but it also improves the results in almost all the datasets tested.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.neucom.2021.05.107", "Book Title/Journal": "Neurocomputing", "TITLE_UPPER": "NEUROCOMPUTING", "Title_SCI": "Neurocomputing", "Title_JCS": "NEUROCOMPUTING", "SCI_FACTOR": 1.085, "JCS_FACTOR": 5.719}, {"Author": "Foyez Ahmed Prodhan and Jiahua Zhang and Shaikh Shamim Hasan and Til Prasad {Pangali Sharma} and Hasiba Pervin Mohana", "Title": "A review of machine learning methods for drought hazard monitoring and forecasting: Current research trends, challenges, and future research directions", "Keywords": "Machine learning, Deep learning, Forecasting, Drought, Big data", "Abstract": "Machine learning is a dynamic field with wide-ranging applications, including drought modeling and forecasting. Drought is a complex, devastating natural disaster for which it is challenging to develop effective prediction models. Therefore, our review focuses on basic information about machine learning methods (MLMs) and their potential applications in developing efficient and effective drought forecasting models. We observed that MLMs have achieved significant advances in the robustness, effectiveness, and accuracy of the algorithms for drought modelling in recent years. The performance comparison of MLMs with other models provides a comprehensive conception of different model evaluation metrics. Further challenges of MLMs, such as inadequate training data sets, noise, outliers, and observation bias for spatial data sets, are explored. Finally, our review conveys in-depth understanding to researchers on machine learning applications in forecasting and modeling and provides drought mitigation strategy guidance for policymakers.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.envsoft.2022.105327", "Book Title/Journal": "Environmental Modelling & Software", "TITLE_UPPER": "ENVIRONMENTAL MODELLING & SOFTWARE", "Title_SCI": "N/A", "Title_JCS": "ENVIRONMENTAL MODELLING & SOFTWARE", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.288}, {"Author": "Yong Zhou and Huanghui Gu and Teng Su and Xuebing Han and Languang Lu and Yuejiu Zheng", "Title": "Remaining useful life prediction with probability distribution for lithium-ion batteries based on edge and cloud collaborative computation", "Keywords": "Battery life prediction, RVM optimization prediction, Parameter transfer: RUL probability prediction", "Abstract": "This paper proposes the architecture of the combination of the battery management system (BMS) and the cloud big data platform. Firstly, BMS measures and extracts the mean voltage falloff (MVF). A regression model of capacity and MVF based on historical data is established with generalized Box-Cox Transformation and least squares. The capacity and MVF are uploaded to the cloud big data platform, and then the mean and variance of the MVF is predicted based on the relevance vector machine, thereby realizing the 2\u00cf\u0192 range prediction of the lithium battery's state of health and the probability density function prediction of the remaining useful life. This paper makes two contributions to the data-driven prediction method. First, the edge-cloud collaborative computing architecture combining BMS and cloud is proposed, which effectively utilizes the advantages of BMS data quality and cloud computing power. Second, through the combination of relevance vector machine with particle swarm optimization and horizontal parameter transfer, the number of samples required for model learning is reduced to 30% and has better accuracy and robustness. Through the verification of NASA data, the results show that the average error is less than 2.18%.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.est.2021.103342", "Book Title/Journal": "Journal of Energy Storage", "TITLE_UPPER": "JOURNAL OF ENERGY STORAGE", "Title_SCI": "Journal of Energy Storage", "Title_JCS": "Journal of Energy Storage", "SCI_FACTOR": 1.088, "JCS_FACTOR": 6.583}, {"Author": "Bing Wang", "Title": "Safety intelligence as an essential perspective for safety management in the era of Safety 4.0: From a theoretical to a practical framework", "Keywords": "Safety intelligence (SI), Safety big data, Safety 4.0, Safety management, Safety decision-making", "Abstract": "In the age of big data, intelligence, and Industry 4.0, intelligence plays an increasingly significant role in management or, more specifically, decision making; thus, it becomes a popular topic and is recognised as an important discipline. Hence, safety intelligence (SI) as a new safety concept and term was proposed. SI aims to transform raw safety data and information into meaningful and actionable information for safety management; it is considered an essential perspective for safety management in the era of Safety 4.0 (computational safety science\u00e2\u20ac\u201da new paradigm for safety science in the age of big data, intelligence, and Industry 4.0). However, thus far, no existing research provides a framework that comprehensively describes SI and guides the implementation of SI practices in organisations. To address this research gap and to provide a framework for SI and its practice in the context of safety management, based on a systematic and comprehensive explanation on SI from different perspectives, this study attempts to propose a theoretical framework for SI from a safety management perspective and then presents an SI practice model aimed at supporting safety management in organisations.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.psep.2020.10.008", "Book Title/Journal": "Process Safety and Environmental Protection", "TITLE_UPPER": "PROCESS SAFETY AND ENVIRONMENTAL PROTECTION", "Title_SCI": "Process Safety and Environmental Protection", "Title_JCS": "PROCESS SAFETY AND ENVIRONMENTAL PROTECTION", "SCI_FACTOR": 1.173, "JCS_FACTOR": 6.158}, {"Author": "Jiangchuan Fan and Ying Zhang and Weiliang Wen and Shenghao Gu and Xianju Lu and Xinyu Guo", "Title": "The future of Internet of Things in agriculture: Plant high-throughput phenotypic platform", "Keywords": "Internet of things in agriculture, Big data, High-throughput phenotype, Data mining", "Abstract": "With continuous collaborative research in sensor technology, communication technology, plant science, computer science and engineering science, Internet of Things (IoT) in agriculture has made a qualitative leap through environmental sensor networks, non-destructive imaging, spectral analysis, robotics, machine vision and laser radar technology. Physical and chemical analysis can continuously obtain environmental data, experimental metadata (including text, image and spectral, 3D point cloud and real-time growth data) through integrated automation platform equipment and technical means. Based on data on multi-scale, multi-environmental and multi-mode plant traits that constitute big data on plant phenotypes, genotype\u00e2\u20ac\u201cphenotype\u00e2\u20ac\u201cenvirotype relationship in the omics system can be explored deeply. Detailed information on the formation mechanism of specific biological traits can promote the process of functional genomics, plant molecular breeding and efficient cultivation. This study summarises the development background, research process and characteristics of high-throughput plant phenotypes. A systematic review of the research progress of IoT in agriculture and plant high-throughput phenotypes is conducted, including the acquisition and analysis of plant phenotype big data, phenotypic trait prediction and multi-recombination analysis based on plant phenomics. This study proposes key techniques for current plant phenotypes, and looks forward to the research on plant phenotype detection technology in the field environment, fusion and data mining of plant phenotype multivariate data, simultaneous observation of multi-scale phenotype platform and promotion of a comprehensive high-throughput phenotype technology.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jclepro.2020.123651", "Book Title/Journal": "Journal of Cleaner Production", "TITLE_UPPER": "JOURNAL OF CLEANER PRODUCTION", "Title_SCI": "Journal of Cleaner Production", "Title_JCS": "Journal of Cleaner Production", "SCI_FACTOR": 1.937, "JCS_FACTOR": 9.297}, {"Author": "Tanveer Ahmad and Dongdong Zhang and Chao Huang and Hongcai Zhang and Ningyi Dai and Yonghua Song and Huanxin Chen", "Title": "Artificial intelligence in sustainable energy industry: Status Quo, challenges and opportunities", "Keywords": "Artificial intelligence, Renewable energy, Energy demand, Decision making, Big data, Energy digitization", "Abstract": "The energy industry is at a crossroads. Digital technological developments have the potential to change our energy supply, trade, and consumption dramatically. The new digitalization model is powered by the artificial intelligence (AI) technology. The integration of energy supply, demand, and renewable sources into the power grid will be controlled autonomously by smart software that optimizes decision-making and operations. AI will play an integral role in achieving this goal. This study focuses on the use of AI techniques in the energy sector. This study aims to present a realistic baseline that allows researchers and readers to compare their AI efforts, ambitions, new state-of-the-art applications, challenges, and global roles in policymaking. We covered three major aspects, including: i) the use of AI in solar and hydrogen power generation; (ii) the use of AI in supply and demand management control; and (iii) recent advances in AI technology. This study explored how AI techniques outperform traditional models in controllability, big data handling, cyberattack prevention, smart grid, IoT, robotics, energy efficiency optimization, predictive maintenance control, and computational efficiency. Big data, the development of a machine learning model, and AI will play an important role in the future energy market. Our study\u00e2\u20ac\u2122s findings show that AI is becoming a key enabler of a complex, new and data-related energy industry, providing a key magic tool to increase operational performance and efficiency in an increasingly cut-throat environment. As a result, the energy industry, utilities, power system operators, and independent power producers may need to focus more on AI technologies if they want meaningful results to remain competitive. New competitors, new business strategies, and a more active approach to customers would require informed and flexible regulatory engagement with the associated complexities of customer safety, privacy, and information security. Given the pace of development in information technology, AI and data analysis, regulatory approvals for new services and products in the new Era of digital energy markets can be enforced as quickly and efficiently as possible.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jclepro.2021.125834", "Book Title/Journal": "Journal of Cleaner Production", "TITLE_UPPER": "JOURNAL OF CLEANER PRODUCTION", "Title_SCI": "Journal of Cleaner Production", "Title_JCS": "Journal of Cleaner Production", "SCI_FACTOR": 1.937, "JCS_FACTOR": 9.297}, {"Author": "Sandya De Alwis and Ziwei Hou and Yishuo Zhang and Myung Hwan Na and Bahadorreza Ofoghi and Atul Sajjanhar", "Title": "A survey on smart farming data, applications and techniques", "Keywords": "Smart farming, Data analysis, Big data, Machine learning, Digital farming, Predictive farming, Farming industry", "Abstract": "The Internet of Things (IoT) and the relevant technologies have had a significant impact on smart farming as a major sub-domain within the field of agriculture. Modern technology supports data collection from IoT devices through several farming processes. The extensive amount of collected smart farming data can be utilized for daily decision making and analysis such as yield prediction, growth analysis, quality maintenance, animal and aquaculture, as well as farm management. This survey focuses on three major aspects of contemporary smart farming. First, it highlights various types of big data generated through smart farming and makes a broad categorization of such data. Second, this paper discusses a comprehensive set of typical applications of big data in smart farming. Third, it identifies and introduces the principal big data and machine learning techniques that are utilized in smart farming data analysis. In doing so, this survey also identifies some of the major, current challenges in smart farming big data analysis.This paper provides a discussion on potential pathways toward more effective smart farming through relevant analytics-guided decision making.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compind.2022.103624", "Book Title/Journal": "Computers in Industry", "TITLE_UPPER": "COMPUTERS IN INDUSTRY", "Title_SCI": "Computers in Industry", "Title_JCS": "COMPUTERS IN INDUSTRY", "SCI_FACTOR": 1.432, "JCS_FACTOR": 7.635}, {"Author": "Tao Zhang and Weixi Ji and Yongtao Qiu", "Title": "A framework of energy-consumption driven discrete manufacturing system", "Keywords": "Energy-efficient optimization, Discrete manufacturing system, Data preprocessing, Data mining", "Abstract": "Because of big data on energy consumption, there is a lack of research on the discrete manufacturing system. The discrete manufacturing system has plenty of multi-source and heterogeneous data; it was challenging to collect real-time data. Recently, low carbon and green manufacturing is a hot field; especially, it can save electrical energy. This paper proposes a significant energy consumption data of a data-driven analysis framework, which promoting the energy efficiency of discrete manufacturing plant, equipment, and workshop production process. Firstly, put forward the evaluation standards of energy efficiency for discrete manufacturing shops. Then make energy-consumption data preprocessing. Efficiency optimization of big data mining method is put forward based on grid computing function. Design the discrete manufacturing system energy-consumption parameter values, then summarizes prediction algorithms and models in order to predict the results and the trends. Finally, introduce the application of a mobile phone shell manufacturing shop to verify the proposed framework. Further research will focus on energy-consumption data mining processing.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.seta.2021.101336", "Book Title/Journal": "Sustainable Energy Technologies and Assessments", "TITLE_UPPER": "SUSTAINABLE ENERGY TECHNOLOGIES AND ASSESSMENTS", "Title_SCI": "Sustainable Energy Technologies and Assessments", "Title_JCS": "Sustainable Energy Technologies and Assessments", "SCI_FACTOR": 1.04, "JCS_FACTOR": 5.353}, {"Author": "J.W. Wang and M. Williams", "Title": "Registries, Databases and Repositories for Developing Artificial Intelligence in Cancer Care", "Keywords": "Artificial intelligence, Big Data, database, deep learning, registries, repository", "Abstract": "Modern artificial intelligence techniques have solved some previously intractable problems and produced impressive results in selected medical domains. One of their drawbacks is that they often need very large amounts of data. Pre-existing datasets in the form of national cancer registries, image/genetic depositories and clinical datasets already exist and have been used for research. In theory, the combination of healthcare Big Data with modern, data-hungry artificial intelligence techniques should offer significant opportunities for artificial intelligence development, but this has not yet happened. Here we discuss some of the structural reasons for this, barriers preventing artificial intelligence from making full use of existing datasets, and make suggestions as to enable progress. To do this, we use the framework of the 6Vs of Big Data and the FAIR criteria for data sharing and availability (Findability, Accessibility, Interoperability, and Reuse). We share our experience in navigating these barriers through The Brain Tumour Data Accelerator, a Brain Tumour Charity-supported initiative to integrate fragmented patient data into an enriched dataset. We conclude with some comments as to the limits of such approaches.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.clon.2021.11.040", "Book Title/Journal": "Clinical Oncology", "TITLE_UPPER": "CLINICAL ONCOLOGY", "Title_SCI": "Clinical Oncology", "Title_JCS": "CLINICAL ONCOLOGY", "SCI_FACTOR": 1.037, "JCS_FACTOR": 4.126}, {"Author": "I. Kregel and D. Stemann and J. Koch and A. Coners", "Title": "Process Mining for Six Sigma: Utilising Digital Traces", "Keywords": "process mining, six sigma, DMAIC, big data analytics, data science, project management", "Abstract": "Six Sigma is one of the most successful quality management philosophies of the past 20 years. However, the current challenges facing companies, such as rising process and supply chain complexity, as well as high volumes of unstructured data, cannot easily be answered by relying on traditional Six Sigma tools. Instead, the Process Mining (PM) technology using big data analytics promises valuable support for 6S and its data analysis capabilities. The article presents a design science research project in which a method for the integration of PM in Six Sigma\u00e2\u20ac\u2122s DMAIC project structure was developed. This method could be extended, refined and tested during three evaluation cycles: an expert evaluation with Six Sigma professionals, a technical experiment and finally a multi case study in a company. The method therefore was eventually endorsed by 6S experts and successfully applied in a first pilot setting. This article presents the first developed method for the integration of PM and Six Sigma. It follows the recommendations of many researchers to test Six Sigma as an application field of PM as well as using the potential of big data analytics. The method can be used by researchers and practitioners alike to implement, test and verify its design in organisations.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cie.2020.107083", "Book Title/Journal": "Computers & Industrial Engineering", "TITLE_UPPER": "COMPUTERS & INDUSTRIAL ENGINEERING", "Title_SCI": "N/A", "Title_JCS": "COMPUTERS & INDUSTRIAL ENGINEERING", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.431}, {"Author": "Mauro A. Encinas and Andrzej T. Tunkiel and Dan Sui", "Title": "Downhole data correction for data-driven rate of penetration prediction modeling", "Keywords": "Drilling, Machine learning, Rate of penetration, Drilling data quality improvement, Recurrent neural networks", "Abstract": "In recent years, machine learning has been adopted in the Oil and Gas industry as a promising technology for solutions to the most demanding problems like downhole parameters estimations and incidents detection. A big amount of available data makes this technology an attractive option for solving a wide variety of drilling problems, as well as a reliable candidate for performing big-data analysis and interpretation. Nevertheless, this approach may cause, in some cases, that petroleum engineering concepts are disregarded in favor of more data-intensive approaches. This study aims to evaluate the impact of drilling data measurement correction on data-driven model performance. In our study, besides using the standard data processing technologies, like gap filling, outlier removal, noise reduction etc., the physics-based drilling models are also implemented for data quality improvement and data correction in consideration of the measurement physics, rarely mentioned in most of publications. In our case study, recurrent neural networks (RNN) that are able to capture temporal natures of a signal are employed for the rate of penetration (ROP) estimation with an adjustable predictive window. The results show that the RNN model produces the best results when using the drilling data recovered through analytical methods. Moreover, the comprehensive data-driven model evaluation and engineering interpretation are conducted to facilitate better understanding of the data-driven models and their applications.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.petrol.2021.109904", "Book Title/Journal": "Journal of Petroleum Science and Engineering", "TITLE_UPPER": "JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING", "Title_SCI": "Journal of Petroleum Science and Engineering", "Title_JCS": "JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING", "SCI_FACTOR": 0.975, "JCS_FACTOR": 4.346}, {"Author": "Oane Visser and Sarah Ruth Sippel and Louis Thiemann", "Title": "Imprecision farming? Examining the (in)accuracy and risks of digital agriculture", "Keywords": "Digital agriculture, Smart farming, Precision agriculture, Accuracy, Big data", "Abstract": "The myriad potential benefits of digital farming hinge on the promise of increased accuracy, which allows \u00e2\u20ac\u02dcdoing more with less\u00e2\u20ac\u2122 through precise, data-driven operations. Yet, precision farming's foundational claim of increased accuracy has hardly been the subject of comprehensive examination. Drawing on social science studies of big data, this article examines digital agriculture's (in)accuracies and their repercussions. Based on an examination of the daily functioning of the various components of yield mapping, it finds that digital farming is often \u00e2\u20ac\u02dcprecisely inaccurate\u00e2\u20ac\u2122, with the high volume and granularity of big data erroneously equated with high accuracy. The prevailing discourse of \u00e2\u20ac\u02dcultra-precise\u00e2\u20ac\u2122 digital technologies ignores farmers' essential efforts in making these technologies more accurate, via calibration, corroboration and interpretation. We suggest that there is the danger of a \u00e2\u20ac\u02dcprecision trap\u00e2\u20ac\u2122. Namely, an exaggerated belief in the precision of big data that over time leads to an erosion of checks and balances (analogue data, farmer observation et cetera) on farms. The danger of \u00e2\u20ac\u02dcprecision traps\u00e2\u20ac\u2122 increases with the opacity of algorithms, with shifts from real-time measurement and advice towards forecasting, and with farmers' increased remoteness from field operations. Furthermore, we identify an emerging \u00e2\u20ac\u02dcprecision divide\u00e2\u20ac\u2122: unequally distributed precision benefits resulting from the growing algorithmic divide between farmers focusing on staple crops, catered well by technological innovation on the one hand, and farmers cultivating other crops, who have to make do with much less advanced or applicable algorithms on the other. Consequently, for the latter farms digital farming may feel more like \u00e2\u20ac\u02dcimprecision farming\u00e2\u20ac\u2122.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jrurstud.2021.07.024", "Book Title/Journal": "Journal of Rural Studies", "TITLE_UPPER": "JOURNAL OF RURAL STUDIES", "Title_SCI": "Journal of Rural Studies", "Title_JCS": "JOURNAL OF RURAL STUDIES", "SCI_FACTOR": 1.497, "JCS_FACTOR": 4.849}, {"Author": "Wei Xiong and Li Xiong", "Title": "Anti-collusion data auction mechanism based on smart contract", "Keywords": "Data auction mechanism, Anti-collusion, Smart contract, Blockchain, Ethereum", "Abstract": "Due to the uncertainty of the value of big data, it is difficult to directly give a reasonable price for big data. Auction is an effective method of distributing goods to the bidder with the highest valuation. Hence, the use of auction strategy can not only guarantee the interests of data sellers, but also conform to market principles. However, existing data auction mechanisms are centralized. It is hard to build trust among sellers, buyers and auctioneers. An open and anonymous online environment may cause entities involved in data auctions to collude to manipulate the results of data auctions. This will cause the price of auction data to fail to reach a fair and truthful level. Therefore, the first anti-collusion data auction mechanism based on smart contract is proposed. Through a well-designed anti-collusion data auction algorithm, mutual distrust and rational buyers and sellers safely participate in the data auction without a trusted third party. The data auction mechanism designed in the smart contract can effectively prevent collusion and realize the fairness and truthfulness of data auction. The webpack in the Truffle Boxes is used to implement the data auction mechanism, and the anti-collusion property of the mechanism has been verified. The source code of the smart contract has been uploaded to GitHub.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.ins.2020.10.053", "Book Title/Journal": "Information Sciences", "TITLE_UPPER": "INFORMATION SCIENCES", "Title_SCI": "Information Sciences", "Title_JCS": "INFORMATION SCIENCES", "SCI_FACTOR": 1.524, "JCS_FACTOR": 6.795}, {"Author": "Maira E. Ezerins and Timothy D. Ludwig and Tara O'Neil and Anne M. Foreman and Yal\u00c3\u00a7\u00c4\u00b1n A\u00c3\u00a7\u00c4\u00b1kg\u00c3\u00b6z", "Title": "Advancing safety analytics: A diagnostic framework for assessing system readiness within occupational safety and health", "Keywords": "Safety analytics, Data analytics, Readiness assessment, Occupational health", "Abstract": "Big data and analytics have shown promise in predicting safety incidents and identifying preventative measures directed towards specific risk variables. However, the safety industry is lagging in big data utilization due to various obstacles, which may include lack of data readiness (e.g., disparate databases, missing data, low validity) and personnel competencies. This paper provides a primer on the application of big data to safety. We then describe a safety analytics readiness assessment framework that highlights system requirements and the challenges that safety professionals may encounter in meeting these requirements. The proposed framework suggests that safety analytics readiness depends on (a) the quality of the data available, (b) organizational norms around data collection, scaling, and nomenclature, (c) foundational infrastructure, including technological platforms and skills required for data collection, storage, and analysis of health and safety metrics, and (d) measurement culture, or the emergent social patterns between employees, data acquisition, and analytic processes. A safety-analytics readiness assessment can assist organizations with understanding current capabilities so measurement systems can be matured to accommodate more advanced analytics for the ultimate purpose of improving decisions that mitigate injury and incidents.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.ssci.2021.105569", "Book Title/Journal": "Safety Science", "TITLE_UPPER": "SAFETY SCIENCE", "Title_SCI": "Safety Science", "Title_JCS": "SAFETY SCIENCE", "SCI_FACTOR": 1.178, "JCS_FACTOR": 4.877}, {"Author": "J. Ramsingh and V. Bhuvaneswari", "Title": "An integrated multi-node Hadoop framework to predict high-risk factors of Diabetes Mellitus using a Multilevel MapReduce based Fuzzy Classifier (MMR-FC) and Modified DBSCAN algorithm", "Keywords": "Fuzzy classifier, MDBSCAN, MapReduce, Hadoop, Diabetes mellitus", "Abstract": "In the era of data deluge, the world is experiencing an intensive growth of Big data with complex structures. While processing of these data is a complex and labor-intensive process, a proper analysis of Big data leads to greater knowledge extraction. In this paper, Big data is used to predict high-risk factors of Diabetes Mellitus using a new integrated framework with four Hadoop clusters, which are developed to classify the data based on Multi-level MapReduce Fuzzy Classifier (MMR-FC) and MapReduce-Modified Density-Based Spatial Clustering of Applications with Noise (MR-MDBSCAN) algorithm. Big data concerning people\u00e2\u20ac\u2122s food habits, physical activity are extracted from social media using the API\u00e2\u20ac\u2122s provided. The MMR-FC takes place at three levels of index (Glycemic Index, Physical activity Index, Sleeping Pattern) values. The fuzzy rules are generated by the MMR-FC algorithm to predict the risk of Diabetes Mellitus using the data extracted. The result from MMR-FC is used as an input to the semantic location prediction framework to predict the high-risk zones of Diabetes Mellitus using the MR-MDBSCAN algorithm. The analysis shows that more than 55% of people are in a high-risk group with positive sentiments on the data extracted. More than 70% of food with a high Glycemic Index is usually consumed during Night and Early Evenings, which reveals that people consume food that has a high Glycemic Index during their sedentary slot and have irregular sleep practices. Around 70% of the unhealthiest dietary patterns are retrieved from urban hotspots such as Delhi, Cochin, Kolkata, and Chennai. From the results, it is evident that 55% of younger generations, users of social networking sites having high possibilities of Type II Diabetes Mellitus at large.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.asoc.2021.107423", "Book Title/Journal": "Applied Soft Computing", "TITLE_UPPER": "APPLIED SOFT COMPUTING", "Title_SCI": "N/A", "Title_JCS": "APPLIED SOFT COMPUTING", "SCI_FACTOR": 0.0, "JCS_FACTOR": 6.725}, {"Author": "Serkan Ayvaz and Koray Alpay", "Title": "Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time", "Keywords": "Predictive maintenance, Internet of things, Manufacturing systems, Artificial intelligence, Machine learning, Big data", "Abstract": "In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.eswa.2021.114598", "Book Title/Journal": "Expert Systems with Applications", "TITLE_UPPER": "EXPERT SYSTEMS WITH APPLICATIONS", "Title_SCI": "Expert Systems with Applications", "Title_JCS": "EXPERT SYSTEMS WITH APPLICATIONS", "SCI_FACTOR": 1.368, "JCS_FACTOR": 6.954}, {"Author": "Amel Souifi and Zohra Cherfi Boulanger and Marc Zolghadri and Maher Barkallah and Mohamed Haddar", "Title": "Uncertainty of key performance indicators for Industry 4.0: A methodology based on the theory of belief functions", "Keywords": "Industry 4.0, Performance management, Decision support, Big Data, Uncertainty modeling", "Abstract": "For the past few years, we have been hearing about Industry 4.0 (or the fourth industrial revolution), which promises to improve productivity, flexibility, quality, customer satisfaction and employee well-being. To assess whether these goals are achieved, it is necessary to implement a performance management system (PMS). However, a PMS must take into account the various challenges associated with Industry 4.0, including the availability of large amounts of data. While it represents an opportunity for companies to improve performance, big data does not necessarily mean good data. It can be uncertain, imprecise, ambiguous, etc. Uncertainty is one of the major challenges and it is essential to take it into account when computing performance indicators to increase confidence in decision making. To address this issue, we propose a method to model uncertainty in key performance indicators (KPIs). Our work allows associating with each indicator an uncertainty noted m, computed on the basis of the theory of belief functions. The KPI and its associated uncertainty form a pair (KP I, m). The method developed allows calculating this uncertainty m for the input data of the performance management system. We show how these modeled uncertainties should be propagated to the KPIs. For these KPI uncertainties, we have defined rules to support decision-making. The method developed, based on the theory of belief functions, is part of a methodology we propose to define and extract smart data from massive data. To our knowledge, this is the first attempt to use this theory to model uncertain performance indicators. Our work has shown its effectiveness and its applicability to a case of bottle filling line simulation. In addition to these results, this work opens up new perspectives, particularly for taking uncertainty into account in expert opinions and in industrial risk assessment.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compind.2022.103666", "Book Title/Journal": "Computers in Industry", "TITLE_UPPER": "COMPUTERS IN INDUSTRY", "Title_SCI": "Computers in Industry", "Title_JCS": "COMPUTERS IN INDUSTRY", "SCI_FACTOR": 1.432, "JCS_FACTOR": 7.635}, {"Author": "Yuquan Xu and Xiaobin Ran and Yuewen Liu and Wei Huang", "Title": "Comparing differences in the spatiotemporal patterns between resident tourists and non-resident tourists using hotel check-in registers", "Keywords": "Big data, Spatiotemporal patterns, Resident tourists and non-resident tourists, Multiple city travel, Hotel check-in registers", "Abstract": "Previous research studied the spatiotemporal patterns in different visitor segments but lacks evidence of the segmentation of resident tourists and non-resident tourists in multi-city travel. To fill this gap, this study conducts a big data study using hotel check-in registers. The exploratory data analysis visualizes the spatiotemporal patterns and the differences between resident tourists and non-resident tourists. Then, the spatiotemporal patterns are measured by the length of stay and the number of visited cities. The regression shows that both the length of stay and the number of visited cities of non-resident tourists are higher than those of resident tourists. Moreover, non-resident tourists reduce their length of stay and their number of visited cities more than resident tourists on three-day holidays, while they increase their number of visited cities less than resident tourists on seven-day holidays. This study has significant implications for understanding spatiotemporal patterns and visitors' segmentations.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.tmp.2021.100860", "Book Title/Journal": "Tourism Management Perspectives", "TITLE_UPPER": "TOURISM MANAGEMENT PERSPECTIVES", "Title_SCI": "Tourism Management Perspectives", "Title_JCS": "Tourism Management Perspectives", "SCI_FACTOR": 1.454, "JCS_FACTOR": 6.586}, {"Author": "Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam", "Title": "IoT data visualization for business intelligence in corporate finance", "Keywords": "IoT, Data visualization, Business intelligence, Corporate finance", "Abstract": "Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.ipm.2021.102736", "Book Title/Journal": "Information Processing & Management", "TITLE_UPPER": "INFORMATION PROCESSING & MANAGEMENT", "Title_SCI": "N/A", "Title_JCS": "INFORMATION PROCESSING & MANAGEMENT", "SCI_FACTOR": 0.0, "JCS_FACTOR": 6.222}, {"Author": "Cuili Shao and Yonggang Yang and Sapna Juneja and Tamizharasi GSeetharam", "Title": "IoT data visualization for business intelligence in corporate finance", "Keywords": "IoT, Data visualization, Business intelligence, Corporate finance", "Abstract": "Business intelligence (BI) incorporates business research, data mining, data visualization, data tools,infrastructure, and best practices to help businesses make more data-driven choices.Business intelligence's challenging characteristics include data breaches, difficulty in analyzing different data sources, and poor data quality is consideredessential factors. In this paper, IoT-based Efficient Data Visualization Framework (IoT- EDVF) has been proposed to strengthen leaks' risk, analyze multiple data sources, and data quality management for business intelligence in corporate finance.Corporate analytics management is introduced to enhance the data analysis system's risk, and the complexity of different sources can allow accessing Business Intelligence. Financial risk analysis is implemented to improve data quality management initiative helps use main metrics of success, which are essential to the individual needs and objectives. The statistical outcomes of the simulation analysis show the increasedperformance with a lower delay response of 5ms and improved revenue analysis with the improvement of 29.42% over existing models proving the proposed framework's reliability.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.ipm.2021.102736", "Book Title/Journal": "Information Processing & Management", "TITLE_UPPER": "INFORMATION PROCESSING & MANAGEMENT", "Title_SCI": "N/A", "Title_JCS": "INFORMATION PROCESSING & MANAGEMENT", "SCI_FACTOR": 0.0, "JCS_FACTOR": 6.222}, {"Author": "Elisabetta Raguseo and Federico Pigni and Claudio Vitari", "Title": "Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness", "Keywords": "Streams of big data, Process efficiency, Product effectiveness, Competitive advantage", "Abstract": "Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.im.2021.103451", "Book Title/Journal": "Information & Management", "TITLE_UPPER": "INFORMATION & MANAGEMENT", "Title_SCI": "N/A", "Title_JCS": "INFORMATION & MANAGEMENT", "SCI_FACTOR": 0.0, "JCS_FACTOR": 7.555}, {"Author": "Elisabetta Raguseo and Federico Pigni and Claudio Vitari", "Title": "Streams of digital data and competitive advantage: The mediation effects of process efficiency and product effectiveness", "Keywords": "Streams of big data, Process efficiency, Product effectiveness, Competitive advantage", "Abstract": "Firms can achieve a competitive advantage by leveraging real-time Digital Data Streams (DDSs). The ability to profit from DDSs is emerging as a critical competency for firms and a novel area for Information Technology (IT) investments. We examine the relationship between DDS readiness and competitive advantage by studying the mediation effect of product effectiveness and process efficiency. The research model is tested with data obtained from 302 companies, and the results confirm the existence of the mediation effects. Interestingly, we confirm that competitive advantage is more significantly impacted by IT investments affecting product effectiveness than those affecting process efficiency.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.im.2021.103451", "Book Title/Journal": "Information & Management", "TITLE_UPPER": "INFORMATION & MANAGEMENT", "Title_SCI": "N/A", "Title_JCS": "INFORMATION & MANAGEMENT", "SCI_FACTOR": 0.0, "JCS_FACTOR": 7.555}, {"Author": "Yonggui Guo and Ibrahim Mohamed and Ali Zidane and Yashesh Panchal and Omar Abou-Sayed and Ahmed Abou-Sayed", "Title": "Automated pressure transient analysis: A cloud-based approach", "Keywords": "Pressure transient analysis, Cloud computing, ISIP, Flow regime, G-function, Web-based application", "Abstract": "Pressure transient analysis provides essential information to evaluate the dimensions of injection induced fractures, permeability damage near the wellbore, and pressure elevation in the injection horizon. For injection wells, shut-in data can be collected and analyzed after each injection cycle to evaluate the well injectivity and predict the well longevity. However, any interactive analysis of the pressure data could be subjective and time-consuming. In this study a novel cloud-based approach to automatically analyzing pressure data is presented, which aims to improve the reliability and efficiency of pressure transient analysis. There are two fundamental requirements for automated pressure transient analysis: 1) Pressure data needs to be automatically retrieved from field sites and fed to the analyzer; 2) The engineer can automatically select instantaneous shut-in pressure (ISIP), identify flow regimes, and determine the fracture closure point if any. To meet these requirements as well as to take advantage of cloud storage and computing technologies, a web-based application has been developed to pull real time injection data from any field sites and push it to a cloud database. A built-in pressure transient workflow has been also proposed to detect any stored or real-time pressure data and perform pressure analysis automatically if the required data is available. The automated pressure transient analysis technology has been applied to multiple injection projects. In general, the analysis results including formation and fracture properties (i.e. permeability, fracture half length, skin factor, and fracture closure pressure) are comparable to results from interactive analysis. Any discrepancies are mainly caused by poor data quality. Issues such as inconsistent selections of ISIP and different slopes defined for pre and after closure analyses also contribute to the divergence. Overall, the automated pressure transient analysis provides consistent results as the exact same criteria are applied to the pressure data, and analysis results are independent of the analyzer's experience and knowledge. As data from oil/gas industry increases exponentially over time, automated data transmission, storage, analysis and access are becoming necessary to maximize the value of the data and reduce operation cost. The automated pressure transient analysis presented here demonstrates that cloud storage and computing combined with automated analysis tools is a viable way to overcome big data challenges faced by oil/gas industry professionals.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.petrol.2020.107627", "Book Title/Journal": "Journal of Petroleum Science and Engineering", "TITLE_UPPER": "JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING", "Title_SCI": "Journal of Petroleum Science and Engineering", "Title_JCS": "JOURNAL OF PETROLEUM SCIENCE AND ENGINEERING", "SCI_FACTOR": 0.975, "JCS_FACTOR": 4.346}, {"Author": "Siqi Liu and Tianyu Wang and Shaowei Wang", "Title": "Toward intelligent wireless communications: Deep learning - based physical layer technologies", "Keywords": "Data-driven, Deep learning, Physical layer, Wireless communications", "Abstract": "Advanced technologies are required in future mobile wireless networks to support services with highly diverse requirements in terms of high data rate and reliability, low latency, and massive access. Deep Learning (DL), one of the most exciting developments in machine learning and big data, has recently shown great potential in the study of wireless communications. In this article, we provide a literature review on the applications of DL in the physical layer. First, we analyze the limitations of existing signal processing techniques in terms of model accuracy, global optimality, and computational scalability. Next, we provide a brief review of classical DL frameworks. Subsequently, we discuss recent DL-based physical layer technologies, including both DL-based signal processing modules and end-to-end systems. Deep neural networks are used to replace a single or several conventional functional modules, whereas the objective of the latter is to replace the entire transceiver structure. Lastly, we discuss the open issues and research directions of the DL-based physical layer in terms of model complexity, data quality, data representation, and algorithm reliability.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.dcan.2021.09.014", "Book Title/Journal": "Digital Communications and Networks", "TITLE_UPPER": "DIGITAL COMMUNICATIONS AND NETWORKS", "Title_SCI": "Digital Communications and Networks", "Title_JCS": "Digital Communications and Networks", "SCI_FACTOR": 1.082, "JCS_FACTOR": 6.797}, {"Author": "Luri {Shirosaki Mar\u00c3\u00a7al de Souza} and Andr\u00c3\u00a9a Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}", "Title": "Evaluating and ranking secondary data sources to be used in the Brazilian LCA database \u00e2\u20ac\u201c \u00e2\u20ac\u0153SICV Brasil\u00e2\u20ac\u009d", "Keywords": "Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors", "Abstract": "The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System \u00e2\u20ac\u201c the \u00e2\u20ac\u0153SICV Brasil\u00e2\u20ac\u009d launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.spc.2020.09.021", "Book Title/Journal": "Sustainable Production and Consumption", "TITLE_UPPER": "SUSTAINABLE PRODUCTION AND CONSUMPTION", "Title_SCI": "Sustainable Production and Consumption", "Title_JCS": "Sustainable Production and Consumption", "SCI_FACTOR": 1.019, "JCS_FACTOR": 5.032}, {"Author": "Luri {Shirosaki Mar\u00c3\u00a7al de Souza} and Andr\u00c3\u00a9a Oliveira Nunes and Gabriela Giusti and Yovana M.B. Saavedra and Thiago Oliveira Rodrigues and Tiago E. {Nunes Braga} and Diogo A. {Lopes Silva}", "Title": "Evaluating and ranking secondary data sources to be used in the Brazilian LCA database \u00e2\u20ac\u201c \u00e2\u20ac\u0153SICV Brasil\u00e2\u20ac\u009d", "Keywords": "Qualidata guide, Data quality, Data format, Life Cycle Inventory, Weighting factors", "Abstract": "The generation of reliable life cycle inventories is essential towards Life Cycle Assessment (LCA) development, and the use of literature inventories as data sources can serve as a driving force for emerging LCA databases. The aim of this paper was to propose a method to select and rank scientific publications to be used as possible data sources for supplying LCA databases with new datasets. A case study was designed to identify eligible datasets to compose the emergent Brazilian Life Cycle Inventory Database System \u00e2\u20ac\u201c the \u00e2\u20ac\u0153SICV Brasil\u00e2\u20ac\u009d launched in 2016. The methodology used was based on an exploratory research composed of three steps: i) a bibliographic survey on the scientific productions of Life Cycle Inventories (LCI) in Brazil from 2000 to 2017; ii) a cross-check of LCI data and information based on the 40 selected requirements used in order to analyze the quality of LCI datasets in terms of mandatory, recommended and optional requirements; and iii) an analysis of the data quality requirements for those datasets with support of principles of Analytical Hierarchy Process (AHP) to elect possible datasets to be included in the SICV Brasil database. In total, 57 publications were analyzed and the results indicated that mandatory requirements had under 50% acceptance and only 10 requirements (less than 25%) were fully met. The best LCI dataset received 73 points (90%) with the scoring method, while 16 datasets were given less than 40 points (50%). Therefore, it is necessary to improve data quality of LCI datasets found in literature before using them to integrate LCA databases. In this regard, this study proposed a guide with short, medium, and long-term measures to mitigate this problem. The idea is to put an action plan into practice to gather more LCI datasets from literature which may be eligible for publication to SICV Brasil to improve this national database with more and relevant high-quality datasets.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.spc.2020.09.021", "Book Title/Journal": "Sustainable Production and Consumption", "TITLE_UPPER": "SUSTAINABLE PRODUCTION AND CONSUMPTION", "Title_SCI": "Sustainable Production and Consumption", "Title_JCS": "Sustainable Production and Consumption", "SCI_FACTOR": 1.019, "JCS_FACTOR": 5.032}, {"Author": "Claudia Caudai and Antonella Galizia and Filippo Geraci and Loredana {Le Pera} and Veronica Morea and Emanuele Salerno and Allegra Via and Teresa Colombo", "Title": "AI applications in functional genomics", "Keywords": "Artificial intelligence, Functional genomics, Genomics, Proteomics, Epigenomics, Transcriptomics, Epitranscriptomics, Metabolomics, Machine learning, Deep learning", "Abstract": "We review the current applications of artificial intelligence (AI) in functional genomics. The recent explosion of AI follows the remarkable achievements made possible by \u00e2\u20ac\u0153deep learning\u00e2\u20ac\u009d, along with a burst of \u00e2\u20ac\u0153big data\u00e2\u20ac\u009d that can meet its hunger. Biology is about to overthrow astronomy as the paradigmatic representative of big data producer. This has been made possible by huge advancements in the field of high throughput technologies, applied to determine how the individual components of a biological system work together to accomplish different processes. The disciplines contributing to this bulk of data are collectively known as functional genomics. They consist in studies of: i) the information contained in the DNA (genomics); ii) the modifications that DNA can reversibly undergo (epigenomics); iii) the RNA transcripts originated by a genome (transcriptomics); iv) the ensemble of chemical modifications decorating different types of RNA transcripts (epitranscriptomics); v) the products of protein-coding transcripts (proteomics); and vi) the small molecules produced from cell metabolism (metabolomics) present in an organism or system at a given time, in physiological or pathological conditions. After reviewing main applications of AI in functional genomics, we discuss important accompanying issues, including ethical, legal and economic issues and the importance of explainability.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.csbj.2021.10.009", "Book Title/Journal": "Computational and Structural Biotechnology Journal", "TITLE_UPPER": "COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL", "Title_SCI": "Computational and Structural Biotechnology Journal", "Title_JCS": "Computational and Structural Biotechnology Journal", "SCI_FACTOR": 1.908, "JCS_FACTOR": 7.271}, {"Author": "Haowen Xu and Andy Berres and Yan Liu and Melissa R. Allen-Dumas and Jibonananda Sanyal", "Title": "An overview of visualization and visual analytics applications in water resources management", "Keywords": "empty", "Abstract": "Recent advances in information, communication, and environmental monitoring technologies have increased the availability, spatiotemporal resolution, and quality of water-related data, thereby leading to the emergence of many innovative big data applications. Among these applications, visualization and visual analytics, also known as the visual computing techniques, empower the synergy of computational methods (e.g., machine learning and statistical models) with human reasoning to improve the understanding and solution toward complex science and engineering problems. These approaches are frequently integrated with geographic information systems and cyberinfrastructure to provide new opportunities and methods for enhancing water resources management. In this paper, we present a comprehensive review of recent hydroinformatics applications that employ visual computing techniques to (1) support complex data-driven research problems, and (2) support the communication and decision-makings in the water resources management sector. Then, we conduct a technical review of the state-of-the-art web-based visualization technologies and libraries to share our experiences on developing shareable, adaptive, and interactive visualizations and visual interfaces for resources management applications. We close with a vision that applies the emerging visual computing technologies and paradigms to develop the next generation of water resources management applications.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.envsoft.2022.105396", "Book Title/Journal": "Environmental Modelling & Software", "TITLE_UPPER": "ENVIRONMENTAL MODELLING & SOFTWARE", "Title_SCI": "N/A", "Title_JCS": "ENVIRONMENTAL MODELLING & SOFTWARE", "SCI_FACTOR": 0.0, "JCS_FACTOR": 5.288}, {"Author": "Haoyu Jiang and Kai Chen and Quanbo Ge and Jinqiang Xu and Yingying Fu and Chunxi Li", "Title": "Data consistency method of heterogeneous power IOT based on hybrid model", "Keywords": "Power IOT system, Hybrid model, Heterogeneous data consistency, Machine learning combination method", "Abstract": "The data of the power Internet of Things (IOT) system is transferred from the IaaS layer to the SaaS layer. The general data preprocessing method mainly solves the problem of big data anomalies and missing at the PaaS layer, but it still lacks the ability to judge the high error data that meets the timing characteristics, making it difficult to deal with heterogeneous power inconsistent issues. This paper shows this phenomenon and its physical mechanism, showing the difficulty of building a quantitative model forward. A data-driven method is needed to form a hybrid model to correct the data. The research object is the electricity meter data on both sides of a commercial building transformer, which comes from different power IOT systems. The low-voltage side was revised based on the high-voltage side. Compared with the correction method based on purely using neural networks, the combined method, Linear Regression (LS) + Differential Evolution (DE) + Extreme Learning Machine (ELM), further reduces the deviation from approximately 4% to 1%.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.isatra.2021.01.056", "Book Title/Journal": "ISA Transactions", "TITLE_UPPER": "ISA TRANSACTIONS", "Title_SCI": "ISA Transactions", "Title_JCS": "ISA TRANSACTIONS", "SCI_FACTOR": 1.147, "JCS_FACTOR": 5.468}, {"Author": "Meghna Raj and Shashank Gupta and Vinay Chamola and Anubhav Elhence and Tanya Garg and Mohammed Atiquzzaman and Dusit Niyato", "Title": "A survey on the role of Internet of Things for adopting and promoting Agriculture 4.0", "Keywords": "empty", "Abstract": "There is a rapid increase in the adoption of emerging technologies like the Internet of Things (IoT), Unmanned Aerial Vehicles (UAV), Internet of Underground Things (IoUT), Data analytics in the agriculture domain to meet the increased food demand to cater to the increasing population. Agriculture 4.0 is set to revolutionize agriculture productivity by using Precision Agriculture (PA), IoT, UAVs, IoUT, and other technologies to increase agriculture produce for growing demographics while addressing various farm-related issues. This survey provides a comprehensive overview of how multiple technologies such as IoT, UAVs, IoUT, Big Data Analytics, Deep Learning Techniques, and Machine Learning methods can be used to manage various farm-related operations. For each of these technologies, a detailed review is done on how the technology is being used in Agriculture 4.0. These discussions include an overview of relevant technologies, their use cases, existing case studies, and research works that demonstrate the use of these technologies in Agriculture 4.0. This paper also highlights the various future research gaps in the adoption of these technologies in Agriculture 4.0.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jnca.2021.103107", "Book Title/Journal": "Journal of Network and Computer Applications", "TITLE_UPPER": "JOURNAL OF NETWORK AND COMPUTER APPLICATIONS", "Title_SCI": "Journal of Network and Computer Applications", "Title_JCS": "JOURNAL OF NETWORK AND COMPUTER APPLICATIONS", "SCI_FACTOR": 1.145, "JCS_FACTOR": 6.281}, {"Author": "Ngakan {Nyoman Kutha Krisnawijaya} and Bedir Tekinerdogan and Cagatay Catal and Rik van der Tol", "Title": "Data analytics platforms for agricultural systems: A systematic literature review", "Keywords": "Data analytics platforms, Agriculture, Systematic literature review, Big Data", "Abstract": "With the rapid developments in ICT, the current agriculture businesses have become increasingly data-driven and are supported by advanced data analytics techniques. In this context, several studies have investigated the adopted data analytics platforms in the agricultural sector. However, the main characteristics and overall findings on these platforms are scattered over the various studies, and to the best of our knowledge, there has been no attempt yet to systematically synthesize the features and obstacles of the adopted data analytics platforms. This article presents the results of an in-depth systematic literature review (SLR) that has explicitly focused on the domains of the platforms, the stakeholders, the objectives, the adopted technologies, the data properties and the obstacles. According to the year-wise analysis, it is found that no relevant primary study between 2010 and 2013 was found. This implies that the research of data analytics in agricultural sectors is a popular topic from recent years, so the results from before 2010 are likely less relevant. In total, 535 papers published from 2010 to 2020 were retrieved using both automatic and manual search strategies, among which 45 journal articles were selected for further analysis. From these primary studies, 33 features and 34 different obstacles were identified. The identified features and obstacles help characterize the different data analytics platforms and pave the way for further research.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compag.2022.106813", "Book Title/Journal": "Computers and Electronics in Agriculture", "TITLE_UPPER": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "Title_SCI": "Computers and Electronics in Agriculture", "Title_JCS": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "SCI_FACTOR": 1.208, "JCS_FACTOR": 5.565}, {"Author": "Evagelos D. Lioutas and Chrysanthi Charatsari", "Title": "Enhancing the ability of agriculture to cope with major crises or disasters: What the experience of COVID-19 teaches us", "Keywords": "Agriculture, COVID-19, Major crises, Smart technology, Community marketing, Resilience", "Abstract": "The COVID-19 outbreak was an unprecedented situation that uncovered forgotten interconnections and interdependencies between agriculture, society, and economy, whereas it also brought to the fore the vulnerability of agrifood production to external disturbances. Building upon the ongoing experience of the COVID-19 pandemic, in this short communication, we discuss three potential mechanisms that, in our opinion, can mitigate the impacts of major crises or disasters in agriculture: resilience-promoting policies, community marketing schemes, and smart farming technology. We argue that resilience-promoting policies should focus on the development of crisis management plans and enhance farmers' capacity to cope with external disturbances. We also stress the need to promote community marketing conduits that ensure an income floor for farmers while in parallel facilitating consumer access to agrifood products when mainstream distribution channels under-serve them. Finally, we discuss some issues that need to be solved to ensure that smart technology and big data can help farmers overcome external shocks.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.agsy.2020.103023", "Book Title/Journal": "Agricultural Systems", "TITLE_UPPER": "AGRICULTURAL SYSTEMS", "Title_SCI": "Agricultural Systems", "Title_JCS": "AGRICULTURAL SYSTEMS", "SCI_FACTOR": 1.694, "JCS_FACTOR": 5.37}, {"Author": "Jiashun Mao and Javed Akhtar and Xiao Zhang and Liang Sun and Shenghui Guan and Xinyu Li and Guangming Chen and Jiaxin Liu and Hyeon-Nae Jeon and Min Sung Kim and Kyoung Tai No and Guanyu Wang", "Title": "Comprehensive strategies of machine-learning-based quantitative structure-activity relationship models", "Keywords": "Data analysis in structural biology, Machine learning, Structural biology", "Abstract": "Summary\nEarly quantitative structure-activity relationship (QSAR) technologies have unsatisfactory versatility and accuracy in fields such as drug discovery because they are based on traditional machine learning and interpretive expert features. The development of Big Data and deep learning technologies significantly improve the processing of unstructured data and unleash the great potential of QSAR. Here we discuss the integration of wet experiments (which provide experimental data and reliable verification), molecular dynamics simulation (which provides mechanistic interpretation at the atomic/molecular levels), and machine learning (including deep learning) techniques to improve QSAR models. We first review the history of traditional QSAR and point out its problems. We then propose a better QSAR model characterized by a new iterative framework to integrate machine learning with disparate data input. Finally, we discuss the application of QSAR and machine learning to many practical research fields, including drug development and clinical trials.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.isci.2021.103052", "Book Title/Journal": "iScience", "TITLE_UPPER": "ISCIENCE", "Title_SCI": "iScience", "Title_JCS": "iScience", "SCI_FACTOR": 1.805, "JCS_FACTOR": 5.458}, {"Author": "Zhigang He and Xiaoyu Shen and Yanyan Sun and Shichao Zhao and Bin Fan and Chaofeng Pan", "Title": "State-of-health estimation based on real data of electric vehicles concerning user behavior", "Keywords": "Electric vehicles, SOH, User behavior, LWLR, LSTM", "Abstract": "State of health (SOH) of lithium-ion battery pack directly determines the driving mileage and output power of the electric vehicle. With the development of big data storage and analysis technology, using big data to off-line estimate battery pack SOH is more feasible than before. This paper proposes a SOH estimation method based on real data of electric vehicles concerning user behavior. The charging capacity is calculated by historical charging data, and locally weighted linear regression (LWLR) algorithm is used to qualitatively characterize the capacity decline trend. The health features are extracted from historical operating data, maximal information coefficient (MIC) algorithm is used to measure the correlation between health features and capacity. Then, long and short-term memory (LSTM)-based neural network will further learn the nonlinear degradation relationship between capacity and health features. Bayesian optimization algorithm is used to ensure the generalization of the model when different electric vehicles produce different user behaviors. The estimation method is validated by the 300 days historical dataset from 100 vehicles with different driving behavior. The results indicates that the maximum relative error of estimating SOH is 0.2%.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.est.2021.102867", "Book Title/Journal": "Journal of Energy Storage", "TITLE_UPPER": "JOURNAL OF ENERGY STORAGE", "Title_SCI": "Journal of Energy Storage", "Title_JCS": "Journal of Energy Storage", "SCI_FACTOR": 1.088, "JCS_FACTOR": 6.583}, {"Author": "Jordan Gacutan and Emma L. Johnston and Heidi Tait and Wally Smith and Graeme F. Clark", "Title": "Continental patterns in marine debris revealed by a decade of citizen science", "Keywords": "Environmental monitoring, Plastic pollution, Bioregional management, Litter, Citizen science, Marine debris", "Abstract": "Anthropogenic marine debris is a persistent threat to oceans, imposing risks to ecosystems and the communities they support. Whilst an understanding of marine debris risks is steadily advancing, monitoring at spatial and temporal scales relevant to management remains limited. Citizen science projects address this shortcoming but are often critiqued on data accuracy and potential bias in sampling efforts. Here we present 10-years of Australia's largest marine debris database - the Australian Marine Debris Initiative (AMDI), in which we perform systematic data filtering, test for differences between collecting groups, and report patterns in marine debris. We defined five stages of data filtering to address issues in data quality and to limit inference to ocean-facing sandy beaches. Significant differences were observed in the average accumulation of items between filtered and remaining data. Further, differences in sampling were compared between collecting groups at the same site (e.g., government, NGOs, and schools), where no significant differences were observed. The filtering process removed 21% of events due to data quality issues and a further 42% of events to restrict analyses to ocean-facing sandy beaches. The remaining 7275 events across 852 sites allowed for an assessment of debris patterns at an unprecedented spatial and temporal resolution. Hard plastics were the most common material found on beaches both nationally and regionally, consisting of up to 75% of total debris. Nationally, land and sea-sourced items accounted for 48% and 7% of debris, respectively, with most debris found on the east coast of Australia. This study demonstrates the value of citizen science datasets with broad spatial and temporal coverage, and the importance of data filtering to improve data quality. The citizen science presented provides an understanding of debris patterns on Australia's ocean beaches and can serve as a foundation for future source reduction plans.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.scitotenv.2021.150742", "Book Title/Journal": "Science of The Total Environment", "TITLE_UPPER": "SCIENCE OF THE TOTAL ENVIRONMENT", "Title_SCI": "Science of the Total Environment", "Title_JCS": "SCIENCE OF THE TOTAL ENVIRONMENT", "SCI_FACTOR": 1.795, "JCS_FACTOR": 7.963}, {"Author": "Wenwei Che and Yumiao Zhang and Changqing Lin and Yik Him Fung and Jimmy C.H. Fung and Alexis K.H. Lau", "Title": "Impacts of pollution heterogeneity on population exposure in dense urban areas using ultra-fine resolution air quality data", "Keywords": "Particulate matter, Nitrogen dioxide, Ozone, Pollution heterogeneity, Urban area", "Abstract": "Traditional air quality data have a spatial resolution of 1\u00c2\u00a0km or above, making it challenging to resolve detailed air pollution exposure in complex urban areas. Combining urban morphology, dynamic traffic emission, regional and local meteorology, physicochemical transformations in air quality models using big data fusion technology, an ultra-fine resolution modeling system was developed to provide air quality data down to street level. Based on one-year ultra-fine resolution data, this study investigated the effects of pollution heterogeneity on the individual and population exposure to particulate matter (PM2.5 and PM10), nitrogen dioxide (NO2), and ozone (O3) in Hong Kong, one of the most densely populated and urbanized cities. Sharp fine-scale variabilities in air pollution were revealed within individual city blocks. Using traditional 1\u00c2\u00a0km average to represent individual exposure resulted in a positively skewed deviation of up to 200% for high-end exposure individuals. Citizens were disproportionally affected by air pollution, with annual pollutant concentrations varied by factors of 2 to 5 among 452 District Council Constituency Areas (DCCAs) in Hong Kong, indicating great environmental inequities among the population. Unfavorable city planning resulted in a positive spatial coincidence between pollution and population, which increased public exposure to air pollutants by as large as 46% among districts in Hong Kong. Our results highlight the importance of ultra-fine pollutant data in quantifying the heterogeneity in pollution exposure in the dense urban area and the critical role of smart urban planning in reducing exposure inequities.", "Year": 2023, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jes.2022.02.041", "Book Title/Journal": "Journal of Environmental Sciences", "TITLE_UPPER": "JOURNAL OF ENVIRONMENTAL SCIENCES", "Title_SCI": "Journal of Environmental Sciences", "Title_JCS": "Journal of Environmental Sciences", "SCI_FACTOR": 1.316, "JCS_FACTOR": 5.565}, {"Author": "Yang Ye and Linyan Huang and Qiming Zheng and Chenxin Liang and Baiyu Dong and Jinsong Deng and Xiuzhen Han", "Title": "A feasible framework to downscale NPP-VIIRS nighttime light imagery using multi-source spatial variables and geographically weighted regression", "Keywords": "Nighttime light (NTL), Downscaling, Geographically weighted regression (GWR), Impervious surface detection", "Abstract": "The cloud-free monthly composite of global nighttime light (NTL) data of the Suomi National Polar-orbiting Partnership with the Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) day/night band (DNB) provides indispensable indications of human activities and settlements. However, the coarse spatial resolution (15 arc sec) of NTL imagery greatly restricts its application potential. This study proposes a feasible framework to downscale NPP-VIIRS NTL using muti-source spatial variables and geographically weighted regression (GWR) method. High-resolution auxiliary variables were acquired from the Landsat 8 OLI/ TIRS and social media platforms. GWR-based downscaling procedures were consequently implemented to obtain NTL at a 100-m resolution. The downscaled NTL data were validated against Loujia1-01 imagery based on the coefficient of determination (R2) and root-mean-square error (RMSE). The results suggest that the data quality was suitably improved after downscaling, yielding higher R2 (0.604 vs. 0.568) and lower RMSE (8.828 vs. 9.870 nW/cm2/sr) values than those of the original NTL data. Finally, the NTL was extendedly applied to detect impervious surfaces, and the downscaled NTL had higher accuracy than the original NTL. Therefore, this study facilitates data quality improvement of NPP-VIIRS NTL imagery by downscaling, thus enabling more accurate applications.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jag.2021.102513", "Book Title/Journal": "International Journal of Applied Earth Observation and Geoinformation", "TITLE_UPPER": "INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION", "Title_SCI": "International Journal of Applied Earth Observation and Geoinformation", "Title_JCS": "International Journal of Applied Earth Observation and Geoinformation", "SCI_FACTOR": 1.623, "JCS_FACTOR": 5.933}, {"Author": "P.C. Taylor and M. Abeysekera and Y. Bian and D. \u00c4\u2020etenovi\u00c4\u2021 and M. Deakin and A. Ehsan and V. Levi and F. Li and R. Oduro and R. Preece and P.G. Taylor and V. Terzija and S.L. Walker and J. Wu", "Title": "An interdisciplinary research perspective on the future of multi-vector energy networks", "Keywords": "Energy markets, Information and communication technologies, Modelling, Multi-vector energy networks, Policy, Risk", "Abstract": "Understanding the future of multi-vector energy networks in the context of the transition to net zero and the energy trilemma (energy security, environmental impact and social cost) requires novel interdisciplinary approaches. A variety of challenges regarding systems, plant, physical infrastructure, sources and nature of uncertainties, technological in general and more specifically Information and Communication Technologies requirements, cyber security, big data analytics, innovative business models and markets, policy and societal changes, are critically important to ensure enhanced flexibility and higher resilience, as well as reduced costs of an integrated energy system. Integration of individual energy networks into multi-vector entities opens a number of opportunities, but also presents a number of challenges requiring interdisciplinary perspectives and solutions. Considering drivers like societal evolution, climate change and technology advances, this paper describes the most important aspects which have to be taken into account when designing, planning and operating future multi-vector energy networks. For this purpose, the issues addressing future architecture, infrastructure, interdependencies and interactions of energy network infrastructures are elaborated through a novel interdisciplinary perspective. Aspects related to optimal operation of multi-vector energy networks, implementation of novel technologies, jointly with new concepts and algorithms, are extensively discussed. The role of policy, markets and regulation in facilitating multi-vector energy networks is also reported. Last but not least, the aspects of risks and uncertainties, relevant for secure and optimal operation of future multi-vector energy networks are discussed.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.ijepes.2021.107492", "Book Title/Journal": "International Journal of Electrical Power & Energy Systems", "TITLE_UPPER": "INTERNATIONAL JOURNAL OF ELECTRICAL POWER & ENERGY SYSTEMS", "Title_SCI": "N/A", "Title_JCS": "INTERNATIONAL JOURNAL OF ELECTRICAL POWER & ENERGY SYSTEMS", "SCI_FACTOR": 0.0, "JCS_FACTOR": 4.63}, {"Author": "Gema {Del R\u00c3\u00ado Castro} and Mar\u00c3\u00ada Camino {Gonz\u00c3\u00a1lez Fern\u00c3\u00a1ndez} and \u00c3\u0081ngel {Uruburu Colsa}", "Title": "Unleashing the convergence amid digitalization and sustainability towards pursuing the Sustainable Development Goals (SDGs): A holistic review", "Keywords": "Sustainability, Sustainable development goals (SDGs), Digitalization, ICT, Big data, Artificial intelligence", "Abstract": "The Sustainable Development Goals (SDGs) within the United Nations 2030 Agenda emerged in 2015, becoming an unprecedented global compass for navigating extant sustainability challenges. Nevertheless, it still represents a nascent field enduring uncertainties and complexities. In this regard, the interplay between digitalization and sustainability unfolds bright opportunities for shaping a greener economy and society, paving the way towards the SDGs. However, little evidence exists so far, about a genuine contribution of digital paradigms to sustainability. Besides, their role to tackle the SDGs research gaps remains unexplored. Thus, a holistic characterization of the aforementioned topics has not been fully explored in the emerging literature, deserving further research. The article endeavors a twofold purpose: (1) categorizing the main SDGs research gaps; (2) coupled with a critical exploration of the potential contribution of digital paradigms, particularly Big Data and Artificial Intelligence, towards overcoming the aforesaid caveats and pursuing the 2030 Agenda. Ultimately, the study seeks to bridge literature gaps by providing a first-of-its-kind overview on the SDGs and their nexus with digitalization, while unraveling policy implications and future research directions. The methodology has consisted of a systematic holistic review and in-depth qualitative analysis of the literature on the realms of the SDGs and digitalization. Our findings evidence that the SDGs present several research gaps, namely: flawed understanding of complexities and interlinkages; design shortcomings and imbalances; implementation and governance hurdles; unsuitable indicators and assessment methodologies; truncated adoption and off-target progress; unclear responsibilities and lacking coordination; untapped role of technological innovation and knowledge management. Moreover, our results show growing expectations about the added value brought by digitalization for pursuing the SDGs, through novel data sources, enhanced analytical capacities and collaborative digital ecosystems. However, current research and practice remains in early-stage, pointing to ethical, social and environmental controversies, along with policy caveats, which merit additional research. In light of the findings, the authors suggest a first-approach exploration of research and policy implications. Results suggest that further multidisciplinary research, dialogue and concerted efforts for transformation are required. Reframing the Agenda, while aligning the sustainable development and digitalization policies, seems advisable to ensure a holistic sustainability. The findings aim at guiding and stimulating further research and science-policy dialogue on the promising nexus amid the SDGs and digitalization.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jclepro.2020.122204", "Book Title/Journal": "Journal of Cleaner Production", "TITLE_UPPER": "JOURNAL OF CLEANER PRODUCTION", "Title_SCI": "Journal of Cleaner Production", "Title_JCS": "Journal of Cleaner Production", "SCI_FACTOR": 1.937, "JCS_FACTOR": 9.297}, {"Author": "Ratchayuda Kongboon and Shabbir H. Gheewala and Sate Sampattagul", "Title": "Greenhouse gas emissions inventory data acquisition and analytics for low carbon cities", "Keywords": "Sustainable city, Low carbon city, Greenhouse gas inventory, Greenhouse gas emissions, Municipalities", "Abstract": "This paper studied greenhouse gas inventory data acquisition and analytics for municipalities in Thailand. A complete and transparent GHG inventory of eight municipalities was developed to document the current situation, and to help decision-makers to clarify their priorities for reducing greenhouse gas emissions. The Global Protocol for Community-Scale Greenhouse Gas Emissions Inventories guidelines was used to investigate and calculate the greenhouse gas emissions and assess data accuracy. The results indicated that the data source, data format, and data collection of each municipality are relatively similar. Moreover, the activity data needed to be obtained from several authorities. The results showed that Nonthaburi Municipality had the highest greenhouse gas emissions at 2,286,838 tCO2e/yr and Buriram Municipality, the lowest at 239,795 tCO2e/yr. On a per-capita basis, Lamphun Municipality was the highest with 10.1 tCO2e/capita and Buriram Municipality the lowest with 3.8 tCO2e/capita. The results suggest that the municipalities should continually develop a GHG database by creating a routine procedure. An information management system should be produced in the shape of big data which can lead to state policies, plans, and actions for city development to ensure the reduction of greenhouse gas emissions. This in turn will lead to a low carbon city.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jclepro.2022.130711", "Book Title/Journal": "Journal of Cleaner Production", "TITLE_UPPER": "JOURNAL OF CLEANER PRODUCTION", "Title_SCI": "Journal of Cleaner Production", "Title_JCS": "Journal of Cleaner Production", "SCI_FACTOR": 1.937, "JCS_FACTOR": 9.297}, {"Author": "Julien Leprince and Clayton Miller and Wim Zeiler", "Title": "Data mining cubes for buildings, a generic framework for multidimensional analytics of building performance data", "Keywords": "Data mining, Data cube, Generic method, Multidimensional analytics, Machine learning, Building data", "Abstract": "Over the last decade, collecting massive volumes of data has been made all the more accessible, pushing the building sector to embrace data mining as a powerful tool for harvesting the potential of big data analytics. However repetitive challenges still persist emerging from the need for a common analytical frame, effective application- and insight-driven targeted data selection, as well as benchmarked-supported claims. This study addresses these concerns by putting forward a generic stepwise multidimensional data mining framework tailored to building data, leveraging the dimensional-structures of data cubes. Using the open Building Data Genome Project 2 set, composed of 3053 energy meters from 1636 buildings, we provide an online, open access, implementation illustration of our method applied to automated pattern identification. We define a 3-dimensional building cube echoing typical analytical frames of interest, namely, bottom-up, top-down and temporal drill-in approaches. Our results highlight the importance of application and insight driven mining for effective dimensional-frame targeting. Impactful visualizations were developed allowing practical human inspection, paving the path towards more interpretable analytics.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.enbuild.2021.111195", "Book Title/Journal": "Energy and Buildings", "TITLE_UPPER": "ENERGY AND BUILDINGS", "Title_SCI": "Energy and Buildings", "Title_JCS": "ENERGY AND BUILDINGS", "SCI_FACTOR": 1.737, "JCS_FACTOR": 5.879}, {"Author": "Thanasis Kotsiopoulos and Panagiotis Sarigiannidis and Dimosthenis Ioannidis and Dimitrios Tzovaras", "Title": "Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm", "Keywords": "Industry 4.0, Machine Learning, Deep Learning, Industrial AI, Smart Grid", "Abstract": "Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cosrev.2020.100341", "Book Title/Journal": "Computer Science Review", "TITLE_UPPER": "COMPUTER SCIENCE REVIEW", "Title_SCI": "Computer Science Review", "Title_JCS": "Computer Science Review", "SCI_FACTOR": 1.646, "JCS_FACTOR": 7.872}, {"Author": "Mehrbakhsh Nilashi and Hossein Ahmadi and Goli Arji and Khalaf Okab Alsalem and Sarminah Samad and Fahad Ghabban and Ahmed Omar Alzahrani and Ali Ahani and Ala Abdulsalam Alarood", "Title": "Big social data and customer decision making in vegetarian restaurants: A combined machine learning method", "Keywords": "Online reviews, Food quality, Vegetarian friendly restaurants, Text mining, Segmentation", "Abstract": "Customers increasingly use various social media to share their opinion about restaurants service quality. Big data collected from social media provides a data platform to improve the service quality of restaurants through customers' online reviews, where online reviews are a trustworthy and reliable source that helps consumers to evaluate food quality. Developing methods for effective evaluation of customer-generated reviews of restaurant services is important. This study develops a new method through effective learning techniques for customer segmentation and their preferences prediction in vegetarian friendly restaurants. The method is developed through text mining (Latent Dirichlet Allocation), cluster analysis (Self Organizing Map) and predictive learning technique (Classification and Regression Trees) to reveal the customer\u00e2\u20ac\u2122 satisfaction levels from the service quality in vegetarian friendly restaurants. Based on the obtained results of our experiments on the data vegetarian friendly restaurants in Bangkok, the models constructed by Classification and Regression Trees were able to give an accurate prediction of customers' preferences on the basis of restaurants' quality factors. The results showed that customers\u00e2\u20ac\u2122 online reviews analysis can be an effective way for customers segmentation to predict their preferences and help the restaurant managers to set priority instructions for service quality improvements.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.jretconser.2021.102630", "Book Title/Journal": "Journal of Retailing and Consumer Services", "TITLE_UPPER": "JOURNAL OF RETAILING AND CONSUMER SERVICES", "Title_SCI": "Journal of Retailing and Consumer Services", "Title_JCS": "Journal of Retailing and Consumer Services", "SCI_FACTOR": 1.568, "JCS_FACTOR": 7.135}, {"Author": "S. Chakraborty and S. Adhikari and R. Ganguli", "Title": "The role of surrogate models in the development of digital twins of dynamic systems", "Keywords": "Digital twin, Vibration, Response, Frequency, Surrogate", "Abstract": "Digital twin technology has significant promise, relevance and potential of widespread applicability in various industrial sectors such as aerospace, infrastructure and automotive. However, the adoption of this technology has been slower due to the lack of clarity for specific applications. A discrete damped dynamic system is used in this paper to explore the concept of a digital twin. As digital twins are also expected to exploit data and computational methods, there is a compelling case for the use of surrogate models in this context. Motivated by this synergy, we have explored the possibility of using surrogate models within the digital twin technology. In particular, the use of Gaussian process (GP) emulator within the digital twin technology is explored. GP has the inherent capability of addressing noisy and sparse data and hence, makes a compelling case to be used within the digital twin framework. Cases involving stiffness variation and mass variation are considered, individually and jointly, along with different levels of noise and sparsity in data. Our numerical simulation results clearly demonstrate that surrogate models, such as GP emulators, have the potential to be an effective tool for the development of digital twins. Aspects related to data quality and sampling rate are analysed. Key concepts introduced in this paper are summarised and ideas for urgent future research needs are proposed.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.apm.2020.09.037", "Book Title/Journal": "Applied Mathematical Modelling", "TITLE_UPPER": "APPLIED MATHEMATICAL MODELLING", "Title_SCI": "Applied Mathematical Modelling", "Title_JCS": "APPLIED MATHEMATICAL MODELLING", "SCI_FACTOR": 1.011, "JCS_FACTOR": 5.129}, {"Author": "Aizatul Shafiqah {Mohd Faizal} and T. Malathi Thevarajah and Sook Mei Khor and Siow-Wee Chang", "Title": "A review of risk prediction models in cardiovascular disease: conventional approach vs. artificial intelligent approach", "Keywords": "Cardiovascular diseases, Risk prediction, Artificial intelligence, Machine learning, Deep learning", "Abstract": "Cardiovascular disease (CVD) is the leading cause of death worldwide and is a global health issue. Traditionally, statistical models are used commonly in the risk prediction and assessment of CVD. However, the adoption of artificial intelligent (AI) approach is rapidly taking hold in the current era of technology to evaluate patient risks and predict the outcome of CVD. In this review, we outline various conventional risk scores and prediction models and do a comparison with the AI approach. The strengths and limitations of both conventional and AI approaches are discussed. Besides that, biomarker discovery related to CVD are also elucidated as the biomarkers can be used in the risk stratification as well as early detection of the disease. Moreover, problems and challenges involved in current CVD studies are explored. Lastly, future prospects of CVD risk prediction and assessment in the multi-modality of big data integrative approaches are proposed.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.cmpb.2021.106190", "Book Title/Journal": "Computer Methods and Programs in Biomedicine", "TITLE_UPPER": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "Title_SCI": "Computer Methods and Programs in Biomedicine", "Title_JCS": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "SCI_FACTOR": 0.924, "JCS_FACTOR": 5.428}, {"Author": "Maria E. Mondejar and Ram Avtar and Heyker Lellani Ba\u00c3\u00b1os Diaz and Rama Kant Dubey and Jes\u00c3\u00bas Esteban and Abigail G\u00c3\u00b3mez-Morales and Brett Hallam and Nsilulu Tresor Mbungu and Chukwuebuka Christopher Okolo and Kumar Arun Prasad and Qianhong She and Sergi Garcia-Segura", "Title": "Digitalization to achieve sustainable development goals: Steps towards a Smart Green Planet", "Keywords": "Digitalization, Food-water-energy nexus, Internet of things, Geographic information system (GIS), Sustainable development", "Abstract": "Digitalization provides access to an integrated network of unexploited big data with potential benefits for society and the environment. The development of smart systems connected to the internet of things can generate unique opportunities to strategically address challenges associated with the United Nations Sustainable Development Goals (SDGs) to ensure an equitable, environmentally sustainable, and healthy society. This perspective describes the opportunities that digitalization can provide towards building the sustainable society of the future. Smart technologies are envisioned as game-changing tools, whereby their integration will benefit the three essential elements of the food-water-energy nexus: (i) sustainable food production; (ii) access to clean and safe potable water; and (iii) green energy generation and usage. It then discusses the benefits of digitalization to catalyze the transition towards sustainable manufacturing practices and enhance citizens' health wellbeing by providing digital access to care, particularly for the underserved communities. Finally, the perspective englobes digitalization benefits by providing a holistic view on how it can contribute to address the serious challenges of endangered planet biodiversity and climate change.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.scitotenv.2021.148539", "Book Title/Journal": "Science of The Total Environment", "TITLE_UPPER": "SCIENCE OF THE TOTAL ENVIRONMENT", "Title_SCI": "Science of the Total Environment", "Title_JCS": "SCIENCE OF THE TOTAL ENVIRONMENT", "SCI_FACTOR": 1.795, "JCS_FACTOR": 7.963}, {"Author": "K.C. Chan and Victor T.T. Wong and Anthony K.F. Yow and P.L. Yuen and Christopher Y.H. Chao", "Title": "Development and performance evaluation of a chiller plant predictive operational control strategy by artificial intelligence", "Keywords": "Chiller plant optimization, Artificial intelligence, Artificial neural network, Particle swarm optimization, VSD chiller, Building energy saving", "Abstract": "Traditionally, chiller plants are controlled and monitored by a predetermined control strategy to ensure appropriate operation based on the designed system configuration. With the use of new technology of variable speed drive (VSD) for compressors, smart control strategies could be leveraged to enhance the system efficiency in lieu of traditional control strategies. For example, using orderly and straightforward switching procedures without considering various factors in switching the units, including the high-efficiency partial load range benefitted from the VSD, the actual performance of the units as a whole and the variable chilled water flow rate, result in the chiller plant not operating at maximum performance and efficiency. To address these issues, a hybrid predictive operational chiller plant control strategy is proposed to optimize the performance of the chiller plant. Artificial intelligence is employed as the data mining algorithm, with big data analysis based on the actual acquired voluminous operation data by fully considering the characteristics of chiller plants without additional installation of large-sized and high-priced equipment. Artificial neural network (ANN) was employed in the control strategy to predict the future outdoor temperature, building cooling load demand and the corresponding power consumption of the chiller plants. At the same time, particle swarm optimization (PSO) was applied to search for the optimized setpoints, e.g., chilled water supply temperature, operating sequence, chilled water flow rate, for the chiller plants. The developed control strategy has been launched in a chiller plant with a cooling capacity of 7,700\u00c2\u00a0kW installed in a hospital in Hong Kong. The system coefficient of performance (COP) and overall energy consumption of the chiller plants were enhanced by about 8.6% and reduced by about 7.9%, respectively, compared with the traditional control strategy. This real-time, continuous, automatic optimization control strategy can determine the most efficient combination of operating parameters of a chiller plant with different control settings. This ensures that the chiller plant operates in its most efficient mode year-round under various operational conditions.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.enbuild.2022.112017", "Book Title/Journal": "Energy and Buildings", "TITLE_UPPER": "ENERGY AND BUILDINGS", "Title_SCI": "Energy and Buildings", "Title_JCS": "ENERGY AND BUILDINGS", "SCI_FACTOR": 1.737, "JCS_FACTOR": 5.879}, {"Author": "Christiane Bahlo and Peter Dahlhaus", "Title": "Livestock data \u00e2\u20ac\u201c Is it there and is it FAIR? A systematic review of livestock farming datasets in Australia", "Keywords": "Livestock data quality, Systematic data review, FAIR data, FAIR assessment, Precision livestock farming, Extensive livestock farming", "Abstract": "The global adoption of the FAIR principles for scientific data: findable, accessible, interoperable and reusable, has been relatively slow in agriculture, compared to other disciplines. A recent review of the literature showed that the use of precision farming technologies and the development and adoption of open data standards was particularly low in extensive livestock farming. However, a plethora of public datasets exist that have the potential to be used to inform precision farming decision tools. Using extensive livestock farming in Australia as example, we investigate the quantity and quality of datasets available via a systematic dataset review. This systematic review of datasets begins with a search of open data catalogues and querying these to find datasets. Software scripts are developed and used to query the Application Programming Interfaces (APIs) of many of the large data catalogues in Australia, while catalogues without public APIs are queried manually via available web portals. Following the systematic search, a combined list of all datasets is collated and tested for FAIRness and other quality metrics. The contribution of this work is the resulting overview of the state of open datasets within the livestock farming domain on the one hand, but also the development of a systematic dataset search strategy, reusable methods and software scripts.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compag.2021.106365", "Book Title/Journal": "Computers and Electronics in Agriculture", "TITLE_UPPER": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "Title_SCI": "Computers and Electronics in Agriculture", "Title_JCS": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "SCI_FACTOR": 1.208, "JCS_FACTOR": 5.565}, {"Author": "Xueqin Pang and Christopher B. Forrest and F\u00c3\u00a9lice L\u00c3\u00aa-Scherban and Aaron J. Masino", "Title": "Prediction of early childhood obesity with machine learning and electronic health record data", "Keywords": "Data quality control, Early childhood obesity, Electronic health record, Machine learning, Prediction", "Abstract": "Objective\nThis study compares seven machine learning models developed to predict childhood obesity from age > 2 to \u00e2\u2030\u00a4 7 years using Electronic Healthcare Record (EHR) data up to age 2 years.\nMaterials and methods\nEHR data from of 860,510 patients with 11,194,579 healthcare encounters were obtained from the Children\u00e2\u20ac\u2122s Hospital of Philadelphia. After applying stringent quality control to remove implausible growth values and including only individuals with all recommended wellness visits by age 7 years, 27,203 (50.78 % male) patients remained for model development. Seven machine learning models were developed to predict obesity incidence as defined by the Centers for Disease Control and Prevention (age/sex adjusted BMI>95th percentile). Model performance was evaluated by multiple standard classifier metrics and the differences among seven models were compared using the Cochran's Q test and post-hoc pairwise testing.\nResults\nXGBoost yielded 0.81 (0.001) AUC, which outperformed all other models. It also achieved statistically significant better performance than all other models on standard classifier metrics (sensitivity fixed at 80 %): precision 30.90 % (0.22 %), F1-socre 44.60 % (0.26 %), accuracy 66.14 % (0.41 %), and specificity 63.27 % (0.41 %).\nDiscussion and conclusion\nEarly childhood obesity prediction models were developed from the largest cohort reported to date. Relative to prior research, our models generalize to include males and females in a single model and extend the time frame for obesity incidence prediction to 7 years of age. The presented machine learning model development workflow can be adapted to various EHR-based studies and may be valuable for developing other clinical prediction models.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.ijmedinf.2021.104454", "Book Title/Journal": "International Journal of Medical Informatics", "TITLE_UPPER": "INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS", "Title_SCI": "International Journal of Medical Informatics", "Title_JCS": "INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS", "SCI_FACTOR": 1.124, "JCS_FACTOR": 4.046}, {"Author": "Felix Krieger and Paul Drews and Patrick Velte", "Title": "Explaining the (non-) adoption of advanced data analytics in auditing: A process theory", "Keywords": "Audit digitization, Audit data analytics, Big data, Machine learning, Advanced data analytics in auditing, Audit innovation", "Abstract": "Audit firms are increasingly engaging with advanced data analytics to improve the efficiency and effectiveness of external audits through the automation of audit work and obtaining a better understanding of the client\u00e2\u20ac\u2122s business risk and thus their own audit risk. This paper examines the process by which audit firms adopt advanced data analytics, which has been left unaddressed by previous research. We derive a process theory from expert interviews which describes the activities within the process and the organizational units involved. It further describes how the adoption process is affected by technological, organizational and environmental contextual factors. Our work contributes to the extent body of research on technology adoption in auditing by using a previously unused theoretical perspective, and contextualizing known factors of technology adoption. The findings presented in this paper emphasize the importance of technological capabilities of audit firms for the adoption of advanced data analytics; technological capabilities within audit teams can be leveraged to support both the ideation of possible use cases for advanced data analytics, as well as the diffusion of solutions into practice.", "Year": 2021, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.accinf.2021.100511", "Book Title/Journal": "International Journal of Accounting Information Systems", "TITLE_UPPER": "INTERNATIONAL JOURNAL OF ACCOUNTING INFORMATION SYSTEMS", "Title_SCI": "International Journal of Accounting Information Systems", "Title_JCS": "International Journal of Accounting Information Systems", "SCI_FACTOR": 0.897, "JCS_FACTOR": 4.4}, {"Author": "Joseph P. Boomgard-Zagrodnik and David J. Brown", "Title": "Machine learning imputation of missing Mesonet temperature observations", "Keywords": "Machine learning, Big data, Surface weather observations, Degree day models, Missing data imputation", "Abstract": "Uninterrupted and reliable weather data is a necessary foundation for agricultural decision making, required for models based on accumulated growing degree days (GDD), chill units, and evapotranspiration. When a weather station experiences a mechanical or communications failure, a replacement (imputed) value should be substituted for any missing data. This study introduces a machine learning, network-based approach to imputing missing 15-minute and daily maximum/minimum air temperature observations from 8.5\u00c2\u00a0years of air temperature, relative humidity, wind, and solar radiation observations at 134 AgWeatherNet (AWN) stations in Washington State. A random forest imputation model trained on temperature and humidity observations from the full network predicted 15-minute, daily maximum, and daily minimum temperature values with mean absolute errors of 0.43\u00c2\u00a0\u00c2\u00b0C, 0.53\u00c2\u00a0\u00c2\u00b0C, and 0.70\u00c2\u00a0\u00c2\u00b0C, respectively. Sensitivity experiments determined that imputation skill was related a number of external factors including volume and type of training data, proximity of surrounding stations, and regional topography. In particular, nocturnal cold air flows in the upper Yakima Valley of south-central Washington caused temperature to be less correlated with surrounding stations in the overnight hours. In a separate experiment, the imputation model was used to predict base- 10\u00c2\u00a0\u00c2\u00b0C GDD on 2020 July 1 trained entirely on 15-minute station data from previous years. Even with the entire season of observations missing, the model predicted the GDD value within an average error 1.4% with 125 of 134 stations within 5% of observations. Since missing data can typically be resolved within a timeframe of a few days, the network-based imputation model is a sufficient substitute for short periods of missing observational weather data. Other potential applications of an imputation model are briefly discussed.", "Year": 2022, "Type Publication": "article", "DOI": "https://doi.org/10.1016/j.compag.2021.106580", "Book Title/Journal": "Computers and Electronics in Agriculture", "TITLE_UPPER": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "Title_SCI": "Computers and Electronics in Agriculture", "Title_JCS": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "SCI_FACTOR": 1.208, "JCS_FACTOR": 5.565}]